{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed08001c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 2023-24 season...\n",
      "2023-24: 4385 records\n",
      "Fetching 2022-23 season...\n",
      "2022-23: 4322 records\n",
      "Fetching 2021-22 season...\n",
      "2021-22: 4182 records\n",
      "Fetching 2020-21 season...\n",
      "2020-21: 2726 records\n",
      "\n",
      "Total combined records: 15615\n",
      "Unique games: 7807\n",
      "Written to 'nba_games_2021_to_2024.csv'\n",
      "Written unique game IDs to 'unique_game_ids.csv'\n",
      "Total records: 15615\n",
      "Unique games: 7807\n",
      "First game ID: 0012000001\n",
      "Last game ID: 2072100016\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamelog\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.static import teams\n",
    "import time\n",
    "\n",
    "# Get 2023-24 season\n",
    "print(\"Fetching 2023-24 season...\")\n",
    "games_finder_24 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2023-24',\n",
    "    timeout=120\n",
    ")\n",
    "games_24 = games_finder_24.get_data_frames()[0]\n",
    "print(f\"2023-24: {len(games_24)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2022-23 season\n",
    "print(\"Fetching 2022-23 season...\")\n",
    "games_finder_23 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2022-23',\n",
    "    timeout=120\n",
    ")\n",
    "games_23 = games_finder_23.get_data_frames()[0]\n",
    "print(f\"2022-23: {len(games_23)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2021-22 season\n",
    "print(\"Fetching 2021-22 season...\")\n",
    "games_finder_22 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2021-22',\n",
    "    timeout=120\n",
    ")\n",
    "games_22 = games_finder_22.get_data_frames()[0]\n",
    "print(f\"2021-22: {len(games_22)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2020-21 season\n",
    "print(\"Fetching 2020-21 season...\")\n",
    "games_finder_21 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2020-21',\n",
    "    timeout=120\n",
    ")\n",
    "games_21 = games_finder_21.get_data_frames()[0]\n",
    "print(f\"2020-21: {len(games_21)} records\")\n",
    "\n",
    "# Concatenate all seasons\n",
    "all_games = pd.concat([games_21, games_22, games_23, games_24], ignore_index=True)\n",
    "print(f\"\\nTotal combined records: {len(all_games)}\")\n",
    "\n",
    "# Get unique game IDs sorted\n",
    "unique_game_ids = sorted(all_games['GAME_ID'].dropna().unique())  # There are NAs to drop\n",
    "print(f\"Unique games: {len(unique_game_ids)}\")\n",
    "\n",
    "# Write to CSV\n",
    "all_games.to_csv('nba_games_2021_to_2024.csv', index=False)\n",
    "print(\"Written to 'nba_games_2021_to_2024.csv'\")\n",
    "\n",
    "# Write unique game IDs to separate file\n",
    "game_ids_df = pd.DataFrame({'GAME_ID': unique_game_ids})\n",
    "game_ids_df['GAME_ID'] = game_ids_df['GAME_ID'].astype(str)  # ensure string since there are zeros at the front\n",
    "game_ids_df.to_csv('unique_game_ids.csv', index=False)\n",
    "print(\"Written unique game IDs to 'unique_game_ids.csv'\")\n",
    "\n",
    "# Print Statements\n",
    "print(f\"Total records: {len(all_games)}\")\n",
    "print(f\"Unique games: {len(unique_game_ids)}\")\n",
    "print(f\"First game ID: {unique_game_ids[0]}\")\n",
    "print(f\"Last game ID: {unique_game_ids[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c3cac",
   "metadata": {},
   "source": [
    "With the NBA API it was a trial and error process to pull data. It did not seem like there was a definitive reason why data was not pulling. So we built a script that would simply run multiple passes over a unique games list and try to get a complete set of data. This proved to be successful. Even testing different sleep times proved to be unsuccessful.\n",
    "\n",
    "Basically we were collecting two sets of data one was from \"boxscoretraditionalv2\" which gave team and player stats. We needed to collect a set for the first half and the complete game this would give us a complete picture of the data so we could predict a second half point total. We also used \"boxscoresummaryv2\" to collect game times, referees, injuries, points by qtr. It would basically pull a list of 7 tables, the multiple pass code was neccesarily for this as it seemed to fail to pull more often. In addition we had to patch this API pull to code in 'GAME_ID' for a couple of the tables that did not have it present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42255892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PASS 3 ===\n",
      "Total games to process: 301\n",
      "First game ID: 0022100426\n",
      "Last game ID: 2052100091\n",
      "\n",
      "‚úÖ Data saved!\n",
      "Successfully processed: 301\n",
      "Player records: 5716\n",
      "Team records: 600\n",
      "\n",
      "üéâ All games successful - no need for pass 4!\n",
      "\n",
      "==================================================\n",
      "PASS 3 COMPLETE\n",
      "==================================================\n",
      "Successfully processed: 301/301\n",
      "Failed: 0/301\n",
      "Total time: 20.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/xk0_fhkn7l3czh5hp6y8gv040000gn/T/ipykernel_26975/222045344.py:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fh_players_combined = pd.concat(all_fh_player_stats, ignore_index=True)\n",
      "/var/folders/0b/xk0_fhkn7l3czh5hp6y8gv040000gn/T/ipykernel_26975/222045344.py:78: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fh_teams_combined = pd.concat(all_fh_team_stats, ignore_index=True)\n",
      "/var/folders/0b/xk0_fhkn7l3czh5hp6y8gv040000gn/T/ipykernel_26975/222045344.py:79: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fh_starters_bench_combined = pd.concat(all_fh_starter_bench, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Getting First Half Box Score Data - Multi-Pass Version\n",
    "from nba_api.stats.endpoints import boxscoretraditionalv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration - Change these for each pass\n",
    "PASS_NUMBER = 3\n",
    "INPUT_FILE = 'unique_game_ids.csv' if PASS_NUMBER == 1 else f'failed_game_ids_fh_pass{PASS_NUMBER-1}.csv'\n",
    "OUTPUT_SUFFIX = '' if PASS_NUMBER == 1 else f'_pt{PASS_NUMBER}'\n",
    "\n",
    "# Loading Game IDs\n",
    "game_ids_df = pd.read_csv(INPUT_FILE, dtype={'GAME_ID': str})\n",
    "game_ids_list = game_ids_df['GAME_ID'].tolist()\n",
    "\n",
    "# Initial Print Statements\n",
    "print(f\"=== PASS {PASS_NUMBER} ===\")\n",
    "print(f\"Total games to process: {len(game_ids_list)}\")\n",
    "print(f\"First game ID: {game_ids_list[0]}\")\n",
    "print(f\"Last game ID: {game_ids_list[-1]}\")\n",
    "\n",
    "# Lists to collect dataframes\n",
    "all_fh_player_stats = []\n",
    "all_fh_team_stats = []\n",
    "all_fh_starter_bench = []\n",
    "\n",
    "# Variables to track progress\n",
    "games_processed = 0\n",
    "games_failed = 0\n",
    "start_time = time.time()\n",
    "failed_game_ids = []\n",
    "\n",
    "# First Half Loop\n",
    "for idx, game_id in enumerate(game_ids_list):\n",
    "    try:\n",
    "        # API function\n",
    "        fh = boxscoretraditionalv2.BoxScoreTraditionalV2(\n",
    "            game_id=game_id,\n",
    "            range_type=1,\n",
    "            start_period=1,\n",
    "            end_period=2\n",
    "        )\n",
    "        \n",
    "        # Frames to DF\n",
    "        dfs = fh.get_data_frames()\n",
    "        \n",
    "        # Append\n",
    "        all_fh_player_stats.append(dfs[0])\n",
    "        all_fh_team_stats.append(dfs[1])\n",
    "        all_fh_starter_bench.append(dfs[2])\n",
    "        \n",
    "        # Track process\n",
    "        games_processed += 1\n",
    "        \n",
    "        # Progress Update every 1000 games\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Progress: {idx + 1}/{len(game_ids_list)} games ({games_processed} success, {games_failed} failed) - {elapsed/60:.1f} min elapsed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        games_failed += 1\n",
    "        # Track failed game ID\n",
    "        failed_game_ids.append(game_id)\n",
    "        # Print at 1st failure and every 50\n",
    "        if games_failed == 1 or games_failed % 50 == 0:\n",
    "            print(f\"Failures: {games_failed}\")\n",
    "        continue\n",
    "\n",
    "# Combine into DFs\n",
    "if all_fh_player_stats:\n",
    "    fh_players_combined = pd.concat(all_fh_player_stats, ignore_index=True)\n",
    "    fh_teams_combined = pd.concat(all_fh_team_stats, ignore_index=True)\n",
    "    fh_starters_bench_combined = pd.concat(all_fh_starter_bench, ignore_index=True)\n",
    "    \n",
    "    # Save combined dataframes with suffix\n",
    "    fh_players_combined.to_csv(f'first_half_players{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    fh_teams_combined.to_csv(f'first_half_teams{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    fh_starters_bench_combined.to_csv(f'first_half_starters_bench{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    \n",
    "    print(f\"Successfully processed: {games_processed}\")\n",
    "    print(f\"Player records: {len(fh_players_combined)}\")\n",
    "    print(f\"Team records: {len(fh_teams_combined)}\")\n",
    "else:\n",
    "    print(\"\\nNo data collected - all games failed\")\n",
    "\n",
    "# Save failed game IDs\n",
    "if failed_game_ids:\n",
    "    failed_df = pd.DataFrame({'GAME_ID': failed_game_ids})\n",
    "    failed_df.to_csv(f'failed_game_ids_fh_pass{PASS_NUMBER}.csv', index=False)\n",
    "    print(f\"\\nFailed game IDs saved to 'failed_game_ids_fh_pass{PASS_NUMBER}.csv'\")\n",
    "    print(f\"Failed games: {len(failed_game_ids)}\")\n",
    "    print(f\"\\nTo run pass {PASS_NUMBER + 1}:\")\n",
    "else:\n",
    "    print(f\"\\n All games successful - no need for pass {PASS_NUMBER + 1}!\")\n",
    "\n",
    "# Final Summary\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"PASS {PASS_NUMBER} COMPLETE\")\n",
    "print(f\"Successfully processed: {games_processed}/{len(game_ids_list)}\")\n",
    "print(f\"Failed: {games_failed}/{len(game_ids_list)}\")\n",
    "print(f\"Total time: {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84affc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "=== STARTING PASS 1 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 7807\n",
      "First game ID: 0012000001\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Progress: 1000/7807 games (747 success, 253 failed) - 120.6 min elapsed\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Progress: 2000/7807 games (1464 success, 536 failed) - 240.6 min elapsed\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Progress: 3000/7807 games (2178 success, 822 failed) - 360.6 min elapsed\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚úÖ Pass 1 data saved!\n",
      "Successfully processed: 5054\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass1.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 1 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 5054/7807\n",
      "Failed: 2753/7807\n",
      "Total time: 832.5 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 2 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 2 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚ùå Pass 2: No data collected - all games failed\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass2.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 2 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 0/2753\n",
      "Failed: 2753/2753\n",
      "Total time: 0.0 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 3 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 3 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚ùå Pass 3: No data collected - all games failed\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass3.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 3 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 0/2753\n",
      "Failed: 2753/2753\n",
      "Total time: 0.0 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 4 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 4 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚ùå Pass 4: No data collected - all games failed\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass4.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 4 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 0/2753\n",
      "Failed: 2753/2753\n",
      "Total time: 0.0 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 5 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 5 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚ùå Pass 5: No data collected - all games failed\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass5.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 5 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 0/2753\n",
      "Failed: 2753/2753\n",
      "Total time: 0.0 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 6 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 6 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "Failures: 1350\n",
      "Failures: 1400\n",
      "Failures: 1450\n",
      "Failures: 1500\n",
      "Failures: 1550\n",
      "Failures: 1600\n",
      "Failures: 1650\n",
      "Failures: 1700\n",
      "Failures: 1750\n",
      "Failures: 1800\n",
      "Failures: 1850\n",
      "Failures: 1900\n",
      "Failures: 1950\n",
      "Failures: 2000\n",
      "Failures: 2050\n",
      "Failures: 2100\n",
      "Failures: 2150\n",
      "Failures: 2200\n",
      "Failures: 2250\n",
      "Failures: 2300\n",
      "Failures: 2350\n",
      "Failures: 2400\n",
      "Failures: 2450\n",
      "Failures: 2500\n",
      "Failures: 2550\n",
      "Failures: 2600\n",
      "Failures: 2650\n",
      "Failures: 2700\n",
      "Failures: 2750\n",
      "\n",
      "‚ùå Pass 6: No data collected - all games failed\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass6.csv'\n",
      "Failed games: 2753\n",
      "\n",
      "============================================================\n",
      "PASS 6 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 0/2753\n",
      "Failed: 2753/2753\n",
      "Total time: 0.0 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 7 with 2753 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 7 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 2753\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "Failures: 300\n",
      "Failures: 350\n",
      "Failures: 400\n",
      "Failures: 450\n",
      "Failures: 500\n",
      "Failures: 550\n",
      "Failures: 600\n",
      "Failures: 650\n",
      "Failures: 700\n",
      "Failures: 750\n",
      "Failures: 800\n",
      "Failures: 850\n",
      "Failures: 900\n",
      "Failures: 950\n",
      "Failures: 1000\n",
      "Failures: 1050\n",
      "Failures: 1100\n",
      "Failures: 1150\n",
      "Failures: 1200\n",
      "Failures: 1250\n",
      "Failures: 1300\n",
      "\n",
      "‚úÖ Pass 7 data saved!\n",
      "Successfully processed: 1444\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass7.csv'\n",
      "Failed games: 1309\n",
      "\n",
      "============================================================\n",
      "PASS 7 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 1444/2753\n",
      "Failed: 1309/2753\n",
      "Total time: 491.8 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 8 with 1309 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 8 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 1309\n",
      "First game ID: 0022000345\n",
      "Last game ID: 2072100016\n",
      "Failures: 1\n",
      "Failures: 50\n",
      "Failures: 100\n",
      "Failures: 150\n",
      "Failures: 200\n",
      "Failures: 250\n",
      "\n",
      "‚úÖ Pass 8 data saved!\n",
      "Successfully processed: 1016\n",
      "‚ö†Ô∏è  Failed game IDs saved to 'failed_game_ids_bss_pass8.csv'\n",
      "Failed games: 293\n",
      "\n",
      "============================================================\n",
      "PASS 8 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 1016/1309\n",
      "Failed: 293/1309\n",
      "Total time: 124.4 minutes\n",
      "\n",
      "‚è≠Ô∏è  Will attempt pass 9 with 293 failed games...\n",
      "\n",
      "============================================================\n",
      "=== STARTING PASS 9 ===\n",
      "============================================================\n",
      "\n",
      "Total games to process: 293\n",
      "First game ID: 2052100036\n",
      "Last game ID: 2052200242\n",
      "\n",
      "‚úÖ Pass 9 data saved!\n",
      "Successfully processed: 293\n",
      "üéâ Pass 9: All games successful!\n",
      "\n",
      "============================================================\n",
      "PASS 9 COMPLETE\n",
      "============================================================\n",
      "Successfully processed: 293/293\n",
      "Failed: 0/293\n",
      "Total time: 4.2 minutes\n",
      "\n",
      "üéâüéâ ALL GAMES PROCESSED SUCCESSFULLY! No need for more passes.\n",
      "\n",
      "============================================================\n",
      "ALL PASSES COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Getting Box Score Summary Data - Multi-Pass with Auto-Loop Version (with GAME_ID fix)\n",
    "from nba_api.stats.endpoints import boxscoresummaryv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MAX_PASSES = 15\n",
    "INITIAL_INPUT_FILE = 'unique_game_ids.csv'\n",
    "\n",
    "# Start with pass 1\n",
    "for pass_num in range(1, MAX_PASSES + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== STARTING PASS {pass_num} ===\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Determine input file\n",
    "    if pass_num == 1:\n",
    "        input_file = INITIAL_INPUT_FILE\n",
    "    else:\n",
    "        input_file = f'failed_game_ids_bss_pass{pass_num-1}.csv'\n",
    "        \n",
    "        # Check if there are failures to process\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"‚úÖ No failures from pass {pass_num-1} - All done!\")\n",
    "            break\n",
    "    \n",
    "    # Loading Game IDs\n",
    "    game_ids_df = pd.read_csv(input_file, dtype={'GAME_ID': str})\n",
    "    game_ids_list = game_ids_df['GAME_ID'].tolist()\n",
    "    \n",
    "    # Output suffix\n",
    "    output_suffix = '' if pass_num == 1 else f'_pt{pass_num}'\n",
    "    \n",
    "    # Initial Print Statements\n",
    "    print(f\"Total games to process: {len(game_ids_list)}\")\n",
    "    print(f\"First game ID: {game_ids_list[0]}\")\n",
    "    print(f\"Last game ID: {game_ids_list[-1]}\")\n",
    "    \n",
    "    # Lists to collect dataframes\n",
    "    all_game_summary = []\n",
    "    all_team_stats = []\n",
    "    all_refs = []\n",
    "    all_inactive = []\n",
    "    all_game_info = []\n",
    "    all_points_by_qtr = []\n",
    "    all_last_meeting = []\n",
    "    \n",
    "    # Variables to track progress\n",
    "    games_processed = 0\n",
    "    games_failed = 0\n",
    "    start_time = time.time()\n",
    "    failed_game_ids = []\n",
    "    \n",
    "    # Loop through games\n",
    "    for idx, game_id in enumerate(game_ids_list):\n",
    "        try:\n",
    "            # API function\n",
    "            summary = boxscoresummaryv2.BoxScoreSummaryV2(game_id=game_id)\n",
    "            \n",
    "            # Get all dataframes\n",
    "            dfs = summary.get_data_frames()\n",
    "            \n",
    "            # Add GAME_ID to each dataframe if it doesn't exist\n",
    "            for i in range(7):  # Only process 0-6, skip 7-8\n",
    "                if 'GAME_ID' not in dfs[i].columns:\n",
    "                    dfs[i]['GAME_ID'] = game_id\n",
    "            \n",
    "            # Append only the ones we want (0-6, drop 7-8)\n",
    "            all_game_summary.append(dfs[0])\n",
    "            all_team_stats.append(dfs[1])\n",
    "            all_refs.append(dfs[2])\n",
    "            all_inactive.append(dfs[3])\n",
    "            all_game_info.append(dfs[4])\n",
    "            all_points_by_qtr.append(dfs[5])\n",
    "            all_last_meeting.append(dfs[6])\n",
    "            \n",
    "            games_processed += 1\n",
    "            \n",
    "            # Progress Update every 1000 games\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Progress: {idx + 1}/{len(game_ids_list)} games ({games_processed} success, {games_failed} failed) - {elapsed/60:.1f} min elapsed\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            games_failed += 1\n",
    "            failed_game_ids.append(game_id)\n",
    "            if games_failed == 1 or games_failed % 50 == 0:\n",
    "                print(f\"Failures: {games_failed}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine into DFs\n",
    "    if all_game_summary:\n",
    "        game_summary_combined = pd.concat(all_game_summary, ignore_index=True)\n",
    "        team_stats_combined = pd.concat(all_team_stats, ignore_index=True)\n",
    "        refs_combined = pd.concat(all_refs, ignore_index=True)\n",
    "        inactive_combined = pd.concat(all_inactive, ignore_index=True)\n",
    "        game_info_combined = pd.concat(all_game_info, ignore_index=True)\n",
    "        points_qtr_combined = pd.concat(all_points_by_qtr, ignore_index=True)\n",
    "        last_meeting_combined = pd.concat(all_last_meeting, ignore_index=True)\n",
    "        \n",
    "        # Save combined dataframes\n",
    "        game_summary_combined.to_csv(f'game_summary{output_suffix}.csv', index=False)\n",
    "        team_stats_combined.to_csv(f'team_stats{output_suffix}.csv', index=False)\n",
    "        refs_combined.to_csv(f'refs{output_suffix}.csv', index=False)\n",
    "        inactive_combined.to_csv(f'inactive_players{output_suffix}.csv', index=False)\n",
    "        game_info_combined.to_csv(f'game_info{output_suffix}.csv', index=False)\n",
    "        points_qtr_combined.to_csv(f'points_by_quarter{output_suffix}.csv', index=False)\n",
    "        last_meeting_combined.to_csv(f'last_meeting{output_suffix}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nPass {pass_num} data saved!\")\n",
    "        print(f\"Successfully processed: {games_processed}\")\n",
    "    else:\n",
    "        print(f\"\\nPass {pass_num}: No data collected - all games failed\")\n",
    "    \n",
    "    # Save failed game IDs\n",
    "    if failed_game_ids:\n",
    "        failed_df = pd.DataFrame({'GAME_ID': failed_game_ids})\n",
    "        failed_df.to_csv(f'failed_game_ids_bss_pass{pass_num}.csv', index=False)\n",
    "        print(f\"Failed game IDs saved to 'failed_game_ids_bss_pass{pass_num}.csv'\")\n",
    "        print(f\"Failed games: {len(failed_game_ids)}\")\n",
    "    else:\n",
    "        print(f\"Pass {pass_num}: All games successful!\")\n",
    "    \n",
    "    # Final Summary for this pass\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"PASS {pass_num} COMPLETE\")\n",
    "    print(f\"Successfully processed: {games_processed}/{len(game_ids_list)}\")\n",
    "    print(f\"Failed: {games_failed}/{len(game_ids_list)}\")\n",
    "    print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    # Check if we should continue to next pass\n",
    "    if not failed_game_ids:\n",
    "        print(f\"\\nALL GAMES PROCESSED SUCCESSFULLY! No need for more passes.\")\n",
    "        break\n",
    "    elif pass_num < MAX_PASSES:\n",
    "        print(f\"\\nWill attempt pass {pass_num + 1} with {len(failed_game_ids)} failed games...\")\n",
    "        time.sleep(5)  # Brief pause between passes\n",
    "    else:\n",
    "        print(f\"\\nReached maximum passes ({MAX_PASSES}). {len(failed_game_ids)} games still failed.\")\n",
    "\n",
    "print(f\"ALL PASSES COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd268331",
   "metadata": {},
   "source": [
    "This next cell block is code to concatenate all the various pass files together. This could have been more elegant but I hard coded the amount of passes I had for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dbed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONCATENATING MULTI-PASS CSV FILES ===\n",
      "\n",
      "Processing First Half files (3 passes)...\n",
      "  ‚úÖ Loaded first_half_players.csv: 120187 records\n",
      "  ‚úÖ Loaded first_half_players_pt2.csv: 22239 records\n",
      "  ‚úÖ Loaded first_half_players_pt3.csv: 5716 records\n",
      "  üìä Combined first_half_players: 148142 total records\n",
      "\n",
      "  ‚úÖ Loaded first_half_teams.csv: 12656 records\n",
      "  ‚úÖ Loaded first_half_teams_pt2.csv: 2354 records\n",
      "  ‚úÖ Loaded first_half_teams_pt3.csv: 600 records\n",
      "  üìä Combined first_half_teams: 15610 total records\n",
      "\n",
      "  ‚úÖ Loaded first_half_starters_bench.csv: 25312 records\n",
      "  ‚úÖ Loaded first_half_starters_bench_pt2.csv: 4708 records\n",
      "  ‚úÖ Loaded first_half_starters_bench_pt3.csv: 1200 records\n",
      "  üìä Combined first_half_starters_bench: 31220 total records\n",
      "\n",
      "Processing Complete Game files (5 passes)...\n",
      "  ‚úÖ Loaded complete_game_players.csv: 142397 records\n",
      "  ‚úÖ Loaded complete_game_players_pt2.csv: 43648 records\n",
      "  ‚úÖ Loaded complete_game_players_pt3.csv: 10429 records\n",
      "  ‚úÖ Loaded complete_game_players_pt4.csv: 1537 records\n",
      "  ‚úÖ Loaded complete_game_players_pt5.csv: 2370 records\n",
      "  üìä Combined complete_game_players: 200381 total records\n",
      "\n",
      "  ‚úÖ Loaded complete_game_teams.csv: 11156 records\n",
      "  ‚úÖ Loaded complete_game_teams_pt2.csv: 3310 records\n",
      "  ‚úÖ Loaded complete_game_teams_pt3.csv: 840 records\n",
      "  ‚úÖ Loaded complete_game_teams_pt4.csv: 134 records\n",
      "  ‚úÖ Loaded complete_game_teams_pt5.csv: 172 records\n",
      "  üìä Combined complete_game_teams: 15612 total records\n",
      "\n",
      "  ‚úÖ Loaded complete_game_starters_bench.csv: 22310 records\n",
      "  ‚úÖ Loaded complete_game_starters_bench_pt2.csv: 6619 records\n",
      "  ‚úÖ Loaded complete_game_starters_bench_pt3.csv: 1680 records\n",
      "  ‚úÖ Loaded complete_game_starters_bench_pt4.csv: 268 records\n",
      "  ‚úÖ Loaded complete_game_starters_bench_pt5.csv: 344 records\n",
      "  üìä Combined complete_game_starters_bench: 31221 total records\n",
      "\n",
      "Processing BoxScoreSummary files (9 passes)...\n",
      "  ‚úÖ Loaded game_summary.csv: 5098 records\n",
      "  ‚úÖ Loaded game_summary_pt2.csv: 1474 records\n",
      "  ‚úÖ Loaded game_summary_pt3.csv: 431 records\n",
      "  ‚úÖ Loaded game_summary_pt4.csv: 76 records\n",
      "  ‚úÖ Loaded game_summary_pt5.csv: 29 records\n",
      "  ‚ö†Ô∏è  game_summary_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded game_summary_pt7.csv: 1444 records\n",
      "  ‚úÖ Loaded game_summary_pt8.csv: 1025 records\n",
      "  ‚úÖ Loaded game_summary_pt9.csv: 293 records\n",
      "  üìä Combined game_summary: 9870 total records\n",
      "\n",
      "  ‚úÖ Loaded team_stats.csv: 10102 records\n",
      "  ‚úÖ Loaded team_stats_pt2.csv: 2948 records\n",
      "  ‚úÖ Loaded team_stats_pt3.csv: 862 records\n",
      "  ‚úÖ Loaded team_stats_pt4.csv: 152 records\n",
      "  ‚úÖ Loaded team_stats_pt5.csv: 58 records\n",
      "  ‚ö†Ô∏è  team_stats_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded team_stats_pt7.csv: 2888 records\n",
      "  ‚úÖ Loaded team_stats_pt8.csv: 2032 records\n",
      "  ‚úÖ Loaded team_stats_pt9.csv: 582 records\n",
      "  üìä Combined team_stats: 19624 total records\n",
      "\n",
      "  ‚úÖ Loaded refs.csv: 15323 records\n",
      "  ‚úÖ Loaded refs_pt2.csv: 4486 records\n",
      "  ‚úÖ Loaded refs_pt3.csv: 1293 records\n",
      "  ‚úÖ Loaded refs_pt4.csv: 228 records\n",
      "  ‚úÖ Loaded refs_pt5.csv: 87 records\n",
      "  ‚ö†Ô∏è  refs_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded refs_pt7.csv: 4332 records\n",
      "  ‚úÖ Loaded refs_pt8.csv: 3091 records\n",
      "  ‚úÖ Loaded refs_pt9.csv: 879 records\n",
      "  üìä Combined refs: 29719 total records\n",
      "\n",
      "  ‚úÖ Loaded inactive_players.csv: 30673 records\n",
      "  ‚úÖ Loaded inactive_players_pt2.csv: 8189 records\n",
      "  ‚úÖ Loaded inactive_players_pt3.csv: 2807 records\n",
      "  ‚úÖ Loaded inactive_players_pt4.csv: 156 records\n",
      "  ‚úÖ Loaded inactive_players_pt5.csv: 43 records\n",
      "  ‚ö†Ô∏è  inactive_players_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded inactive_players_pt7.csv: 10076 records\n",
      "  ‚úÖ Loaded inactive_players_pt8.csv: 4202 records\n",
      "  ‚úÖ Loaded inactive_players_pt9.csv: 504 records\n",
      "  üìä Combined inactive_players: 56650 total records\n",
      "\n",
      "  ‚úÖ Loaded game_info.csv: 5053 records\n",
      "  ‚úÖ Loaded game_info_pt2.csv: 1474 records\n",
      "  ‚úÖ Loaded game_info_pt3.csv: 431 records\n",
      "  ‚úÖ Loaded game_info_pt4.csv: 76 records\n",
      "  ‚úÖ Loaded game_info_pt5.csv: 29 records\n",
      "  ‚ö†Ô∏è  game_info_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded game_info_pt7.csv: 1444 records\n",
      "  ‚úÖ Loaded game_info_pt8.csv: 1013 records\n",
      "  ‚úÖ Loaded game_info_pt9.csv: 293 records\n",
      "  üìä Combined game_info: 9813 total records\n",
      "\n",
      "  ‚úÖ Loaded points_by_quarter.csv: 10102 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt2.csv: 2948 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt3.csv: 862 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt4.csv: 152 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt5.csv: 58 records\n",
      "  ‚ö†Ô∏è  points_by_quarter_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded points_by_quarter_pt7.csv: 2888 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt8.csv: 2026 records\n",
      "  ‚úÖ Loaded points_by_quarter_pt9.csv: 586 records\n",
      "  üìä Combined points_by_quarter: 19622 total records\n",
      "\n",
      "  ‚úÖ Loaded last_meeting.csv: 4982 records\n",
      "  ‚úÖ Loaded last_meeting_pt2.csv: 1281 records\n",
      "  ‚úÖ Loaded last_meeting_pt3.csv: 411 records\n",
      "  ‚úÖ Loaded last_meeting_pt4.csv: 0 records\n",
      "  ‚úÖ Loaded last_meeting_pt5.csv: 0 records\n",
      "  ‚ö†Ô∏è  last_meeting_pt6.csv not found, skipping\n",
      "  ‚úÖ Loaded last_meeting_pt7.csv: 1436 records\n",
      "  ‚úÖ Loaded last_meeting_pt8.csv: 561 records\n",
      "  ‚úÖ Loaded last_meeting_pt9.csv: 0 records\n",
      "  üìä Combined last_meeting: 8671 total records\n",
      "\n",
      "============================================================\n",
      "SAVING COMBINED FILES\n",
      "============================================================\n",
      "‚úÖ Saved first_half_players_COMBINED.csv: 148142 records\n",
      "‚úÖ Saved first_half_teams_COMBINED.csv: 15610 records\n",
      "‚úÖ Saved first_half_starters_bench_COMBINED.csv: 31220 records\n",
      "‚úÖ Saved complete_game_players_COMBINED.csv: 200381 records\n",
      "‚úÖ Saved complete_game_teams_COMBINED.csv: 15612 records\n",
      "‚úÖ Saved complete_game_starters_bench_COMBINED.csv: 31221 records\n",
      "‚úÖ Saved game_summary_COMBINED.csv: 9870 records\n",
      "‚úÖ Saved team_stats_COMBINED.csv: 19624 records\n",
      "‚úÖ Saved refs_COMBINED.csv: 29719 records\n",
      "‚úÖ Saved inactive_players_COMBINED.csv: 56650 records\n",
      "‚úÖ Saved game_info_COMBINED.csv: 9813 records\n",
      "‚úÖ Saved points_by_quarter_COMBINED.csv: 19622 records\n",
      "‚úÖ Saved last_meeting_COMBINED.csv: 8671 records\n",
      "\n",
      "üéâ All files combined and saved!\n",
      "Total combined datasets: 13\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all the csvs\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dictionary to store all combined dataframes\n",
    "combined_data = {}\n",
    "\n",
    "# First Half files (3 passes)\n",
    "fh_files = [\n",
    "    ('first_half_players', 3),\n",
    "    ('first_half_teams', 3),\n",
    "    ('first_half_starters_bench', 3)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in fh_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\" Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# Complete Game files (5 passes)\n",
    "cg_files = [\n",
    "    ('complete_game_players', 5),\n",
    "    ('complete_game_teams', 5),\n",
    "    ('complete_game_starters_bench', 5)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in cg_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# BoxScoreSummary files (9 passes)\n",
    "bss_files = [\n",
    "    ('game_summary', 9),\n",
    "    ('team_stats', 9),\n",
    "    ('refs', 9),\n",
    "    ('inactive_players', 9),\n",
    "    ('game_info', 9),\n",
    "    ('points_by_quarter', 9),\n",
    "    ('last_meeting', 9)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in bss_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"  üìä Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# Save all combined files\n",
    "print(\"SAVING COMBINED FILES\")\n",
    "\n",
    "for name, df in combined_data.items():\n",
    "    output_filename = f'{name}_COMBINED.csv'\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved {output_filename}: {len(df)} records\")\n",
    "\n",
    "print(f\"\\nAll files combined and saved!\")\n",
    "print(f\"Total combined datasets: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6081a",
   "metadata": {},
   "source": [
    "This next cell block does a quick pass to check if we have a complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254500da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING FOR MISSING GAMES ===\n",
      "\n",
      "Total games expected: 7807\n",
      "First game ID: 0012000001\n",
      "Last game ID: 2072100016\n",
      "\n",
      "============================================================\n",
      "\n",
      "FIRST HALF DATASETS:\n",
      "  first_half_players_COMBINED.csv:\n",
      "    Games found: 7805/7807 (100.0%)\n",
      "    Missing: 5408\n",
      "  first_half_teams_COMBINED.csv:\n",
      "    Games found: 7805/7807 (100.0%)\n",
      "    Missing: 5408\n",
      "  first_half_starters_bench_COMBINED.csv:\n",
      "    Games found: 7805/7807 (100.0%)\n",
      "    Missing: 5408\n",
      "\n",
      "============================================================\n",
      "\n",
      "COMPLETE GAME DATASETS:\n",
      "  complete_game_players_COMBINED.csv:\n",
      "    Games found: 7807/7807 (100.0%)\n",
      "    Missing: 5407\n",
      "  complete_game_teams_COMBINED.csv:\n",
      "    Games found: 7807/7807 (100.0%)\n",
      "    Missing: 5407\n",
      "  complete_game_starters_bench_COMBINED.csv:\n",
      "    Games found: 7807/7807 (100.0%)\n",
      "    Missing: 5407\n",
      "\n",
      "============================================================\n",
      "\n",
      "BOX SCORE SUMMARY DATASETS:\n",
      "  game_summary_COMBINED.csv:\n",
      "    Games found: 7801/7807 (99.9%)\n",
      "    Missing: 5410\n",
      "  team_stats_COMBINED.csv:\n",
      "    Games found: 7803/7807 (99.9%)\n",
      "    Missing: 7807\n",
      "  refs_COMBINED.csv:\n",
      "    Games found: 7807/7807 (100.0%)\n",
      "    Missing: 7807\n",
      "  inactive_players_COMBINED.csv:\n",
      "    Games found: 6758/7807 (86.6%)\n",
      "    Missing: 7807\n",
      "  game_info_COMBINED.csv:\n",
      "    Games found: 7803/7807 (99.9%)\n",
      "    Missing: 7807\n",
      "  points_by_quarter_COMBINED.csv:\n",
      "    Games found: 7802/7807 (99.9%)\n",
      "    Missing: 5411\n",
      "  last_meeting_COMBINED.csv:\n",
      "    Games found: 6961/7807 (89.2%)\n",
      "    Missing: 6220\n",
      "\n",
      "============================================================\n",
      "\n",
      "SUMMARY:\n",
      "Total expected games: 7807\n",
      "\n",
      "Best coverage: complete_game_players_COMBINED.csv\n",
      "  7807/7807 (100.0%)\n",
      "\n",
      "Worst coverage: inactive_players_COMBINED.csv\n",
      "  6758/7807 (86.6%)\n",
      "\n",
      "‚ö†Ô∏è  Games missing from ALL datasets: 5407\n",
      "Sample missing IDs: ['0022300816', '0042100104', '0022300047', '0022100208', '0022201215', '0042100155', '0022200853', '0022100006', '0022100169', '0012100048']\n",
      "‚úÖ Saved to 'games_missing_from_all_datasets.csv'\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check for missing games in combined datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the original game IDs list\n",
    "original_game_ids = pd.read_csv('unique_game_ids.csv', dtype={'GAME_ID': str})\n",
    "total_games = len(original_game_ids)\n",
    "all_game_ids = set(original_game_ids['GAME_ID'].tolist())\n",
    "\n",
    "print(f\"Total games expected: {total_games}\")\n",
    "print(f\"First game ID: {original_game_ids['GAME_ID'].iloc[0]}\")\n",
    "print(f\"Last game ID: {original_game_ids['GAME_ID'].iloc[-1]}\")\n",
    "\n",
    "# Dictionary to track coverage by dataset\n",
    "coverage_report = {}\n",
    "\n",
    "# Check First Half datasets\n",
    "print(\"FIRST HALF DATASETS:\")\n",
    "fh_files = [\n",
    "    'first_half_players_COMBINED.csv',\n",
    "    'first_half_teams_COMBINED.csv',\n",
    "    'first_half_starters_bench_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in fh_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {filename}:\")\n",
    "            print(f\"    Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"    Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "\n",
    "# Check Complete Game datasets\n",
    "print(\"COMPLETE GAME DATASETS:\")\n",
    "cg_files = [\n",
    "    'complete_game_players_COMBINED.csv',\n",
    "    'complete_game_teams_COMBINED.csv',\n",
    "    'complete_game_starters_bench_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in cg_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "# Check BoxScoreSummary datasets\n",
    "print(\"BOX SCORE SUMMARY DATASETS:\")\n",
    "bss_files = [\n",
    "    'game_summary_COMBINED.csv',\n",
    "    'team_stats_COMBINED.csv',\n",
    "    'refs_COMBINED.csv',\n",
    "    'inactive_players_COMBINED.csv',\n",
    "    'game_info_COMBINED.csv',\n",
    "    'points_by_quarter_COMBINED.csv',\n",
    "    'last_meeting_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in bss_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "# Summary\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"Total expected games: {total_games}\")\n",
    "\n",
    "if coverage_report:\n",
    "    best_coverage = max(coverage_report.items(), key=lambda x: x[1]['coverage_pct'])\n",
    "    worst_coverage = min(coverage_report.items(), key=lambda x: x[1]['coverage_pct'])\n",
    "    \n",
    "    print(f\"\\nBest coverage: {best_coverage[0]}\")\n",
    "    print(f\"  {best_coverage[1]['games_found']}/{total_games} ({best_coverage[1]['coverage_pct']:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nWorst coverage: {worst_coverage[0]}\")\n",
    "    print(f\"  {worst_coverage[1]['games_found']}/{total_games} ({worst_coverage[1]['coverage_pct']:.1f}%)\")\n",
    "    \n",
    "    # Find games missing from ALL datasets\n",
    "    all_missing = set.intersection(*[set(v['missing_ids']) for v in coverage_report.values()])\n",
    "    \n",
    "    if all_missing:\n",
    "        print(f\"\\nGames missing from ALL datasets: {len(all_missing)}\")\n",
    "        print(f\"Sample missing IDs: {list(all_missing)[:10]}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        missing_df = pd.DataFrame({'GAME_ID': sorted(all_missing)})\n",
    "        missing_df.to_csv('games_missing_from_all_datasets.csv', index=False)\n",
    "        print(f\" Saved to 'games_missing_from_all_datasets.csv'\")\n",
    "    else:\n",
    "        print(f\"\\nNo games are missing from ALL datasets!\")\n",
    "        print(\"(Some datasets may have missing games, but coverage varies)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368816b",
   "metadata": {},
   "source": [
    "Upon inspecting that data we had thoughts about pulling more specific player information but this was incredibly cumbersome with the API since we would have to make individual pulls on each PLAYER_ID. In thinking about it more it did not make sense to go through the trouble since we want the model to generalize well since we're using an old dataset and players move teams, players retire, and new players arrive.\n",
    "\n",
    "The next cell block is for feature engineering. It is a bit of a mess and desperately needs to be refactored. We added more code snippets to add more features to the datasets. The need for more opponent features created another nested loop which felt easier at the time but simply kept growing it would have been better to replace it with a function. Also the dataset got to be over 1,000 columns and I added some redundant columns because I forgot which ones were already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e4672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0/15613\n",
      "Progress: 500/15613\n",
      "Progress: 1000/15613\n",
      "Progress: 1500/15613\n",
      "Progress: 2000/15613\n",
      "Progress: 2500/15613\n",
      "Progress: 3000/15613\n",
      "Progress: 3500/15613\n",
      "Progress: 4000/15613\n",
      "Progress: 4500/15613\n",
      "Progress: 5000/15613\n",
      "Progress: 5500/15613\n",
      "Progress: 6000/15613\n",
      "Progress: 6500/15613\n",
      "Progress: 7000/15613\n",
      "Progress: 7500/15613\n",
      "Progress: 8000/15613\n",
      "Progress: 8500/15613\n",
      "Progress: 9000/15613\n",
      "Progress: 9500/15613\n",
      "Progress: 10000/15613\n",
      "Progress: 10500/15613\n",
      "Progress: 11000/15613\n",
      "Progress: 11500/15613\n",
      "Progress: 12000/15613\n",
      "Progress: 12500/15613\n",
      "Progress: 13000/15613\n",
      "Progress: 13500/15613\n",
      "Progress: 14000/15613\n",
      "Progress: 14500/15613\n",
      "Progress: 15000/15613\n",
      "Progress: 15500/15613\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "time_windows = [7, 14, 30] # 1 week, 2 week, and 1 month rolling snapshots\n",
    "player_groups = [1, 2, 3, 4, 5, 6, 7, 'rest'] #top 7 of the rotation averages and lump sum for rest of team\n",
    "\n",
    "# Stats to average\n",
    "player_stats_avg = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "player_stats_total = ['MIN']\n",
    "\n",
    "# Team Complete Game\n",
    "team_stats_avg_cg = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "team_stats_total_cg = ['MIN']\n",
    "\n",
    "# Team First Half\n",
    "team_stats_avg_fh = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "team_stats_total_fh = ['MIN']\n",
    "\n",
    "# Load our data\n",
    "cg_players = pd.read_csv('complete_game_players_COMBINED.csv')\n",
    "cg_teams = pd.read_csv('complete_game_teams_COMBINED.csv')\n",
    "fh_players = pd.read_csv('first_half_players_COMBINED.csv')\n",
    "fh_teams = pd.read_csv('first_half_teams_COMBINED.csv')\n",
    "games_master = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "game_summary = pd.read_csv('game_summary_COMBINED.csv')\n",
    "refs = pd.read_csv('refs_COMBINED.csv')\n",
    "last_meeting = pd.read_csv('last_meeting_COMBINED.csv')\n",
    "\n",
    "# Cast numeric for the above\n",
    "for col in player_stats_avg + player_stats_total:\n",
    "    if col in cg_players.columns:\n",
    "        cg_players[col] = pd.to_numeric(cg_players[col], errors='coerce')\n",
    "        fh_players[col] = pd.to_numeric(fh_players[col], errors='coerce')\n",
    "\n",
    "for col in team_stats_avg + team_stats_total:\n",
    "    if col in cg_teams.columns:\n",
    "        cg_teams[col] = pd.to_numeric(cg_teams[col], errors='coerce')\n",
    "        fh_teams[col] = pd.to_numeric(fh_teams[col], errors='coerce')\n",
    "\n",
    "# Format Dates\n",
    "games_master['GAME_DATE'] = pd.to_datetime(games_master['GAME_DATE'])\n",
    "last_meeting['LAST_GAME_DATE_EST'] = pd.to_datetime(last_meeting['LAST_GAME_DATE_EST'], errors='coerce')\n",
    "\n",
    "\n",
    "# Creating an index with unique game_id and date\n",
    "game_date_map = games_master[['GAME_ID', 'GAME_DATE']].drop_duplicates('GAME_ID').set_index('GAME_ID')['GAME_DATE']\n",
    "\n",
    "# Apply it\n",
    "for df in [cg_players, fh_players, cg_teams, fh_teams]:\n",
    "    df['GAME_DATE'] = df['GAME_ID'].map(game_date_map)\n",
    "\n",
    "## Computing Second-Half Stats since we pulled first half and complete game\n",
    "# Merge Key\n",
    "cg_teams['merge_key'] = cg_teams['GAME_ID'].astype(str) + '_' + cg_teams['TEAM_ID'].astype(str)\n",
    "fh_teams['merge_key'] = fh_teams['GAME_ID'].astype(str) + '_' + fh_teams['TEAM_ID'].astype(str)\n",
    "\n",
    "# Merge Alignment\n",
    "merged = cg_teams.merge(\n",
    "    fh_teams[['merge_key', 'PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA', 'REB', 'AST', 'TO', 'STL', 'BLK']], \n",
    "    on='merge_key', \n",
    "    how='left',\n",
    "    suffixes=('', '_fh')\n",
    ")\n",
    "\n",
    "# Second-half stats to compute\n",
    "second_half_stats = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA', 'REB', 'AST', 'TO', 'STL', 'BLK']\n",
    "\n",
    "# Quick diff for counting stats\n",
    "for stat in second_half_stats:\n",
    "    cg_teams[f'second_half_{stat}'] = merged[stat] - merged[f'{stat}_fh']\n",
    "\n",
    "# Calculate percentage stats\n",
    "cg_teams['second_half_FG_PCT'] = np.where(\n",
    "    cg_teams['second_half_FGA'] > 0,\n",
    "    cg_teams['second_half_FGM'] / cg_teams['second_half_FGA'],\n",
    "    np.nan\n",
    ")\n",
    "cg_teams['second_half_FG3_PCT'] = np.where(\n",
    "    cg_teams['second_half_FG3A'] > 0,\n",
    "    cg_teams['second_half_FG3M'] / cg_teams['second_half_FG3A'],\n",
    "    np.nan\n",
    ")\n",
    "cg_teams['second_half_FT_PCT'] = np.where(\n",
    "    cg_teams['second_half_FTA'] > 0,\n",
    "    cg_teams['second_half_FTM'] / cg_teams['second_half_FTA'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Kill merge key since its unnessary now\n",
    "cg_teams.drop('merge_key', axis=1, inplace=True)\n",
    "fh_teams.drop('merge_key', axis=1, inplace=True)\n",
    "\n",
    "# Add it to the stats list\n",
    "team_stats_avg_cg.extend([f'second_half_{stat}' for stat in second_half_stats])\n",
    "team_stats_avg_cg.extend(['second_half_FG_PCT', 'second_half_FG3_PCT', 'second_half_FT_PCT'])\n",
    "\n",
    "## Players\n",
    "# Lasso stats into time windows\n",
    "def calculate_player_time_features(player_df, game_id, game_date, player_id, team_id, days):\n",
    "    window_start = game_date - pd.Timedelta(days=days)\n",
    "    \n",
    "    player_games = player_df[\n",
    "        (player_df['PLAYER_ID'] == player_id) &\n",
    "        (player_df['TEAM_ID'] == team_id) &\n",
    "        (player_df['GAME_DATE'] >= window_start) &\n",
    "        (player_df['GAME_DATE'] < game_date) &\n",
    "        (player_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    # Excludes inactivity\n",
    "    if len(player_games) == 0:\n",
    "        base_features = {stat: np.nan for stat in player_stats_total + player_stats_avg}\n",
    "        base_features['FG_PCT'] = np.nan\n",
    "        base_features['FG3_PCT'] = np.nan\n",
    "        base_features['FT_PCT'] = np.nan\n",
    "        return base_features\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Sum counting stats for percentages later\n",
    "    for stat in player_stats_total:\n",
    "        features[stat] = player_games[stat].sum()\n",
    "        \n",
    "    # Averages for features\n",
    "    for stat in player_stats_avg:\n",
    "        features[stat] = player_games[stat].mean()\n",
    "    \n",
    "    # Calculate percentages from underlying counting stats instead of averaging percentages\n",
    "    total_fgm = player_games['FGM'].sum()\n",
    "    total_fga = player_games['FGA'].sum()\n",
    "    total_fg3m = player_games['FG3M'].sum()\n",
    "    total_fg3a = player_games['FG3A'].sum()\n",
    "    total_ftm = player_games['FTM'].sum()\n",
    "    total_fta = player_games['FTA'].sum()\n",
    "    \n",
    "    features['FG_PCT'] = total_fgm / total_fga if total_fga > 0 else np.nan\n",
    "    features['FG3_PCT'] = total_fg3m / total_fg3a if total_fg3a > 0 else np.nan\n",
    "    features['FT_PCT'] = total_ftm / total_fta if total_fta > 0 else np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ranking our top players, which is done on a 30 day basis\n",
    "# 30 day basis will show dropoffs of top players.\n",
    "def get_team_top_players(player_df, game_id, game_date, team_id):\n",
    "    window_start = game_date - pd.Timedelta(days=30)\n",
    "    \n",
    "    team_players = player_df[\n",
    "        (player_df['TEAM_ID'] == team_id) &\n",
    "        (player_df['GAME_DATE'] >= window_start) &\n",
    "        (player_df['GAME_DATE'] < game_date) &\n",
    "        (player_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    # Excludes inactivity\n",
    "    if len(team_players) == 0:\n",
    "        return []\n",
    "    \n",
    "    player_minutes = team_players.groupby('PLAYER_ID')['MIN'].sum().sort_values(ascending=False)\n",
    "    return player_minutes.index.tolist()\n",
    "\n",
    "## Teamwide\n",
    "# Lasso stats into time windows\n",
    "def calculate_team_time_features(team_df, game_id, game_date, team_id, days, stats_avg, stats_total):\n",
    "    window_start = game_date - pd.Timedelta(days=days)\n",
    "    \n",
    "    team_games = team_df[\n",
    "        (team_df['TEAM_ID'] == team_id) &\n",
    "        (team_df['GAME_DATE'] >= window_start) &\n",
    "        (team_df['GAME_DATE'] < game_date) &\n",
    "        (team_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        base_features = {stat: np.nan for stat in stats_total + stats_avg}  # Changed\n",
    "        base_features['FG_PCT'] = np.nan\n",
    "        base_features['FG3_PCT'] = np.nan\n",
    "        base_features['FT_PCT'] = np.nan\n",
    "        base_features['WIN_PCT'] = np.nan\n",
    "        return base_features\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Sum totals\n",
    "    for stat in stats_total:  # Changed\n",
    "        features[stat] = team_games[stat].sum()\n",
    "    \n",
    "    # Average the averages (including second-half stats for CG only)\n",
    "    for stat in stats_avg:  # Changed\n",
    "        features[stat] = team_games[stat].mean()\n",
    "    \n",
    "    # Calculate TRUE percentages from summed makes/attempts\n",
    "    total_fgm = team_games['FGM'].sum()\n",
    "    total_fga = team_games['FGA'].sum()\n",
    "    total_fg3m = team_games['FG3M'].sum()\n",
    "    total_fg3a = team_games['FG3A'].sum()\n",
    "    total_ftm = team_games['FTM'].sum()\n",
    "    total_fta = team_games['FTA'].sum()\n",
    "    \n",
    "    features['FG_PCT'] = total_fgm / total_fga if total_fga > 0 else np.nan\n",
    "    features['FG3_PCT'] = total_fg3m / total_fg3a if total_fg3a > 0 else np.nan\n",
    "    features['FT_PCT'] = total_ftm / total_fta if total_fta > 0 else np.nan\n",
    "    \n",
    "    # Win percentage\n",
    "    team_game_ids = team_games['GAME_ID'].unique()\n",
    "    team_game_results = games_master[\n",
    "        (games_master['GAME_ID'].isin(team_game_ids)) &\n",
    "        (games_master['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    if len(team_game_results) > 0:\n",
    "        features['WIN_PCT'] = (team_game_results['WL'] == 'W').mean()\n",
    "    else:\n",
    "        features['WIN_PCT'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Functions for Schedule\n",
    "# Rest days, back-to-back games, and some general game density\n",
    "def calculate_schedule_features(games_master, game_id, game_date, team_id):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        features['days_since_last_game'] = np.nan\n",
    "        features['is_back_to_back'] = 0\n",
    "        features['games_in_last_3_days'] = 0\n",
    "        features['games_in_last_5_days'] = 0\n",
    "        features['games_in_last_7_days'] = 0\n",
    "    else:\n",
    "        # Here we compute last game and back to backs\n",
    "        last_game_date = team_games['GAME_DATE'].iloc[-1]\n",
    "        features['days_since_last_game'] = (game_date - last_game_date).days\n",
    "        features['is_back_to_back'] = 1 if features['days_since_last_game'] == 1 else 0\n",
    "        \n",
    "        # Game density\n",
    "        recent_games_3d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=3)]\n",
    "        recent_games_5d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=5)]\n",
    "        recent_games_7d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=7)]\n",
    "        \n",
    "        features['games_in_last_3_days'] = len(recent_games_3d)\n",
    "        features['games_in_last_5_days'] = len(recent_games_5d)\n",
    "        features['games_in_last_7_days'] = len(recent_games_7d)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Lets get home and road game lengths (streaks of home or away)\n",
    "def calculate_home_road_context(games_master, game_id, game_date, team_id):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Determine if current game is home\n",
    "    current_game = games_master[(games_master['GAME_ID'] == game_id) & (games_master['TEAM_ID'] == team_id)]\n",
    "    if len(current_game) > 0:\n",
    "        matchup = current_game['MATCHUP'].iloc[0]\n",
    "        features['is_home_game'] = 1 if 'vs.' in str(matchup) else 0 # vs and @ determined home vs away\n",
    "    else:\n",
    "        features['is_home_game'] = np.nan\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        # First game of season - count as game #1\n",
    "        features['home_stand_game_number'] = 1 if features['is_home_game'] == 1 else 0\n",
    "        features['road_trip_game_number'] = 1 if features['is_home_game'] == 0 else 0\n",
    "        return features\n",
    "    \n",
    "    # Count consecutive home or road games (BEFORE current game)\n",
    "    consecutive_count = 0\n",
    "    \n",
    "    for _, game in team_games.iloc[::-1].iterrows():\n",
    "        matchup = str(game['MATCHUP'])\n",
    "        is_home = 1 if 'vs.' in matchup else 0\n",
    "        \n",
    "        # Check if this past game matches current game's location\n",
    "        if features['is_home_game'] == is_home:\n",
    "            consecutive_count += 1\n",
    "        else:\n",
    "            break  # Different arena, stop counting\n",
    "    \n",
    "    # Add 1 to include the CURRENT game\n",
    "    features['home_stand_game_number'] = consecutive_count + 1 if features['is_home_game'] == 1 else 0\n",
    "    features['road_trip_game_number'] = consecutive_count + 1 if features['is_home_game'] == 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Record Features\n",
    "# Win and Loss streaks\n",
    "def calculate_streak_features(games_master, game_id, game_date, team_id, lookback_games=[3, 5, 10]):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        features['current_streak'] = 0\n",
    "        for n in lookback_games:\n",
    "            features[f'wins_last_{n}'] = np.nan\n",
    "        features['home_win_pct_last_10'] = np.nan\n",
    "        features['road_win_pct_last_10'] = np.nan\n",
    "        return features\n",
    "    \n",
    "    # Current streak (positive = wins, negative = losses)\n",
    "    recent_results = team_games['WL'].iloc[::-1].values\n",
    "    current_result = recent_results[0] if len(recent_results) > 0 else None\n",
    "    streak = 0\n",
    "    \n",
    "    for result in recent_results:\n",
    "        if result == current_result:\n",
    "            streak += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    features['current_streak'] = streak if current_result == 'W' else -streak\n",
    "    \n",
    "    # Wins in last N games\n",
    "    for n in lookback_games:\n",
    "        last_n = team_games.tail(n)\n",
    "        if len(last_n) > 0:\n",
    "            features[f'wins_last_{n}'] = (last_n['WL'] == 'W').sum()\n",
    "        else:\n",
    "            features[f'wins_last_{n}'] = np.nan\n",
    "    \n",
    "    # Home/Road splits\n",
    "    last_10 = team_games.tail(10)\n",
    "    home_games = last_10[last_10['MATCHUP'].str.contains('vs.', na=False)]\n",
    "    road_games = last_10[last_10['MATCHUP'].str.contains('@', na=False)]\n",
    "    \n",
    "    features['home_win_pct_last_10'] = (home_games['WL'] == 'W').mean() if len(home_games) > 0 else np.nan\n",
    "    features['road_win_pct_last_10'] = (road_games['WL'] == 'W').mean() if len(road_games) > 0 else np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Head to head record calculations (Team matched against their Opponent)\n",
    "def calculate_head_to_head_features(games_master, last_meeting, game_id, game_date, team_id):\n",
    "    features = {}\n",
    "    \n",
    "    # Get opponent team_id for this game\n",
    "    game_teams = games_master[games_master['GAME_ID'] == game_id]['TEAM_ID'].unique()\n",
    "    opponent_id = [t for t in game_teams if t != team_id]\n",
    "    \n",
    "    if len(opponent_id) == 0:\n",
    "        features['days_since_last_matchup'] = np.nan\n",
    "        features['won_last_matchup'] = np.nan\n",
    "        features['point_diff_last_matchup'] = np.nan\n",
    "        features['season_record_vs_opponent'] = np.nan\n",
    "        return features\n",
    "    \n",
    "    opponent_id = opponent_id[0]\n",
    "    \n",
    "    # Last meeting info\n",
    "    last_mtg = last_meeting[last_meeting['GAME_ID'] == game_id]\n",
    "    if len(last_mtg) > 0 and pd.notna(last_mtg['LAST_GAME_DATE_EST'].iloc[0]):\n",
    "        last_game_date = last_mtg['LAST_GAME_DATE_EST'].iloc[0]\n",
    "        features['days_since_last_matchup'] = (game_date - last_game_date).days\n",
    "        \n",
    "        # Determine who won\n",
    "        home_pts = last_mtg['LAST_GAME_HOME_TEAM_POINTS'].iloc[0]\n",
    "        visitor_pts = last_mtg['LAST_GAME_VISITOR_TEAM_POINTS'].iloc[0]\n",
    "        home_team = last_mtg['LAST_GAME_HOME_TEAM_ID'].iloc[0]\n",
    "        \n",
    "        if home_team == team_id:\n",
    "            features['won_last_matchup'] = 1 if home_pts > visitor_pts else 0\n",
    "            features['point_diff_last_matchup'] = home_pts - visitor_pts\n",
    "        else:\n",
    "            features['won_last_matchup'] = 1 if visitor_pts > home_pts else 0\n",
    "            features['point_diff_last_matchup'] = visitor_pts - home_pts\n",
    "    else:\n",
    "        features['days_since_last_matchup'] = np.nan\n",
    "        features['won_last_matchup'] = np.nan\n",
    "        features['point_diff_last_matchup'] = np.nan\n",
    "    \n",
    "    # Season record vs opponent\n",
    "    season_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ]\n",
    "    \n",
    "    # Find games against this opponent\n",
    "    opponent_game_ids = games_master[\n",
    "        (games_master['TEAM_ID'] == opponent_id)\n",
    "    ]['GAME_ID'].unique()\n",
    "    \n",
    "    matchup_games = season_games[season_games['GAME_ID'].isin(opponent_game_ids)]\n",
    "    \n",
    "    if len(matchup_games) > 0:\n",
    "        features['season_record_vs_opponent'] = (matchup_games['WL'] == 'W').sum()\n",
    "    else:\n",
    "        features['season_record_vs_opponent'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Referee Features -- basically a join\n",
    "def get_referee_ids(refs, game_id):\n",
    "    game_refs = refs[refs['GAME_ID'] == game_id].sort_values('JERSEY_NUM')\n",
    "    \n",
    "    ref_ids = [np.nan, np.nan, np.nan]\n",
    "    for i, (_, ref) in enumerate(game_refs.iterrows()):\n",
    "        if i < 3:\n",
    "            ref_ids[i] = ref['OFFICIAL_ID']\n",
    "    \n",
    "    return {\n",
    "        'ref_1_id': ref_ids[0],\n",
    "        'ref_2_id': ref_ids[1],\n",
    "        'ref_3_id': ref_ids[2]\n",
    "    }\n",
    "\n",
    "## Opponent features -- this is calculating features for the opponent\n",
    "def get_opponent_id(games_master, game_id, team_id):\n",
    "    \"\"\"Get the opponent team ID for this game\"\"\"\n",
    "    game_teams = games_master[games_master['GAME_ID'] == game_id]['TEAM_ID'].unique()\n",
    "    opponent_ids = [t for t in game_teams if t != team_id]\n",
    "    return opponent_ids[0] if len(opponent_ids) > 0 else None\n",
    "\n",
    "# Unique filter to make sure we are not counting duplicates\n",
    "unique_games = games_master[['GAME_ID', 'GAME_DATE', 'TEAM_ID']].drop_duplicates()\n",
    "unique_games = unique_games.sort_values('GAME_DATE')\n",
    "\n",
    "# Index Reset\n",
    "unique_games = unique_games.reset_index(drop=True)\n",
    "\n",
    "# Preparing to run it\n",
    "all_features = []\n",
    "\n",
    "# Looping over all games\n",
    "for idx, row in unique_games.iterrows():    \n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Progress: {idx}/{len(unique_games)}\")\n",
    "    \n",
    "    game_id = row['GAME_ID']\n",
    "    game_date = row['GAME_DATE']\n",
    "    team_id = row['TEAM_ID']\n",
    "    \n",
    "    game_features = {\n",
    "        'GAME_ID': game_id,\n",
    "        'GAME_DATE': game_date,\n",
    "        'TEAM_ID': team_id\n",
    "    }\n",
    "    \n",
    "    # Player features\n",
    "    top_players_cg = get_team_top_players(cg_players, game_id, game_date, team_id)\n",
    "    top_players_fh = get_team_top_players(fh_players, game_id, game_date, team_id)\n",
    "    \n",
    "    for days in time_windows:\n",
    "        # Complete game players\n",
    "        for n in player_groups:\n",
    "            if n == 'rest':\n",
    "                players = top_players_cg[7:] if len(top_players_cg) > 7 else []\n",
    "            else:\n",
    "                players = top_players_cg[:n] if len(top_players_cg) >= n else []\n",
    "            \n",
    "            if len(players) == 0:\n",
    "                for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                    game_features[f'top{n}_cg_{stat.lower()}_{days}d'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            player_features_list = [\n",
    "                calculate_player_time_features(cg_players, game_id, game_date, p, team_id, days)\n",
    "                for p in players\n",
    "            ]\n",
    "            \n",
    "            for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                values = [pf[stat] for pf in player_features_list if not pd.isna(pf[stat])]\n",
    "                game_features[f'top{n}_cg_{stat.lower()}_{days}d'] = np.mean(values) if values else np.nan\n",
    "        \n",
    "        # First half players\n",
    "        for n in player_groups:\n",
    "            if n == 'rest':\n",
    "                players = top_players_fh[7:] if len(top_players_fh) > 7 else []\n",
    "            else:\n",
    "                players = top_players_fh[:n] if len(top_players_fh) >= n else []\n",
    "            \n",
    "            if len(players) == 0:\n",
    "                for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                    game_features[f'top{n}_fh_{stat.lower()}_{days}d'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            player_features_list = [\n",
    "                calculate_player_time_features(fh_players, game_id, game_date, p, team_id, days)\n",
    "                for p in players\n",
    "            ]\n",
    "            \n",
    "            for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                values = [pf[stat] for pf in player_features_list if not pd.isna(pf[stat])]\n",
    "                game_features[f'top{n}_fh_{stat.lower()}_{days}d'] = np.mean(values) if values else np.nan\n",
    "    \n",
    "    # Team features\n",
    "    for days in time_windows:\n",
    "    # Complete game with second-half stats\n",
    "        team_cg = calculate_team_time_features(cg_teams, game_id, game_date, team_id, days, \n",
    "                                                team_stats_avg_cg, team_stats_total_cg)\n",
    "        for stat, value in team_cg.items():\n",
    "            game_features[f'team_cg_{stat.lower()}_{days}d'] = value\n",
    "        \n",
    "        # First half WITHOUT second-half stats\n",
    "        team_fh = calculate_team_time_features(fh_teams, game_id, game_date, team_id, days,\n",
    "                                                team_stats_avg_fh, team_stats_total_fh)\n",
    "        for stat, value in team_fh.items():\n",
    "            game_features[f'team_fh_{stat.lower()}_{days}d'] = value\n",
    "    \n",
    "    # Schedule and Record Featurres\n",
    "    schedule_feats = calculate_schedule_features(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(schedule_feats)\n",
    "    \n",
    "    home_road_feats = calculate_home_road_context(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(home_road_feats)\n",
    "    \n",
    "    streak_feats = calculate_streak_features(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(streak_feats)\n",
    "    \n",
    "    h2h_feats = calculate_head_to_head_features(games_master, last_meeting, game_id, game_date, team_id)\n",
    "    game_features.update(h2h_feats)\n",
    "    \n",
    "    # Ref features\n",
    "    ref_feats = get_referee_ids(refs, game_id)\n",
    "    game_features.update(ref_feats)\n",
    "    \n",
    "    # Current Game Stats columns\n",
    "    current_game_fh = fh_teams[\n",
    "        (fh_teams['GAME_ID'] == game_id) & \n",
    "        (fh_teams['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    \n",
    "    if len(current_game_fh) > 0:\n",
    "        game_features['current_fh_pts'] = current_game_fh['PTS'].iloc[0]\n",
    "        game_features['current_fh_fgm'] = current_game_fh['FGM'].iloc[0]\n",
    "        game_features['current_fh_fga'] = current_game_fh['FGA'].iloc[0]\n",
    "        game_features['current_fh_fg3m'] = current_game_fh['FG3M'].iloc[0]\n",
    "        game_features['current_fh_fg3a'] = current_game_fh['FG3A'].iloc[0]\n",
    "        game_features['current_fh_ftm'] = current_game_fh['FTM'].iloc[0]\n",
    "        game_features['current_fh_fta'] = current_game_fh['FTA'].iloc[0]\n",
    "        game_features['current_fh_reb'] = current_game_fh['REB'].iloc[0]\n",
    "        game_features['current_fh_ast'] = current_game_fh['AST'].iloc[0]\n",
    "        game_features['current_fh_to'] = current_game_fh['TO'].iloc[0]\n",
    "        game_features['current_fh_stl'] = current_game_fh['STL'].iloc[0]\n",
    "        game_features['current_fh_blk'] = current_game_fh['BLK'].iloc[0]\n",
    "        game_features['current_fh_pf'] = current_game_fh['PF'].iloc[0]\n",
    "        \n",
    "        # Calculate percentages and pace\n",
    "        fga = current_game_fh['FGA'].iloc[0]\n",
    "        fg3a = current_game_fh['FG3A'].iloc[0]\n",
    "        fta = current_game_fh['FTA'].iloc[0]\n",
    "        \n",
    "        game_features['current_fh_fg_pct'] = current_game_fh['FGM'].iloc[0] / fga if fga > 0 else np.nan\n",
    "        game_features['current_fh_fg3_pct'] = current_game_fh['FG3M'].iloc[0] / fg3a if fg3a > 0 else np.nan\n",
    "        game_features['current_fh_ft_pct'] = current_game_fh['FTM'].iloc[0] / fta if fta > 0 else np.nan\n",
    "        game_features['current_fh_pace'] = fga + current_game_fh['TO'].iloc[0]\n",
    "    else:\n",
    "        fh_stats = ['pts', 'fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'to', \n",
    "                    'stl', 'blk', 'pf', 'fg_pct', 'fg3_pct', 'ft_pct', 'pace']\n",
    "        for stat in fh_stats:\n",
    "            game_features[f'current_fh_{stat}'] = np.nan\n",
    "\n",
    "    ## Opponnet Features\n",
    "    opponent_id = get_opponent_id(games_master, game_id, team_id)\n",
    "    \n",
    "    if opponent_id is not None:\n",
    "        # Opponent team features for each time window\n",
    "        for days in time_windows:\n",
    "            opp_team_cg = calculate_team_time_features(cg_teams, game_id, game_date, opponent_id, days,\n",
    "                                                        team_stats_avg_cg, team_stats_total_cg)\n",
    "            for stat, value in opp_team_cg.items():\n",
    "                game_features[f'opp_team_cg_{stat.lower()}_{days}d'] = value\n",
    "            \n",
    "            opp_team_fh = calculate_team_time_features(fh_teams, game_id, game_date, opponent_id, days,\n",
    "                                                        team_stats_avg_fh, team_stats_total_fh)\n",
    "            for stat, value in opp_team_fh.items():\n",
    "                game_features[f'opp_team_fh_{stat.lower()}_{days}d'] = value\n",
    "        \n",
    "        # Opponent schedule features\n",
    "        opp_schedule_feats = calculate_schedule_features(games_master, game_id, game_date, opponent_id)\n",
    "        for key, value in opp_schedule_feats.items():\n",
    "            game_features[f'opp_{key}'] = value\n",
    "        \n",
    "        # Opponent streak features\n",
    "        opp_streak_feats = calculate_streak_features(games_master, game_id, game_date, opponent_id)\n",
    "        for key, value in opp_streak_feats.items():\n",
    "            game_features[f'opp_{key}'] = value\n",
    "            \n",
    "        # Team vs Opponent Differentials\n",
    "        if not pd.isna(game_features.get('days_since_last_game')) and not pd.isna(game_features.get('opp_days_since_last_game')):\n",
    "            game_features['rest_advantage'] = game_features['days_since_last_game'] - game_features['opp_days_since_last_game']\n",
    "        else:\n",
    "            game_features['rest_advantage'] = np.nan\n",
    "        \n",
    "        # Win percentage differential\n",
    "        for days in time_windows:\n",
    "            team_win_pct = game_features.get(f'team_cg_win_pct_{days}d', np.nan)\n",
    "            opp_win_pct = game_features.get(f'opp_team_cg_win_pct_{days}d', np.nan)\n",
    "            if not pd.isna(team_win_pct) and not pd.isna(opp_win_pct):\n",
    "                game_features[f'win_pct_diff_{days}d'] = team_win_pct - opp_win_pct\n",
    "            else:\n",
    "                game_features[f'win_pct_diff_{days}d'] = np.nan\n",
    "        \n",
    "        # Recent form differential (wins in last 5)\n",
    "        team_wins_5 = game_features.get('wins_last_5', np.nan)\n",
    "        opp_wins_5 = game_features.get('opp_wins_last_5', np.nan)\n",
    "        if not pd.isna(team_wins_5) and not pd.isna(opp_wins_5):\n",
    "            game_features['recent_form_diff'] = team_wins_5 - opp_wins_5\n",
    "        else:\n",
    "            game_features['recent_form_diff'] = np.nan\n",
    "        \n",
    "        # Scoring differential\n",
    "        for days in time_windows:\n",
    "            team_pts = game_features.get(f'team_cg_pts_{days}d', np.nan)\n",
    "            opp_pts = game_features.get(f'opp_team_cg_pts_{days}d', np.nan)\n",
    "            if not pd.isna(team_pts) and not pd.isna(opp_pts):\n",
    "                game_features[f'avg_scoring_diff_{days}d'] = team_pts - opp_pts\n",
    "            else:\n",
    "                game_features[f'avg_scoring_diff_{days}d'] = np.nan\n",
    "        \n",
    "        # Opponent Current Game Columns        \n",
    "        opp_game_fh = fh_teams[\n",
    "            (fh_teams['GAME_ID'] == game_id) & \n",
    "            (fh_teams['TEAM_ID'] == opponent_id)\n",
    "        ]\n",
    "        \n",
    "        if len(opp_game_fh) > 0:\n",
    "            game_features['opp_current_fh_pts'] = opp_game_fh['PTS'].iloc[0]\n",
    "            game_features['opp_current_fh_fgm'] = opp_game_fh['FGM'].iloc[0]\n",
    "            game_features['opp_current_fh_fga'] = opp_game_fh['FGA'].iloc[0]\n",
    "            game_features['opp_current_fh_fg3m'] = opp_game_fh['FG3M'].iloc[0]\n",
    "            game_features['opp_current_fh_fg3a'] = opp_game_fh['FG3A'].iloc[0]\n",
    "            game_features['opp_current_fh_reb'] = opp_game_fh['REB'].iloc[0]\n",
    "            game_features['opp_current_fh_ast'] = opp_game_fh['AST'].iloc[0]\n",
    "            game_features['opp_current_fh_to'] = opp_game_fh['TO'].iloc[0]\n",
    "            game_features['opp_current_fh_pf'] = opp_game_fh['PF'].iloc[0]\n",
    "            \n",
    "            opp_fga = opp_game_fh['FGA'].iloc[0]\n",
    "            opp_fg3a = opp_game_fh['FG3A'].iloc[0]\n",
    "            \n",
    "            game_features['opp_current_fh_fg_pct'] = opp_game_fh['FGM'].iloc[0] / opp_fga if opp_fga > 0 else np.nan\n",
    "            game_features['opp_current_fh_fg3_pct'] = opp_game_fh['FG3M'].iloc[0] / opp_fg3a if opp_fg3a > 0 else np.nan\n",
    "            game_features['opp_current_fh_pace'] = opp_fga + opp_game_fh['TO'].iloc[0]\n",
    "        else:\n",
    "            opp_fh_stats = ['pts', 'fgm', 'fga', 'fg3m', 'fg3a', 'reb', 'ast', 'to', 'pf', \n",
    "                            'fg_pct', 'fg3_pct', 'pace']\n",
    "            for stat in opp_fh_stats:\n",
    "                game_features[f'opp_current_fh_{stat}'] = np.nan\n",
    "        \n",
    "        # Halftime Stats        \n",
    "        if len(current_game_fh) > 0 and len(opp_game_fh) > 0:\n",
    "            \n",
    "            # Totals\n",
    "            game_features['halftime_total'] = current_game_fh['PTS'].iloc[0] + opp_game_fh['PTS'].iloc[0]\n",
    "            \n",
    "            # PAce\n",
    "            team_pace = game_features.get('current_fh_pace', 0)\n",
    "            opp_pace = game_features.get('opp_current_fh_pace', 0)\n",
    "            game_features['halftime_total_pace'] = team_pace + opp_pace\n",
    "            \n",
    "            # Shooting PCTs\n",
    "            team_fg_pct = game_features.get('current_fh_fg_pct', np.nan)\n",
    "            opp_fg_pct = game_features.get('opp_current_fh_fg_pct', np.nan)\n",
    "            if not pd.isna(team_fg_pct) and not pd.isna(opp_fg_pct):\n",
    "                game_features['halftime_combined_fg_pct'] = (team_fg_pct + opp_fg_pct) / 2\n",
    "            else:\n",
    "                game_features['halftime_combined_fg_pct'] = np.nan\n",
    "            \n",
    "            # Turnovers\n",
    "            game_features['halftime_total_to'] = current_game_fh['TO'].iloc[0] + opp_game_fh['TO'].iloc[0]\n",
    "            \n",
    "            # Scoring\n",
    "            team_avg = game_features.get('team_fh_pts_7d', np.nan)\n",
    "            opp_avg = game_features.get('opp_team_fh_pts_7d', np.nan)\n",
    "            team_current = current_game_fh['PTS'].iloc[0]\n",
    "            opp_current = opp_game_fh['PTS'].iloc[0]\n",
    "            \n",
    "            if not pd.isna(team_avg) and not pd.isna(opp_avg):\n",
    "                team_var = team_current - team_avg\n",
    "                opp_var = opp_current - opp_avg\n",
    "                game_features['halftime_combined_scoring_variance'] = team_var + opp_var\n",
    "            else:\n",
    "                game_features['halftime_combined_scoring_variance'] = np.nan\n",
    "            \n",
    "            # Lead\n",
    "            game_features['halftime_lead_abs'] = abs(current_game_fh['PTS'].iloc[0] - opp_game_fh['PTS'].iloc[0])\n",
    "        \n",
    "        else:\n",
    "            game_features['halftime_total'] = np.nan\n",
    "            game_features['halftime_total_pace'] = np.nan\n",
    "            game_features['halftime_combined_fg_pct'] = np.nan\n",
    "            game_features['halftime_total_to'] = np.nan\n",
    "            game_features['halftime_combined_scoring_variance'] = np.nan\n",
    "            game_features['halftime_lead_abs'] = np.nan\n",
    "    \n",
    "    ## Getting Second Half Targets\n",
    "    current_game_cg = cg_teams[\n",
    "        (cg_teams['GAME_ID'] == game_id) & \n",
    "        (cg_teams['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    \n",
    "    # Team's second-half score\n",
    "    if len(current_game_cg) > 0 and len(current_game_fh) > 0:\n",
    "        complete_pts = current_game_cg['PTS'].iloc[0]\n",
    "        first_half_pts = current_game_fh['PTS'].iloc[0]\n",
    "        game_features['actual_second_half_pts'] = complete_pts - first_half_pts\n",
    "    else:\n",
    "        game_features['actual_second_half_pts'] = np.nan\n",
    "    \n",
    "    # Opponent's second-half score\n",
    "    if opponent_id is not None:\n",
    "        opp_game_cg = cg_teams[\n",
    "            (cg_teams['GAME_ID'] == game_id) & \n",
    "            (cg_teams['TEAM_ID'] == opponent_id)\n",
    "        ]\n",
    "        \n",
    "        if len(opp_game_cg) > 0 and len(opp_game_fh) > 0:\n",
    "            opp_complete_pts = opp_game_cg['PTS'].iloc[0]\n",
    "            opp_first_half_pts = opp_game_fh['PTS'].iloc[0]\n",
    "            opp_second_half_pts = opp_complete_pts - opp_first_half_pts\n",
    "            \n",
    "            # Both teams combined for second half (this is the target)\n",
    "            if not pd.isna(game_features['actual_second_half_pts']) and not pd.isna(opp_second_half_pts):\n",
    "                game_features['actual_second_half_total'] = game_features['actual_second_half_pts'] + opp_second_half_pts\n",
    "            else:\n",
    "                game_features['actual_second_half_total'] = np.nan\n",
    "        else:\n",
    "            game_features['actual_second_half_total'] = np.nan\n",
    "    else:\n",
    "        game_features['actual_second_half_total'] = np.nan\n",
    "\n",
    "    all_features.append(game_features)\n",
    "    \n",
    "# Create dataframe\n",
    "features_df = pd.DataFrame(all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccbddbe",
   "metadata": {},
   "source": [
    "Used this to inspect the dataset and then also write the dataset. They're split as a relic of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd66b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()\n",
    "features_df.to_csv('nba_time_based_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2263bf",
   "metadata": {},
   "source": [
    "Next cell is using a mapping table to try and create a key based on the team and the date so we can join it to our NBA dataset. There were a number of games missing and a big bulk of those is because we only have half of the 2023 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930364f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "‚úÖ Created GAME_ID to h2_total mapping\n",
      "Total games: 7804\n",
      "Games with h2_total: 3158\n",
      "Games missing h2_total: 4646\n",
      "\n",
      "üíæ Saved to game_id_to_h2_total.csv\n",
      "\n",
      "Sample of mapping:\n",
      "     GAME_ID  h2_total\n",
      "0   42000406     112.5\n",
      "1   42000405     112.5\n",
      "2   42000404     113.5\n",
      "3   42000403     113.0\n",
      "4   42000402     112.0\n",
      "5   42000401     112.5\n",
      "6   42000306     111.5\n",
      "7   42000305     105.5\n",
      "8   42000316     109.5\n",
      "9   42000304     108.0\n",
      "10  42000315     109.0\n",
      "11  42000303     113.5\n",
      "12  42000314     109.5\n",
      "13  42000302     110.5\n",
      "14  42000313     111.5\n",
      "15  42000301     111.0\n",
      "16  42000312     112.0\n",
      "17  42000207     106.0\n",
      "18  42000311     112.0\n",
      "19  42000217     109.0\n"
     ]
    }
   ],
   "source": [
    "# Mapping table for betting line dataset to be joined to NBA data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "betting_data = pd.read_csv('betting_data.csv')\n",
    "games_master = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "\n",
    "# Format dates\n",
    "betting_data['date'] = pd.to_datetime(betting_data['date'])\n",
    "games_master['GAME_DATE'] = pd.to_datetime(games_master['GAME_DATE'])\n",
    "\n",
    "# Create team abbreviation mapping\n",
    "team_map = {\n",
    "    'atl': 'ATL', 'bos': 'BOS', 'bkn': 'BKN', 'cha': 'CHA', 'chi': 'CHI',\n",
    "    'cle': 'CLE', 'dal': 'DAL', 'den': 'DEN', 'det': 'DET', 'gs': 'GSW',\n",
    "    'hou': 'HOU', 'ind': 'IND', 'lac': 'LAC', 'lal': 'LAL', 'mem': 'MEM',\n",
    "    'mia': 'MIA', 'mil': 'MIL', 'min': 'MIN', 'no': 'NOP', 'nyk': 'NYK',\n",
    "    'okc': 'OKC', 'orl': 'ORL', 'phi': 'PHI', 'phx': 'PHX', 'por': 'POR',\n",
    "    'sac': 'SAC', 'sa': 'SAS', 'tor': 'TOR', 'utah': 'UTA', 'wsh': 'WAS',\n",
    "    'nj': 'BKN', 'ny': 'NYK'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "betting_data['away_team'] = betting_data['away'].map(team_map)\n",
    "betting_data['home_team'] = betting_data['home'].map(team_map)\n",
    "\n",
    "# Use dates and teams to join\n",
    "betting_data['match_key'] = (\n",
    "    betting_data['date'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    betting_data['away_team'] + '_' + \n",
    "    betting_data['home_team']\n",
    ")\n",
    "\n",
    "# Create a key for NBA games\n",
    "games_master['is_home'] = games_master['MATCHUP'].str.contains('vs.', na=False)\n",
    "\n",
    "# Separate home and away games\n",
    "home_games = games_master[games_master['is_home'] == True][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "away_games = games_master[games_master['is_home'] == False][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "\n",
    "home_games.columns = ['GAME_ID', 'GAME_DATE', 'home_team']\n",
    "away_games.columns = ['GAME_ID', 'GAME_DATE', 'away_team']\n",
    "\n",
    "# Need a version for home and away\n",
    "game_matchups = home_games.merge(away_games, on=['GAME_ID', 'GAME_DATE'])\n",
    "\n",
    "# Create key for NBA games\n",
    "game_matchups['match_key'] = (\n",
    "    game_matchups['GAME_DATE'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    game_matchups['away_team'] + '_' + \n",
    "    game_matchups['home_team']\n",
    ")\n",
    "\n",
    "# Join based on keys\n",
    "game_id_target = game_matchups.merge(\n",
    "    betting_data[['match_key', 'h2_total']],\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Keep only GAME_ID and h2_total\n",
    "game_id_target = game_id_target[['GAME_ID', 'h2_total']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nCreated GAME_ID to h2_total mapping\")\n",
    "print(f\"Total games: {len(game_id_target)}\")\n",
    "print(f\"Games with h2_total: {game_id_target['h2_total'].notna().sum()}\")\n",
    "print(f\"Games missing h2_total: {game_id_target['h2_total'].isna().sum()}\")\n",
    "\n",
    "# Save the mapping\n",
    "game_id_target.to_csv('game_id_to_h2_total.csv', index=False)\n",
    "print(f\"\\nSaved to game_id_to_h2_total.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ce94e",
   "metadata": {},
   "source": [
    "Cell below actually does the join. Upon further investigation we realized that the failed joins were preseason and international exhibition games so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62608b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After join: (15613, 1383)\n",
      "Rows with h2_total: 6316\n",
      "Rows without h2_total: 9297\n",
      "\n",
      "After dropping preseason/exhibition: (6316, 1383)\n",
      "\n",
      "Saving to nba_features_with_target.csv...\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "# Merge Betting Lines On and Filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features_df = pd.read_csv('nba_time_based_features.csv')\n",
    "game_id_target = pd.read_csv('game_id_to_h2_total.csv')\n",
    "\n",
    "# Join h2_total to features\n",
    "features_with_target = features_df.merge(\n",
    "    game_id_target,\n",
    "    on='GAME_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"After join: {features_with_target.shape}\")\n",
    "print(f\"Rows with h2_total: {features_with_target['h2_total'].notna().sum()}\")\n",
    "print(f\"Rows without h2_total: {features_with_target['h2_total'].isna().sum()}\")\n",
    "\n",
    "# Drop rows without h2_total (preseason/exhibition games)\n",
    "features_with_target = features_with_target[features_with_target['h2_total'].notna()]\n",
    "\n",
    "print(f\"\\nAfter dropping preseason/exhibition: {features_with_target.shape}\")\n",
    "\n",
    "# Save the joined dataset\n",
    "print(f\"\\nSaving to nba_features_with_target.csv...\")\n",
    "features_with_target.to_csv('nba_features_with_target.csv', index=False)\n",
    "print(f\"Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4791",
   "metadata": {},
   "source": [
    "This next cell block preps the data for modelling. First it drops all columns that would cost data leaks (basically ones that have second half information) since we are using walk forward building dates are fine. We are testing on the second most recent season we had betting information on and before that is train data. We held out the last partial season of data for seperate validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e3efc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 2342 games\n",
      "Test: 2646 games\n",
      "Holdout: 1328 games\n",
      "\n",
      "Saved: train_data.csv, test_data.csv, holdout_data.csv, feature_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Train / Test splits for data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('nba_features_with_target.csv')\n",
    "df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "# Drop leakage columns\n",
    "drop_columns = [\n",
    "    # Rolling second-half team stats\n",
    "    'team_cg_second_half_pts_7d', 'team_cg_second_half_fgm_7d', \n",
    "    'team_cg_second_half_fga_7d', 'team_cg_second_half_fg3m_7d',\n",
    "    'team_cg_second_half_fg3a_7d', 'team_cg_second_half_ftm_7d',\n",
    "    'team_cg_second_half_fta_7d', 'team_cg_second_half_reb_7d',\n",
    "    'team_cg_second_half_ast_7d', 'team_cg_second_half_to_7d',\n",
    "    'team_cg_second_half_stl_7d', 'team_cg_second_half_blk_7d',\n",
    "    'team_cg_second_half_fg_pct_7d', 'team_cg_second_half_fg3_pct_7d',\n",
    "    'team_cg_second_half_ft_pct_7d', 'team_cg_second_half_pts_14d',\n",
    "    'team_cg_second_half_fgm_14d', 'team_cg_second_half_fga_14d',\n",
    "    'team_cg_second_half_fg3m_14d', 'team_cg_second_half_fg3a_14d',\n",
    "    'team_cg_second_half_ftm_14d', 'team_cg_second_half_fta_14d',\n",
    "    'team_cg_second_half_reb_14d', 'team_cg_second_half_ast_14d',\n",
    "    'team_cg_second_half_to_14d', 'team_cg_second_half_stl_14d',\n",
    "    'team_cg_second_half_blk_14d', 'team_cg_second_half_fg_pct_14d',\n",
    "    'team_cg_second_half_fg3_pct_14d', 'team_cg_second_half_ft_pct_14d',\n",
    "    'team_cg_second_half_pts_30d', 'team_cg_second_half_fgm_30d',\n",
    "    'team_cg_second_half_fga_30d', 'team_cg_second_half_fg3m_30d',\n",
    "    'team_cg_second_half_fg3a_30d', 'team_cg_second_half_ftm_30d',\n",
    "    'team_cg_second_half_fta_30d', 'team_cg_second_half_reb_30d',\n",
    "    'team_cg_second_half_ast_30d', 'team_cg_second_half_to_30d',\n",
    "    'team_cg_second_half_stl_30d', 'team_cg_second_half_blk_30d',\n",
    "    'team_cg_second_half_fg_pct_30d', 'team_cg_second_half_fg3_pct_30d',\n",
    "    'team_cg_second_half_ft_pct_30d',\n",
    "\n",
    "    # Direct post-game leakage columns\n",
    "    'actual_second_half_pts', 'actual_second_half_fgm',\n",
    "    'actual_second_half_fga', 'actual_second_half_fg3m',\n",
    "    'actual_second_half_fg3a', 'actual_second_half_ftm',\n",
    "    'actual_second_half_fta', 'actual_second_half_reb',\n",
    "    'actual_second_half_ast', 'actual_second_half_to',\n",
    "    'actual_second_half_stl', 'actual_second_half_blk',\n",
    "    'actual_second_half_fg_pct', 'actual_second_half_fg3_pct',\n",
    "    'actual_second_half_ft_pct', 'actual_second_half_plus_minus'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "# Get seasons\n",
    "def get_nba_season(date):\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    return f\"{year}-{year+1}\" if month >= 10 else f\"{year-1}-{year}\"\n",
    "\n",
    "df['season'] = df['GAME_DATE'].apply(get_nba_season)\n",
    "df = df.sort_values('GAME_DATE')\n",
    "\n",
    "seasons = sorted(df['season'].unique())\n",
    "\n",
    "# Train on earliest seasons, test on second-to-last, holdout on last (where we have partial data to validate seperately)\n",
    "holdout_season = seasons[-1]\n",
    "test_season = seasons[-2]\n",
    "train_seasons = seasons[:-2]\n",
    "\n",
    "# Filter data (need both target and betting line)\n",
    "train_data = df[\n",
    "    (df['season'].isin(train_seasons)) & \n",
    "    (df['actual_second_half_total'].notna()) &\n",
    "    (df['h2_total'].notna())\n",
    "]\n",
    "\n",
    "test_data = df[\n",
    "    (df['season'] == test_season) & \n",
    "    (df['actual_second_half_total'].notna()) &\n",
    "    (df['h2_total'].notna())\n",
    "]\n",
    "\n",
    "holdout_data = df[\n",
    "    (df['season'] == holdout_season) & \n",
    "    (df['actual_second_half_total'].notna())\n",
    "]\n",
    "\n",
    "# Print statements to give us a sense of dataset size\n",
    "print(f\"\\nTrain: {len(train_data)} games\")\n",
    "print(f\"Test: {len(test_data)} games\")\n",
    "print(f\"Holdout: {len(holdout_data)} games\")\n",
    "\n",
    "# Prepare features (exclude metadata, target, and betting line)\n",
    "metadata_cols = ['GAME_ID', 'GAME_DATE', 'TEAM_ID', 'season']\n",
    "target_col = 'actual_second_half_total'\n",
    "betting_line_col = 'h2_total'\n",
    "\n",
    "feature_cols = [c for c in df.columns \n",
    "                if c not in metadata_cols + [target_col, betting_line_col]]\n",
    "\n",
    "# Save datasets\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "holdout_data.to_csv('holdout_data.csv', index=False)\n",
    "\n",
    "with open('feature_list.txt', 'w') as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "print(\"\\nSaved: train_data.csv, test_data.csv, holdout_data.csv, feature_list.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "556de189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled!\n",
      "[0]\tvalidation_0-rmse:13.96279\tvalidation_1-rmse:13.56825\n",
      "[50]\tvalidation_0-rmse:10.18589\tvalidation_1-rmse:13.46736\n",
      "[100]\tvalidation_0-rmse:7.69453\tvalidation_1-rmse:13.48778\n",
      "[150]\tvalidation_0-rmse:6.06479\tvalidation_1-rmse:13.51003\n",
      "[161]\tvalidation_0-rmse:5.74957\tvalidation_1-rmse:13.51027\n",
      "\n",
      "Test Metrics:\n",
      "MAE:  10.610 points\n",
      "RMSE: 13.458 points\n",
      "R¬≤:   0.004\n",
      "Saved xgboost_model.json\n",
      "Saved feature_importance.csv\n",
      "Saved test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# First XGBoost Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Format Dates\n",
    "train_data['GAME_DATE'] = pd.to_datetime(train_data['GAME_DATE'])\n",
    "test_data['GAME_DATE'] = pd.to_datetime(test_data['GAME_DATE'])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = ['GAME_ID', 'GAME_DATE', 'TEAM_ID', 'season']\n",
    "\n",
    "# Target\n",
    "target_col = 'actual_second_half_total'\n",
    "\n",
    "# Comparison at end not used in modelling\n",
    "betting_line_col = 'h2_total'\n",
    "\n",
    "# Feature columns\n",
    "drop_from_features = set(metadata_cols + [target_col])\n",
    "if betting_line_col in train_data.columns:\n",
    "    drop_from_features.add(betting_line_col)\n",
    "\n",
    "feature_cols = [c for c in train_data.columns if c not in drop_from_features]\n",
    "\n",
    "# Split into X and y\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data[target_col]\n",
    "\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data[target_col]\n",
    "\n",
    "# Imputer using Median\n",
    "\n",
    "missing_train = X_train.isnull().sum()\n",
    "cols_with_missing = missing_train[missing_train > 0]\n",
    "\n",
    "if len(cols_with_missing) > 0:\n",
    "    for col in cols_with_missing.index:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)  # use train medians for test\n",
    "    print(\"Missing values handled!\")\n",
    "else:\n",
    "    print(\"No missing values!\")\n",
    "\n",
    "## Model\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=0.05,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=120,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "\n",
    "# Evalutation \n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"MAE:  {mae_test:.3f} points\")\n",
    "print(f\"RMSE: {rmse_test:.3f} points\")\n",
    "print(f\"R¬≤:   {r2_test:.3f}\")\n",
    "\n",
    "# Export Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Store Predictions\n",
    "test_results = test_data.copy()\n",
    "test_results['actual_second_half_total_predicted'] = y_pred_test\n",
    "test_results['prediction_error'] = np.abs(y_test - y_pred_test)\n",
    "\n",
    "show_cols = ['GAME_ID', 'GAME_DATE', 'actual_second_half_total',\n",
    "            'actual_second_half_total_predicted', 'prediction_error']\n",
    "if betting_line_col in test_results.columns:\n",
    "    test_results['edge_vs_line'] = test_results['actual_second_half_total_predicted'] - test_results[betting_line_col]\n",
    "    show_cols.append(betting_line_col)\n",
    "    show_cols.append('edge_vs_line')\n",
    "\n",
    "\n",
    "# Save model and results\n",
    "model.save_model('xgboost_model.json')\n",
    "print(\"Saved xgboost_model.json\")\n",
    "\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(\"Saved feature_importance.csv\")\n",
    "\n",
    "test_results.to_csv('test_predictions.csv', index=False)\n",
    "print(\"Saved test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b28e7e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:13.24557\tvalidation_1-rmse:12.61378\n",
      "[50]\tvalidation_0-rmse:10.74339\tvalidation_1-rmse:12.61934\n",
      "[100]\tvalidation_0-rmse:8.57413\tvalidation_1-rmse:12.66605\n",
      "[150]\tvalidation_0-rmse:6.78661\tvalidation_1-rmse:12.72903\n",
      "\n",
      "Residual Model Metrics:\n",
      "MAE:  9.735 pts\n",
      "RMSE: 12.601 pts\n",
      "R¬≤:   0.002\n",
      "FULL TOTAL PERFORMANCE (Actual Second Half Totals)\n",
      "MAE:  9.735 pts\n",
      "RMSE: 12.601 pts\n",
      "R¬≤:   0.127\n",
      "Saved model (xgboost_residual_model.json)\n",
      "Saved predictions (test_predictions_residual.csv)\n"
     ]
    }
   ],
   "source": [
    "# XGboost using h2_total residual\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Format dates\n",
    "train_data[\"GAME_DATE\"] = pd.to_datetime(train_data[\"GAME_DATE\"])\n",
    "test_data[\"GAME_DATE\"] = pd.to_datetime(test_data[\"GAME_DATE\"])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Residual\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = [c for c in train_data.columns if c not in metadata_cols + [target_col]]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "# Add Residual Here\n",
    "y_train = train_data[target_col] - train_data[vegas_col]\n",
    "\n",
    "\n",
    "X_test = test_data[feature_cols].copy()\n",
    "# Add Residual Here\n",
    "y_test = test_data[target_col] - test_data[vegas_col]\n",
    "\n",
    "# Imputer for missing values uses median\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isnull().any():\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "\n",
    "## Model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=0.05,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=120,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Evalutation\n",
    "\n",
    "y_pred_resid = model.predict(X_test)\n",
    "mae_resid = mean_absolute_error(y_test, y_pred_resid)\n",
    "rmse_resid = np.sqrt(mean_squared_error(y_test, y_pred_resid))\n",
    "r2_resid = r2_score(y_test, y_pred_resid)\n",
    "\n",
    "print(f\"\\nResidual Model Metrics:\")\n",
    "print(f\"MAE:  {mae_resid:.3f} pts\")\n",
    "print(f\"RMSE: {rmse_resid:.3f} pts\")\n",
    "print(f\"R¬≤:   {r2_resid:.3f}\")\n",
    "\n",
    "# Reconstruct Predictions\n",
    "y_pred_total = y_pred_resid + X_test[vegas_col]\n",
    "y_true_total = y_test + X_test[vegas_col]\n",
    "\n",
    "mae_total = mean_absolute_error(y_true_total, y_pred_total)\n",
    "rmse_total = np.sqrt(mean_squared_error(y_true_total, y_pred_total))\n",
    "r2_total = r2_score(y_true_total, y_pred_total)\n",
    "\n",
    "print(\"FULL TOTAL PERFORMANCE (Actual Second Half Totals)\")\n",
    "print(f\"MAE:  {mae_total:.3f} pts\")\n",
    "print(f\"RMSE: {rmse_total:.3f} pts\")\n",
    "print(f\"R¬≤:   {r2_total:.3f}\")\n",
    "\n",
    "# Prepping test results\n",
    "test_results = test_data.copy()\n",
    "test_results[\"predicted_residual\"] = y_pred_resid\n",
    "test_results[\"predicted_total\"] = y_pred_total\n",
    "test_results[\"residual_error\"] = y_test - y_pred_resid\n",
    "test_results[\"actual_residual\"] = y_test\n",
    "test_results[\"edge_vs_vegas\"] = y_pred_resid\n",
    "\n",
    "# Save everything\n",
    "model.save_model(\"xgboost_residual_model.json\")\n",
    "test_results.to_csv(\"test_predictions_residual.csv\", index=False)\n",
    "\n",
    "print(\"Saved model (xgboost_residual_model.json)\")\n",
    "print(\"Saved predictions (test_predictions_residual.csv)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ce8dcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECOND HALF TOTALS ‚Äì MODEL BENCHMARK COMPARISON\n",
      "                     MAE   RMSE    R¬≤  Œî MAE vs Vegas  Œî RMSE vs Vegas\n",
      "Vegas Line         9.738 12.618 0.124           0.000            0.000\n",
      "No Residual Model 10.610 13.458 0.004           0.872            0.840\n",
      "Residual Model     9.735 12.601 0.127          -0.004           -0.017\n"
     ]
    }
   ],
   "source": [
    "# Model Comparison\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load models\n",
    "resid = pd.read_csv(\"test_predictions.csv\")                  # main (no residual) model\n",
    "no_resid = pd.read_csv(\"test_predictions_residual.csv\")      # residual model\n",
    "\n",
    "# I named the columns badly...\n",
    "def get_pred_col(df):\n",
    "    if \"predicted_total\" in df.columns:\n",
    "        return \"predicted_total\"\n",
    "    elif \"actual_second_half_total_predicted\" in df.columns:\n",
    "        return \"actual_second_half_total_predicted\"\n",
    "    else:\n",
    "        raise KeyError(\"still can't get the columns right\")\n",
    "\n",
    "pred_col_resid = get_pred_col(resid)\n",
    "pred_col_no_resid = get_pred_col(no_resid)\n",
    "\n",
    "# Targets\n",
    "y_true = resid[\"actual_second_half_total\"]\n",
    "y_vegas = resid[\"h2_total\"]\n",
    "\n",
    "# Predictions\n",
    "y_pred_no_residual = resid[pred_col_resid]\n",
    "y_pred_residual = no_resid[pred_col_no_resid]\n",
    "\n",
    "# Evaluator function\n",
    "def evaluate(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Metrics Calc\n",
    "results = {\n",
    "    \"Vegas Line\": evaluate(y_true, y_vegas),\n",
    "    \"No Residual Model\": evaluate(y_true, y_pred_no_residual),\n",
    "    \"Residual Model\": evaluate(y_true, y_pred_residual),\n",
    "}\n",
    "\n",
    "# Summary output\n",
    "summary = pd.DataFrame(results, index=[\"MAE\", \"RMSE\", \"R¬≤\"]).T\n",
    "summary[\"Œî MAE vs Vegas\"] = summary[\"MAE\"] - summary.loc[\"Vegas Line\", \"MAE\"]\n",
    "summary[\"Œî RMSE vs Vegas\"] = summary[\"RMSE\"] - summary.loc[\"Vegas Line\", \"RMSE\"]\n",
    "\n",
    "print(\"SECOND HALF TOTALS ‚Äì MODEL BENCHMARK COMPARISON\")\n",
    "print(summary.to_string(float_format=lambda x: f\"{x:0.3f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0f916",
   "metadata": {},
   "source": [
    "Basically as we were testing we found out that the Vegas line is actually very very good. Even without using first half data its actually better then our tuned XGBoost model. Knowing that we have the Vegas second half total we used that as a residual in another XGBoost model, which gave us some improvements over the Vegas line. Is this enough to make betting model that is profitable? We do not quite know yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9ce12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCA components: 30\n",
      "Explained variance (first 5 PCs cumulative): [0.12836405 0.17675469 0.21865164 0.25765873 0.28763853]\n",
      "Components to reach ~99% variance: 30\n",
      "k= 6 -> silhouette: 0.0725\n",
      "k= 8 -> silhouette: 0.0678\n",
      "k=10 -> silhouette: 0.0611\n",
      "k=12 -> silhouette: 0.0627\n",
      "\n",
      "Selected k=6 (silhouette=0.0725)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agg_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Summary for Cluster\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 93\u001b[0m     agg_cols[target_col] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m betting_col \u001b[38;5;129;01min\u001b[39;00m out\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     95\u001b[0m     agg_cols[betting_col] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agg_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading training data\n",
    "df = pd.read_csv(\"train_data.csv\")\n",
    "df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Comparison col\n",
    "betting_col = \"h2_total\"\n",
    "\n",
    "# Drop unnecceasry columns and keep only features\n",
    "drop_cols = set(metadata_cols + [target_col, betting_col])\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Imputer with medium\n",
    "missing = X.isna().sum().sum()\n",
    "if missing > 0:\n",
    "    for c in feature_cols:\n",
    "        if X[c].isna().any():\n",
    "            X[c].fillna(X[c].median(), inplace=True)\n",
    "else:\n",
    "    print(\"No imputing neccesary\")\n",
    "\n",
    "\n",
    "# Scale and apply PCA\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# keep up to 30 PCs or fewer if features < 30\n",
    "n_components = min(30, X_scaled.shape[1])\n",
    "pca = PCA(n_components=n_components, svd_solver=\"auto\", random_state=42)\n",
    "X_pcs = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_.cumsum()\n",
    "var_99 = np.argmax(explained >= 0.99) + 1 if (explained >= 0.99).any() else n_components\n",
    "\n",
    "print(f\"\\nPCA components: {n_components}\")\n",
    "print(f\"Explained variance (first 5 PCs cumulative): {explained[:5]}\")\n",
    "print(f\"Components to reach ~99% variance: {var_99}\")\n",
    "\n",
    "# KMeans test and evaluating based on silhouette\n",
    "best = {\"k\": None, \"score\": -1, \"model\": None, \"labels\": None}\n",
    "for k in [6, 8, 10, 12]:\n",
    "    km = KMeans(n_clusters=k, n_init=20, max_iter=500, random_state=42)\n",
    "    labels = km.fit_predict(X_pcs)\n",
    "    score = silhouette_score(X_pcs, labels) if len(np.unique(labels)) > 1 else -1\n",
    "    print(f\"k={k:2d} -> silhouette: {score:.4f}\")\n",
    "    if score > best[\"score\"]:\n",
    "        best = {\"k\": k, \"score\": score, \"model\": km, \"labels\": labels}\n",
    "\n",
    "k_best = best[\"k\"]\n",
    "kmeans = best[\"model\"]\n",
    "labels = best[\"labels\"]\n",
    "print(f\"\\nSelected k={k_best} (silhouette={best['score']:.4f})\")\n",
    "\n",
    "# Display Cluster Information\n",
    "pc_cols = [f\"PC{i+1}\" for i in range(X_pcs.shape[1])]\n",
    "out = df[[\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]].copy()\n",
    "for i, col in enumerate(pc_cols):\n",
    "    out[col] = X_pcs[:, i]\n",
    "out[\"cluster_id\"] = labels\n",
    "\n",
    "# Adding target col and betting col to explore clusters\n",
    "if target_col in df.columns:\n",
    "    out[target_col] = df[target_col].values\n",
    "if betting_col in df.columns:\n",
    "    out[betting_col] = df[betting_col].values\n",
    "    if target_col in df.columns:\n",
    "        out[\"edge_vs_line\"] = out[target_col] - out[betting_col]\n",
    "\n",
    "out.to_csv(\"pca_kmeans_train.csv\", index=False)\n",
    "\n",
    "# Summary for Cluster\n",
    "if target_col in out.columns:\n",
    "    agg_cols[target_col] = [\"mean\", \"median\"]\n",
    "if betting_col in out.columns:\n",
    "    agg_cols[betting_col] = [\"mean\", \"median\"]\n",
    "if \"edge_vs_line\" in out.columns:\n",
    "    agg_cols[\"edge_vs_line\"] = [\"mean\", \"median\"]\n",
    "\n",
    "summary = (\n",
    "    out.groupby(\"cluster_id\")\n",
    "        .agg(**{k: pd.NamedAgg(column=k, aggfunc=v) for k, v in {\n",
    "           **({target_col: \"mean\"} if target_col in out.columns else {}),\n",
    "           **({betting_col: \"mean\"} if betting_col in out.columns else {}),\n",
    "           **({\"edge_vs_line\": \"mean\"} if \"edge_vs_line\" in out.columns else {})\n",
    "        }.items()})\n",
    "        .rename(columns={\n",
    "            target_col: \"avg_actual_2H_total\",\n",
    "            betting_col: \"avg_h2_total\",\n",
    "            \"edge_vs_line\": \"avg_edge_vs_line\"\n",
    "        })\n",
    "        .reset_index()\n",
    ")\n",
    "sizes = out[\"cluster_id\"].value_counts().rename_axis(\"cluster_id\").reset_index(name=\"count\")\n",
    "cluster_summary = sizes.merge(summary, on=\"cluster_id\", how=\"left\")\n",
    "cluster_summary.sort_values(\"count\", ascending=False, inplace=True)\n",
    "cluster_summary.to_csv(\"cluster_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nCluster Summary (top 8):\")\n",
    "print(cluster_summary.head(8).to_string(index=False))\n",
    "\n",
    "# Save unsupervised learning to pickle files\n",
    "joblib.dump(scaler, \"unsup_scaler.pkl\")\n",
    "joblib.dump(pca, \"unsup_pca.pkl\")\n",
    "joblib.dump(kmeans, \"unsup_kmeans.pkl\")\n",
    "print(\"\\nSaved: unsup_scaler.pkl, unsup_pca.pkl, unsup_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eabf0e",
   "metadata": {},
   "source": [
    " PCA performing well is not surprising so many of the features we created were derivatives of stats that are already present. So it makes sense that PCA was able to compress these down into 30 features, there might only be 30 features that observe different qualities of an NBA game from the dataset.\n",
    "\n",
    " Cluster doees not seem to add much of anything. The silhouettes were incredibly small. We think this makes sense because the NBA is a multi-billion dollar industry and it makes sense that its converged similar team composition, style of play, and game outcomes.  Yes there is innovation and variance but teams are incentivized so heavily to win that we do not actually see these innovations completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading train and test data...\n",
      "\n",
      "Using saved scaler & PCA (unsup_scaler.pkl / unsup_pca.pkl)\n",
      "\n",
      "PCA features shape -> train: (4988, 50), test: (1328, 50)\n",
      "[0]\tvalidation_0-rmse:12.93217\tvalidation_1-rmse:13.18861\n",
      "[50]\tvalidation_0-rmse:12.24320\tvalidation_1-rmse:13.21503\n",
      "[75]\tvalidation_0-rmse:11.91613\tvalidation_1-rmse:13.23899\n",
      "RESIDUAL PERFORMANCE (Model vs Vegas)\n",
      "üîç BENCHMARK vs VEGAS\n",
      "Vegas    -> MAE: 10.299 | RMSE: 13.215 | R¬≤: 0.121\n",
      "Residual -> MAE: 10.287 | RMSE: 13.189 | R¬≤: 0.125\n",
      "ŒîMAE vs Vegas:  -0.012  (negative is better)\n",
      "ŒîRMSE vs Vegas: -0.027 (negative is better)\n",
      "Saved test_predictions_residual_pca.csv\n",
      "Saved xgboost_residual_pca.json\n",
      "Saved pca_feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "# Running the XGBoost with residual with the PCA treatment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading train and test data...\")\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Format Data\n",
    "for df in (train, test):\n",
    "    if 'GAME_DATE' in df.columns:\n",
    "        df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'], errors='coerce')\n",
    "\n",
    "## Columns\n",
    "# Filter\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Residual\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "# Features\n",
    "feature_cols = [c for c in train.columns if c not in (metadata_cols + [target_col])]\n",
    "\n",
    "# Split\n",
    "X_train = train[feature_cols].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "y_train_total = train[target_col].values\n",
    "y_test_total = test[target_col].values\n",
    "\n",
    "# Residual targets\n",
    "y_train = (train[target_col] - train[vegas_col]).values\n",
    "y_test = (test[target_col]  - test[vegas_col]).values\n",
    "\n",
    "# Impute with median this method is so much easier\n",
    "medians = X_train.median(numeric_only=True)\n",
    "X_train = X_train.fillna(medians)\n",
    "X_test = X_test.fillna(medians)\n",
    "\n",
    "\n",
    "# Load our scaler and pca\n",
    "use_saved = False\n",
    "scaler_path = \"unsup_scaler.pkl\"\n",
    "pca_path = \"unsup_pca.pkl\"\n",
    "\n",
    "if os.path.exists(scaler_path) and os.path.exists(pca_path):\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    pca    = joblib.load(pca_path)\n",
    "    # Verify feature count matches\n",
    "    try:\n",
    "        if hasattr(scaler, \"n_features_in_\") and scaler.n_features_in_ == X_train.shape[1]:\n",
    "            use_saved = True\n",
    "    except Exception:\n",
    "        use_saved = False\n",
    "\n",
    "if use_saved:\n",
    "    print(\"\\nUsing saved scaler & PCA (unsup_scaler.pkl / unsup_pca.pkl)\")\n",
    "else:\n",
    "    print(\"Falling back to recreating scale and pca\")\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Choose components: keep up to 99% variance, cap at 50 to be safe\n",
    "    pca_full = PCA(svd_solver=\"auto\", random_state=42)\n",
    "    pca_full.fit(X_train_scaled)\n",
    "    cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    n_comp = int(np.searchsorted(cumsum, 0.99) + 1)\n",
    "    n_comp = min(max(n_comp, 10), 50)  # between 10 and 50\n",
    "    print(f\"Selected PCA components: {n_comp} (‚âà99% variance)\")\n",
    "\n",
    "    pca = PCA(n_components=n_comp, svd_solver=\"auto\", random_state=42)\n",
    "    # Re-fit with chosen n_components\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Save for reuse next time\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(pca, pca_path)\n",
    "    print(\"Saved new unsup_scaler.pkl and unsup_pca.pkl\")\n",
    "\n",
    "# Transform using the scaler/PCA in use (saved or fresh)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nPCA features shape -> train: {X_train_pca.shape}, test: {X_test_pca.shape}\")\n",
    "\n",
    "# Model with specs\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1300,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=6,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0.15,\n",
    "    reg_alpha=0.6,\n",
    "    reg_lambda=1.2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=75,\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_pca, y_train,\n",
    "    eval_set=[(X_train_pca, y_train), (X_test_pca, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Eval\n",
    "print(\"RESIDUAL PERFORMANCE (Model vs Vegas)\")\n",
    "\n",
    "y_pred_resid = model.predict(X_test_pca)\n",
    "\n",
    "# Model Stats base\n",
    "mae_resid = mean_absolute_error(y_test, y_pred_resid)\n",
    "rmse_resid = np.sqrt(mean_squared_error(y_test, y_pred_resid))\n",
    "r2_resid = r2_score(y_test, y_pred_resid)\n",
    "\n",
    "\n",
    "# Reconstruct totals\n",
    "y_pred_total = y_pred_resid + test[vegas_col].values\n",
    "mae_total = mean_absolute_error(y_test_total, y_pred_total)\n",
    "rmse_total = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "r2_total = r2_score(y_test_total, y_pred_total)\n",
    "\n",
    "# Vegas baseline on the same split\n",
    "mae_line = mean_absolute_error(y_test_total, test[vegas_col].values)\n",
    "rmse_line = np.sqrt(mean_squared_error(y_test_total, test[vegas_col].values))\n",
    "r2_line = r2_score(y_test_total, test[vegas_col].values)\n",
    "\n",
    "print(\"üîç BENCHMARK vs VEGAS\")\n",
    "print(f\"Vegas    -> MAE: {mae_line:.3f} | RMSE: {rmse_line:.3f} | R¬≤: {r2_line:.3f}\")\n",
    "print(f\"Residual -> MAE: {mae_total:.3f} | RMSE: {rmse_total:.3f} | R¬≤: {r2_total:.3f}\")\n",
    "print(f\"ŒîMAE vs Vegas:  {mae_total - mae_line:+.3f}\")\n",
    "print(f\"ŒîRMSE vs Vegas: {rmse_total - rmse_line:+.3f}\")\n",
    "\n",
    "# Saves\n",
    "test_out = test.copy()\n",
    "test_out[\"predicted_residual_pca\"] = y_pred_resid\n",
    "test_out[\"predicted_total_pca\"] = y_pred_total\n",
    "test_out[\"edge_vs_vegas_pca\"] = y_pred_resid\n",
    "test_out[\"abs_error_pca\"] = np.abs(y_test_total - y_pred_total)\n",
    "\n",
    "test_out.to_csv(\"test_predictions_residual_pca.csv\", index=False)\n",
    "model.save_model(\"xgboost_residual_pca.json\")\n",
    "\n",
    "# Save another feature list\n",
    "with open(\"pca_feature_cols.json\", \"w\") as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "\n",
    "print(\"Saved test_predictions_residual_pca.csv\")\n",
    "print(\"Saved xgboost_residual_pca.json\")\n",
    "print(\"Saved pca_feature_cols.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c61c8400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING DATA\n",
      "============================================================\n",
      "\n",
      "NBA Games: (15615, 28)\n",
      "  Date range: 2020-12-11 00:00:00 to 2024-06-17 00:00:00\n",
      "\n",
      "Betting Data: (23118, 27)\n",
      "  Date range: 2007-10-30 00:00:00 to 2025-06-22 00:00:00\n",
      "\n",
      "============================================================\n",
      "ANALYZING H2_TOTAL AVAILABILITY\n",
      "============================================================\n",
      "\n",
      "Betting rows with h2_total: 19,817 / 23,118\n",
      "  h2_total date range: 2007-10-30 00:00:00 to 2023-01-16 00:00:00\n",
      "\n",
      "Betting with h2_total in NBA date range: 3,158\n",
      "  Date range: 2020-12-22 00:00:00 to 2023-01-16 00:00:00\n",
      "\n",
      "h2_total availability by month:\n",
      "year_month\n",
      "2020-12     67\n",
      "2021-01    222\n",
      "2021-02    212\n",
      "2021-03    204\n",
      "2021-04    240\n",
      "2021-05    173\n",
      "2021-06     45\n",
      "2021-07      8\n",
      "2021-10     93\n",
      "2021-11    225\n",
      "2021-12    209\n",
      "2022-01    231\n",
      "2022-02    163\n",
      "2022-03    229\n",
      "2022-04    129\n",
      "2022-05     38\n",
      "2022-06      6\n",
      "2022-10    102\n",
      "2022-11    222\n",
      "2022-12    222\n",
      "2023-01    118\n",
      "Freq: M, dtype: int64\n",
      "\n",
      "üéØ H2_TOTAL DATE RANGE:\n",
      "  Start: 2020-12-22 00:00:00\n",
      "  End: 2023-01-16 00:00:00\n",
      "  Duration: 755 days\n",
      "\n",
      "============================================================\n",
      "FILTERING NBA GAMES TO H2_TOTAL RANGE\n",
      "============================================================\n",
      "\n",
      "Original NBA games: 15,615\n",
      "NBA games in h2_total range: 9,078\n",
      "Excluded (outside h2_total range): 6,537\n",
      "\n",
      "============================================================\n",
      "TEAM MAPPING\n",
      "============================================================\n",
      "Unmapped teams: 0\n",
      "\n",
      "============================================================\n",
      "CREATING MATCHUPS\n",
      "============================================================\n",
      "NBA matchups: 4,537\n",
      "\n",
      "============================================================\n",
      "JOIN ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä Match Results:\n",
      "  NBA games (in h2_total range): 4,535\n",
      "  Betting lines with h2_total: 3,158\n",
      "  ‚úÖ Matched: 3,158\n",
      "  ‚ö†Ô∏è  NBA games without h2_total: 1,377\n",
      "  ‚ö†Ô∏è  Betting h2_total without NBA: 0\n",
      "\n",
      "üìà Match Rate: 69.6%\n",
      "   (3,158 / 4,535 NBA games have h2_total)\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "‚úÖ Saved nba_games_with_h2_total.csv\n",
      "   Total games: 4,537\n",
      "   With h2_total: 3,158\n",
      "   Without h2_total: 1,379\n",
      "\n",
      "‚ùå Saved nba_games_missing_h2_total.csv\n",
      "   NBA games without h2_total: 1,379\n",
      "\n",
      "   Sample games missing h2_total:\n",
      "     GAME_DATE away_team home_team     GAME_ID\n",
      "630 2021-03-11       LAK       DEL  2042000301\n",
      "639 2021-03-09       LAK       SCW  2042000211\n",
      "640 2021-03-09       DEL       RAP  2042000201\n",
      "641 2021-03-08       AUS       DEL  2042000131\n",
      "642 2021-03-08       LAK       ERI  2042000121\n",
      "643 2021-03-08       GLI       RAP  2042000101\n",
      "644 2021-03-08       RGV       SCW  2042000111\n",
      "645 2021-03-07       LBN       DRT    32000001\n",
      "646 2021-03-06       DEL       MHU  2022000132\n",
      "647 2021-03-06       ACC       GBO  2022000135\n",
      "\n",
      "   Missing h2_total by date (first 20 days):\n",
      "GAME_DATE\n",
      "2021-02-10    6\n",
      "2021-02-11    6\n",
      "2021-02-12    6\n",
      "2021-02-13    5\n",
      "2021-02-14    4\n",
      "2021-02-15    6\n",
      "2021-02-16    5\n",
      "2021-02-17    6\n",
      "2021-02-18    6\n",
      "2021-02-19    4\n",
      "2021-02-20    5\n",
      "2021-02-21    6\n",
      "2021-02-22    6\n",
      "2021-02-23    5\n",
      "2021-02-24    5\n",
      "2021-02-25    5\n",
      "2021-02-26    6\n",
      "2021-02-27    6\n",
      "2021-02-28    5\n",
      "2021-03-01    5\n",
      "dtype: int64\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "üéØ H2_TOTAL Coverage:\n",
      "  Date range analyzed: 2020-12-22 00:00:00 to 2023-01-16 00:00:00\n",
      "  NBA games in range: 4,537\n",
      "  Games with h2_total: 3,158 (69.6%)\n",
      "  Games missing h2_total: 1,377 (30.4%)\n",
      "\n",
      "üìÅ Files Created:\n",
      "  - nba_games_with_h2_total.csv (use this for modeling)\n",
      "  - nba_games_missing_h2_total.csv (failed joins to investigate)\n",
      "\n",
      "üí° Next Step:\n",
      "   Review nba_games_missing_h2_total.csv to understand why 1377 games\n",
      "   within the h2_total date range don't have betting lines\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find where h2_total ends and analyze joins for that specific range\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load datasets\n",
    "games = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "betting = pd.read_csv('betting_data.csv')\n",
    "\n",
    "# Convert dates\n",
    "games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "betting['date'] = pd.to_datetime(betting['date'])\n",
    "\n",
    "nba_min = games['GAME_DATE'].min()\n",
    "nba_max = games['GAME_DATE'].max()\n",
    "\n",
    "print(f\"\\nNBA Games: {games.shape}\")\n",
    "print(f\"  Date range: {nba_min} to {nba_max}\")\n",
    "\n",
    "print(f\"\\nBetting Data: {betting.shape}\")\n",
    "print(f\"  Date range: {betting['date'].min()} to {betting['date'].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIND WHERE H2_TOTAL DATA ENDS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING H2_TOTAL AVAILABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to betting data with h2_total only\n",
    "betting_with_h2 = betting[betting['h2_total'].notna()].copy()\n",
    "\n",
    "print(f\"\\nBetting rows with h2_total: {len(betting_with_h2):,} / {len(betting):,}\")\n",
    "print(f\"  h2_total date range: {betting_with_h2['date'].min()} to {betting_with_h2['date'].max()}\")\n",
    "\n",
    "# Filter to NBA date range\n",
    "betting_h2_in_nba_range = betting_with_h2[\n",
    "    (betting_with_h2['date'] >= nba_min) &\n",
    "    (betting_with_h2['date'] <= nba_max)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nBetting with h2_total in NBA date range: {len(betting_h2_in_nba_range):,}\")\n",
    "print(f\"  Date range: {betting_h2_in_nba_range['date'].min()} to {betting_h2_in_nba_range['date'].max()}\")\n",
    "\n",
    "# Group by month to see when h2_total stops\n",
    "betting_h2_in_nba_range['year_month'] = betting_h2_in_nba_range['date'].dt.to_period('M')\n",
    "monthly_h2 = betting_h2_in_nba_range.groupby('year_month').size()\n",
    "\n",
    "print(f\"\\nh2_total availability by month:\")\n",
    "print(monthly_h2)\n",
    "\n",
    "# Find the last date with h2_total\n",
    "h2_total_end_date = betting_h2_in_nba_range['date'].max()\n",
    "h2_total_start_date = betting_h2_in_nba_range['date'].min()\n",
    "\n",
    "print(f\"\\nüéØ H2_TOTAL DATE RANGE:\")\n",
    "print(f\"  Start: {h2_total_start_date}\")\n",
    "print(f\"  End: {h2_total_end_date}\")\n",
    "print(f\"  Duration: {(h2_total_end_date - h2_total_start_date).days} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# FILTER NBA GAMES TO H2_TOTAL RANGE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTERING NBA GAMES TO H2_TOTAL RANGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "games_in_h2_range = games[\n",
    "    (games['GAME_DATE'] >= h2_total_start_date) &\n",
    "    (games['GAME_DATE'] <= h2_total_end_date)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nOriginal NBA games: {len(games):,}\")\n",
    "print(f\"NBA games in h2_total range: {len(games_in_h2_range):,}\")\n",
    "print(f\"Excluded (outside h2_total range): {len(games) - len(games_in_h2_range):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEAM MAPPING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEAM MAPPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "team_map = {\n",
    "    'atl': 'ATL', 'bos': 'BOS', 'bkn': 'BKN', 'cha': 'CHA', 'chi': 'CHI',\n",
    "    'cle': 'CLE', 'dal': 'DAL', 'den': 'DEN', 'det': 'DET', 'gs': 'GSW',\n",
    "    'hou': 'HOU', 'ind': 'IND', 'lac': 'LAC', 'lal': 'LAL', 'mem': 'MEM',\n",
    "    'mia': 'MIA', 'mil': 'MIL', 'min': 'MIN', 'no': 'NOP', 'nyk': 'NYK',\n",
    "    'okc': 'OKC', 'orl': 'ORL', 'phi': 'PHI', 'phx': 'PHX', 'por': 'POR',\n",
    "    'sac': 'SAC', 'sa': 'SAS', 'tor': 'TOR', 'utah': 'UTA', 'wsh': 'WAS',\n",
    "    'nj': 'BKN', 'ny': 'NYK'\n",
    "}\n",
    "\n",
    "betting_h2_in_nba_range['away_team'] = betting_h2_in_nba_range['away'].map(team_map)\n",
    "betting_h2_in_nba_range['home_team'] = betting_h2_in_nba_range['home'].map(team_map)\n",
    "\n",
    "unmapped = betting_h2_in_nba_range[\n",
    "    betting_h2_in_nba_range['away_team'].isna() | \n",
    "    betting_h2_in_nba_range['home_team'].isna()\n",
    "]\n",
    "print(f\"Unmapped teams: {len(unmapped)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE MATCHUPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING MATCHUPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# NBA matchups\n",
    "games_in_h2_range['is_home'] = games_in_h2_range['MATCHUP'].str.contains('vs.', na=False)\n",
    "\n",
    "home_games = games_in_h2_range[games_in_h2_range['is_home'] == True][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "away_games = games_in_h2_range[games_in_h2_range['is_home'] == False][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "\n",
    "home_games.columns = ['GAME_ID', 'GAME_DATE', 'home_team']\n",
    "away_games.columns = ['GAME_ID', 'GAME_DATE', 'away_team']\n",
    "\n",
    "nba_matchups = home_games.merge(away_games, on=['GAME_ID', 'GAME_DATE'])\n",
    "\n",
    "print(f\"NBA matchups: {len(nba_matchups):,}\")\n",
    "\n",
    "# Create match keys\n",
    "nba_matchups['match_key'] = (\n",
    "    nba_matchups['GAME_DATE'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    nba_matchups['away_team'] + '_' + \n",
    "    nba_matchups['home_team']\n",
    ")\n",
    "\n",
    "betting_h2_in_nba_range['match_key'] = (\n",
    "    betting_h2_in_nba_range['date'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    betting_h2_in_nba_range['away_team'] + '_' + \n",
    "    betting_h2_in_nba_range['home_team']\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# JOIN ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JOIN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "nba_keys = set(nba_matchups['match_key'])\n",
    "betting_keys = set(betting_h2_in_nba_range['match_key'].dropna())\n",
    "\n",
    "matched = nba_keys & betting_keys\n",
    "nba_only = nba_keys - betting_keys\n",
    "betting_only = betting_keys - nba_keys\n",
    "\n",
    "print(f\"\\nüìä Match Results:\")\n",
    "print(f\"  NBA games (in h2_total range): {len(nba_keys):,}\")\n",
    "print(f\"  Betting lines with h2_total: {len(betting_keys):,}\")\n",
    "print(f\"  ‚úÖ Matched: {len(matched):,}\")\n",
    "print(f\"  ‚ö†Ô∏è  NBA games without h2_total: {len(nba_only):,}\")\n",
    "print(f\"  ‚ö†Ô∏è  Betting h2_total without NBA: {len(betting_only):,}\")\n",
    "\n",
    "match_rate = len(matched) / len(nba_keys) * 100 if len(nba_keys) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Match Rate: {match_rate:.1f}%\")\n",
    "print(f\"   ({len(matched):,} / {len(nba_keys):,} NBA games have h2_total)\")\n",
    "\n",
    "# ============================================================================\n",
    "# JOIN AND SAVE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform join\n",
    "nba_with_h2 = nba_matchups.merge(\n",
    "    betting_h2_in_nba_range[['match_key', 'h2_total', 'h2_spread', 'spread', 'total']],\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save main dataset\n",
    "nba_with_h2.to_csv('nba_games_with_h2_total.csv', index=False)\n",
    "print(f\"‚úÖ Saved nba_games_with_h2_total.csv\")\n",
    "print(f\"   Total games: {len(nba_with_h2):,}\")\n",
    "print(f\"   With h2_total: {nba_with_h2['h2_total'].notna().sum():,}\")\n",
    "print(f\"   Without h2_total: {nba_with_h2['h2_total'].isna().sum():,}\")\n",
    "\n",
    "# Save failed NBA joins (NBA games that should have h2_total but don't)\n",
    "if len(nba_only) > 0:\n",
    "    failed_nba = nba_matchups[nba_matchups['match_key'].isin(nba_only)]\n",
    "    failed_nba.to_csv('nba_games_missing_h2_total.csv', index=False)\n",
    "    print(f\"\\n‚ùå Saved nba_games_missing_h2_total.csv\")\n",
    "    print(f\"   NBA games without h2_total: {len(failed_nba):,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n   Sample games missing h2_total:\")\n",
    "    print(failed_nba[['GAME_DATE', 'away_team', 'home_team', 'GAME_ID']].head(10))\n",
    "    \n",
    "    # Group by date to see patterns\n",
    "    failed_by_date = failed_nba.groupby(failed_nba['GAME_DATE'].dt.date).size()\n",
    "    print(f\"\\n   Missing h2_total by date (first 20 days):\")\n",
    "    print(failed_by_date.head(20))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüéØ H2_TOTAL Coverage:\")\n",
    "print(f\"  Date range analyzed: {h2_total_start_date} to {h2_total_end_date}\")\n",
    "print(f\"  NBA games in range: {len(nba_matchups):,}\")\n",
    "print(f\"  Games with h2_total: {len(matched):,} ({match_rate:.1f}%)\")\n",
    "print(f\"  Games missing h2_total: {len(nba_only):,} ({len(nba_only)/len(nba_matchups)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÅ Files Created:\")\n",
    "print(f\"  - nba_games_with_h2_total.csv (use this for modeling)\")\n",
    "print(f\"  - nba_games_missing_h2_total.csv (failed joins to investigate)\")\n",
    "\n",
    "print(f\"\\nüí° Next Step:\")\n",
    "print(f\"   Review nba_games_missing_h2_total.csv to understand why {len(nba_only)} games\")\n",
    "print(f\"   within the h2_total date range don't have betting lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038943c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and holdout data...\n",
      "Holdout: 1328 rows\n",
      "\n",
      "Predicting...\n",
      "‚úÖ Saved holdout_predictions.csv ((1328, 1341))\n",
      "\n",
      "Cleaning predictions...\n",
      "‚úÖ Saved holdout_predictions_clean.csv ((1328, 7))\n",
      "‚úÖ Saved holdout_predictions_games.csv ((664, 7))\n",
      "\n",
      "üéâ Done!\n",
      "\n",
      "üìä Summary:\n",
      "   Total predictions: 1328\n",
      "   Unique games: 664\n",
      "   Games with h2_total: 664\n",
      "   Games with actuals: 664\n"
     ]
    }
   ],
   "source": [
    "# Predict Holdouts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Loading Model\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(\"xgboost_residual_model.json\")\n",
    "\n",
    "# Load Data\n",
    "holdout_data = pd.read_csv(\"holdout_data.csv\")\n",
    "print(f\"Holdout: {len(holdout_data)} rows\")\n",
    "\n",
    "# Features\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "target_col = \"actual_second_half_total\"\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "feature_cols = [c for c in holdout_data.columns if c not in metadata_cols + [target_col]]\n",
    "X_holdout = holdout_data[feature_cols].copy()\n",
    "\n",
    "# Impute missing values\n",
    "for col in X_holdout.columns:\n",
    "    if X_holdout[col].isnull().any():\n",
    "        X_holdout[col].fillna(X_holdout[col].median(), inplace=True)\n",
    "\n",
    "# Predict\n",
    "predicted_residuals = model.predict(X_holdout)\n",
    "\n",
    "# Write predictions to DF\n",
    "holdout_data[\"predicted_residual\"] = predicted_residuals\n",
    "holdout_data[\"predicted_total\"] = predicted_residuals + holdout_data[vegas_col]\n",
    "holdout_data[\"bet_edge\"] = predicted_residuals\n",
    "\n",
    "# Save complete dataset\n",
    "holdout_data.to_csv(\"holdout_predictions.csv\", index=False)\n",
    "print(f\"Saved holdout_predictions.csv\")\n",
    "\n",
    "## Clean up for a streamlined df.\n",
    "\n",
    "# Select key columns\n",
    "clean_predictions = holdout_data[[\n",
    "    'GAME_ID',\n",
    "    'GAME_DATE',\n",
    "    'TEAM_ID',\n",
    "    'h2_total',\n",
    "    'predicted_total',\n",
    "    'actual_second_half_total',\n",
    "    'bet_edge'\n",
    "]].copy()\n",
    "\n",
    "# Format dates\n",
    "clean_predictions['GAME_DATE'] = pd.to_datetime(clean_predictions['GAME_DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Rename columns\n",
    "clean_predictions.columns = [\n",
    "    'game_id',\n",
    "    'date',\n",
    "    'team_id',\n",
    "    'h2_total',\n",
    "    'predicted_total',\n",
    "    'actual_total',\n",
    "    'bet_edge'\n",
    "]\n",
    "\n",
    "# Sort by date\n",
    "clean_predictions = clean_predictions.sort_values('date')\n",
    "\n",
    "# Save clean (two rows per game - home and away teams)\n",
    "clean_predictions.to_csv('holdout_predictions_clean.csv', index=False)\n",
    "print(f\"Saved holdout_predictions_clean.csv\")\n",
    "\n",
    "# Save game-level (one row per game)\n",
    "game_level = clean_predictions.drop_duplicates('game_id').copy()\n",
    "game_level.to_csv('holdout_predictions_games.csv', index=False)\n",
    "print(f\"Saved holdout_predictions_games.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60f56721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " threshold  total_bets  win_rate  profit        roi\n",
      "      1.25          81 51.851852     -90  -1.010101\n",
      "      1.50          46 50.000000    -230  -4.545455\n",
      "      0.50         335 45.074627   -5140 -13.948440\n",
      "      2.00          18 44.444444    -300 -15.151515\n",
      "      1.00         149 44.295302   -2530 -15.436242\n",
      "      0.75         234 44.017094   -4110 -15.967366\n",
      "      1.75          30 43.333333    -570 -17.272727\n",
      "      2.50           8 37.500000    -250 -28.409091\n",
      "      2.75           8 37.500000    -250 -28.409091\n",
      "      2.25          11 36.363636    -370 -30.578512\n",
      "Saved threshold_optimization_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Finding betting thresholds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use games data\n",
    "holdout_data = pd.read_csv(\"holdout_predictions_games.csv\")\n",
    "\n",
    "# Filter to games with both actuals and betting lines\n",
    "bettable = holdout_data[\n",
    "    (holdout_data[\"h2_total\"].notna()) &\n",
    "    (holdout_data[\"actual_total\"].notna())\n",
    "].copy()\n",
    "\n",
    "# Comparison cols\n",
    "bet_edge = bettable[\"bet_edge\"].values\n",
    "actual_total = bettable[\"actual_total\"].values  # Changed\n",
    "vegas_line = bettable[\"h2_total\"].values\n",
    "\n",
    "# Results dict\n",
    "results = []\n",
    "\n",
    "# Testing a thresholds in a range .5 all the way to 7.25 in .25 increments\n",
    "thresholds = np.arange(0.5, 7.25, 0.25)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Filtering bets\n",
    "    bet_over = bet_edge > threshold\n",
    "    bet_under = bet_edge < -threshold\n",
    "    \n",
    "    # Count bets\n",
    "    over_count = bet_over.sum()\n",
    "    under_count = bet_under.sum()\n",
    "    total_bets = over_count + under_count\n",
    "    \n",
    "    if total_bets == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate wins\n",
    "    over_wins = ((actual_total > vegas_line) & bet_over).sum()\n",
    "    under_wins = ((actual_total < vegas_line) & bet_under).sum()\n",
    "    total_wins = over_wins + under_wins\n",
    "    \n",
    "    # Win rates\n",
    "    over_win_rate = over_wins / over_count * 100 if over_count > 0 else 0\n",
    "    under_win_rate = under_wins / under_count * 100 if under_count > 0 else 0\n",
    "    total_win_rate = total_wins / total_bets * 100\n",
    "    \n",
    "    # Profit with 110 odds\n",
    "    over_profit = (over_wins * 100) - ((over_count - over_wins) * 110)\n",
    "    under_profit = (under_wins * 100) - ((under_count - under_wins) * 110)\n",
    "    total_profit = over_profit + under_profit\n",
    "    \n",
    "    # ROI\n",
    "    total_risked = total_bets * 110\n",
    "    roi = (total_profit / total_risked * 100) if total_risked > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'total_bets': total_bets,\n",
    "        'over_bets': over_count,\n",
    "        'under_bets': under_count,\n",
    "        'total_wins': total_wins,\n",
    "        'over_wins': over_wins,\n",
    "        'under_wins': under_wins,\n",
    "        'win_rate': total_win_rate,\n",
    "        'over_win_rate': over_win_rate,\n",
    "        'under_win_rate': under_win_rate,\n",
    "        'profit': total_profit,\n",
    "        'roi': roi\n",
    "    })\n",
    "\n",
    "# CAst to DF\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# prints\n",
    "top_roi = results_df.nlargest(10, 'roi')\n",
    "print(top_roi[['threshold', 'total_bets', 'win_rate', 'profit', 'roi']].to_string(index=False))\n",
    "\n",
    "# Write results\n",
    "results_df.to_csv('threshold_optimization_results.csv', index=False)\n",
    "print(f\"Saved threshold_optimization_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
