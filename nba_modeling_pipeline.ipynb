{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nba_api.stats.endpoints import leaguegamelog\n",
    "from nba_api.stats.endpoints import leaguegamefinder\n",
    "from nba_api.stats.static import teams\n",
    "import time\n",
    "\n",
    "# Get 2023-24 season\n",
    "print(\"Fetching 2023-24 season...\")\n",
    "games_finder_24 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2023-24',\n",
    "    timeout=120\n",
    ")\n",
    "games_24 = games_finder_24.get_data_frames()[0]\n",
    "print(f\"2023-24: {len(games_24)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2022-23 season\n",
    "print(\"Fetching 2022-23 season...\")\n",
    "games_finder_23 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2022-23',\n",
    "    timeout=120\n",
    ")\n",
    "games_23 = games_finder_23.get_data_frames()[0]\n",
    "print(f\"2022-23: {len(games_23)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2021-22 season\n",
    "print(\"Fetching 2021-22 season...\")\n",
    "games_finder_22 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2021-22',\n",
    "    timeout=120\n",
    ")\n",
    "games_22 = games_finder_22.get_data_frames()[0]\n",
    "print(f\"2021-22: {len(games_22)} records\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get 2020-21 season\n",
    "print(\"Fetching 2020-21 season...\")\n",
    "games_finder_21 = leaguegamefinder.LeagueGameFinder(\n",
    "    season_nullable='2020-21',\n",
    "    timeout=120\n",
    ")\n",
    "games_21 = games_finder_21.get_data_frames()[0]\n",
    "print(f\"2020-21: {len(games_21)} records\")\n",
    "\n",
    "# Concatenate all seasons\n",
    "all_games = pd.concat([games_21, games_22, games_23, games_24], ignore_index=True)\n",
    "print(f\"\\nTotal combined records: {len(all_games)}\")\n",
    "\n",
    "# Get unique game IDs sorted\n",
    "unique_game_ids = sorted(all_games['GAME_ID'].dropna().unique())  # There are NAs to drop\n",
    "print(f\"Unique games: {len(unique_game_ids)}\")\n",
    "\n",
    "# Write to CSV\n",
    "all_games.to_csv('nba_games_2021_to_2024.csv', index=False)\n",
    "print(\"Written to 'nba_games_2021_to_2024.csv'\")\n",
    "\n",
    "# Write unique game IDs to separate file\n",
    "game_ids_df = pd.DataFrame({'GAME_ID': unique_game_ids})\n",
    "game_ids_df['GAME_ID'] = game_ids_df['GAME_ID'].astype(str)  # ensure string since there are zeros at the front\n",
    "game_ids_df.to_csv('unique_game_ids.csv', index=False)\n",
    "print(\"Written unique game IDs to 'unique_game_ids.csv'\")\n",
    "\n",
    "# Print Statements\n",
    "print(f\"Total records: {len(all_games)}\")\n",
    "print(f\"Unique games: {len(unique_game_ids)}\")\n",
    "print(f\"First game ID: {unique_game_ids[0]}\")\n",
    "print(f\"Last game ID: {unique_game_ids[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c3cac",
   "metadata": {},
   "source": [
    "With the NBA API it was a trial and error process to pull data. It did not seem like there was a definitive reason why data was not pulling. So we built a script that would simply run multiple passes over a unique games list and try to get a complete set of data. This proved to be successful. Even testing different sleep times proved to be unsuccessful.\n",
    "\n",
    "Basically we were collecting two sets of data one was from \"boxscoretraditionalv2\" which gave team and player stats. We needed to collect a set for the first half and the complete game this would give us a complete picture of the data so we could predict a second half point total. We also used \"boxscoresummaryv2\" to collect game times, referees, injuries, points by qtr. It would basically pull a list of 7 tables, the multiple pass code was neccesarily for this as it seemed to fail to pull more often. In addition we had to patch this API pull to code in 'GAME_ID' for a couple of the tables that did not have it present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42255892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting First Half Box Score Data - Multi-Pass Version\n",
    "from nba_api.stats.endpoints import boxscoretraditionalv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration - Change these for each pass\n",
    "PASS_NUMBER = 3\n",
    "INPUT_FILE = 'unique_game_ids.csv' if PASS_NUMBER == 1 else f'failed_game_ids_fh_pass{PASS_NUMBER-1}.csv'\n",
    "OUTPUT_SUFFIX = '' if PASS_NUMBER == 1 else f'_pt{PASS_NUMBER}'\n",
    "\n",
    "# Loading Game IDs\n",
    "game_ids_df = pd.read_csv(INPUT_FILE, dtype={'GAME_ID': str})\n",
    "game_ids_list = game_ids_df['GAME_ID'].tolist()\n",
    "\n",
    "# Initial Print Statements\n",
    "print(f\"=== PASS {PASS_NUMBER} ===\")\n",
    "print(f\"Total games to process: {len(game_ids_list)}\")\n",
    "print(f\"First game ID: {game_ids_list[0]}\")\n",
    "print(f\"Last game ID: {game_ids_list[-1]}\")\n",
    "\n",
    "# Lists to collect dataframes\n",
    "all_fh_player_stats = []\n",
    "all_fh_team_stats = []\n",
    "all_fh_starter_bench = []\n",
    "\n",
    "# Variables to track progress\n",
    "games_processed = 0\n",
    "games_failed = 0\n",
    "start_time = time.time()\n",
    "failed_game_ids = []\n",
    "\n",
    "# First Half Loop\n",
    "for idx, game_id in enumerate(game_ids_list):\n",
    "    try:\n",
    "        # API function\n",
    "        fh = boxscoretraditionalv2.BoxScoreTraditionalV2(\n",
    "            game_id=game_id,\n",
    "            range_type=1,\n",
    "            start_period=1,\n",
    "            end_period=2\n",
    "        )\n",
    "        \n",
    "        # Frames to DF\n",
    "        dfs = fh.get_data_frames()\n",
    "        \n",
    "        # Append\n",
    "        all_fh_player_stats.append(dfs[0])\n",
    "        all_fh_team_stats.append(dfs[1])\n",
    "        all_fh_starter_bench.append(dfs[2])\n",
    "        \n",
    "        # Track process\n",
    "        games_processed += 1\n",
    "        \n",
    "        # Progress Update every 1000 games\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Progress: {idx + 1}/{len(game_ids_list)} games ({games_processed} success, {games_failed} failed) - {elapsed/60:.1f} min elapsed\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        games_failed += 1\n",
    "        # Track failed game ID\n",
    "        failed_game_ids.append(game_id)\n",
    "        # Print at 1st failure and every 50\n",
    "        if games_failed == 1 or games_failed % 50 == 0:\n",
    "            print(f\"Failures: {games_failed}\")\n",
    "        continue\n",
    "\n",
    "# Combine into DFs\n",
    "if all_fh_player_stats:\n",
    "    fh_players_combined = pd.concat(all_fh_player_stats, ignore_index=True)\n",
    "    fh_teams_combined = pd.concat(all_fh_team_stats, ignore_index=True)\n",
    "    fh_starters_bench_combined = pd.concat(all_fh_starter_bench, ignore_index=True)\n",
    "    \n",
    "    # Save combined dataframes with suffix\n",
    "    fh_players_combined.to_csv(f'first_half_players{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    fh_teams_combined.to_csv(f'first_half_teams{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    fh_starters_bench_combined.to_csv(f'first_half_starters_bench{OUTPUT_SUFFIX}.csv', index=False)\n",
    "    \n",
    "    print(f\"Successfully processed: {games_processed}\")\n",
    "    print(f\"Player records: {len(fh_players_combined)}\")\n",
    "    print(f\"Team records: {len(fh_teams_combined)}\")\n",
    "else:\n",
    "    print(\"\\nNo data collected - all games failed\")\n",
    "\n",
    "# Save failed game IDs\n",
    "if failed_game_ids:\n",
    "    failed_df = pd.DataFrame({'GAME_ID': failed_game_ids})\n",
    "    failed_df.to_csv(f'failed_game_ids_fh_pass{PASS_NUMBER}.csv', index=False)\n",
    "    print(f\"\\nFailed game IDs saved to 'failed_game_ids_fh_pass{PASS_NUMBER}.csv'\")\n",
    "    print(f\"Failed games: {len(failed_game_ids)}\")\n",
    "    print(f\"\\nTo run pass {PASS_NUMBER + 1}:\")\n",
    "else:\n",
    "    print(f\"\\n All games successful - no need for pass {PASS_NUMBER + 1}!\")\n",
    "\n",
    "# Final Summary\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"PASS {PASS_NUMBER} COMPLETE\")\n",
    "print(f\"Successfully processed: {games_processed}/{len(game_ids_list)}\")\n",
    "print(f\"Failed: {games_failed}/{len(game_ids_list)}\")\n",
    "print(f\"Total time: {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84affc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Box Score Summary Data - Multi-Pass with Auto-Loop Version (with GAME_ID fix)\n",
    "from nba_api.stats.endpoints import boxscoresummaryv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MAX_PASSES = 15\n",
    "INITIAL_INPUT_FILE = 'unique_game_ids.csv'\n",
    "\n",
    "# Start with pass 1\n",
    "for pass_num in range(1, MAX_PASSES + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== STARTING PASS {pass_num} ===\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Determine input file\n",
    "    if pass_num == 1:\n",
    "        input_file = INITIAL_INPUT_FILE\n",
    "    else:\n",
    "        input_file = f'failed_game_ids_bss_pass{pass_num-1}.csv'\n",
    "        \n",
    "        # Check if there are failures to process\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"âœ… No failures from pass {pass_num-1} - All done!\")\n",
    "            break\n",
    "    \n",
    "    # Loading Game IDs\n",
    "    game_ids_df = pd.read_csv(input_file, dtype={'GAME_ID': str})\n",
    "    game_ids_list = game_ids_df['GAME_ID'].tolist()\n",
    "    \n",
    "    # Output suffix\n",
    "    output_suffix = '' if pass_num == 1 else f'_pt{pass_num}'\n",
    "    \n",
    "    # Initial Print Statements\n",
    "    print(f\"Total games to process: {len(game_ids_list)}\")\n",
    "    print(f\"First game ID: {game_ids_list[0]}\")\n",
    "    print(f\"Last game ID: {game_ids_list[-1]}\")\n",
    "    \n",
    "    # Lists to collect dataframes\n",
    "    all_game_summary = []\n",
    "    all_team_stats = []\n",
    "    all_refs = []\n",
    "    all_inactive = []\n",
    "    all_game_info = []\n",
    "    all_points_by_qtr = []\n",
    "    all_last_meeting = []\n",
    "    \n",
    "    # Variables to track progress\n",
    "    games_processed = 0\n",
    "    games_failed = 0\n",
    "    start_time = time.time()\n",
    "    failed_game_ids = []\n",
    "    \n",
    "    # Loop through games\n",
    "    for idx, game_id in enumerate(game_ids_list):\n",
    "        try:\n",
    "            # API function\n",
    "            summary = boxscoresummaryv2.BoxScoreSummaryV2(game_id=game_id)\n",
    "            \n",
    "            # Get all dataframes\n",
    "            dfs = summary.get_data_frames()\n",
    "            \n",
    "            # Add GAME_ID to each dataframe if it doesn't exist\n",
    "            for i in range(7):  # Only process 0-6, skip 7-8\n",
    "                if 'GAME_ID' not in dfs[i].columns:\n",
    "                    dfs[i]['GAME_ID'] = game_id\n",
    "            \n",
    "            # Append only the ones we want (0-6, drop 7-8)\n",
    "            all_game_summary.append(dfs[0])\n",
    "            all_team_stats.append(dfs[1])\n",
    "            all_refs.append(dfs[2])\n",
    "            all_inactive.append(dfs[3])\n",
    "            all_game_info.append(dfs[4])\n",
    "            all_points_by_qtr.append(dfs[5])\n",
    "            all_last_meeting.append(dfs[6])\n",
    "            \n",
    "            games_processed += 1\n",
    "            \n",
    "            # Progress Update every 1000 games\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"Progress: {idx + 1}/{len(game_ids_list)} games ({games_processed} success, {games_failed} failed) - {elapsed/60:.1f} min elapsed\")\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            games_failed += 1\n",
    "            failed_game_ids.append(game_id)\n",
    "            if games_failed == 1 or games_failed % 50 == 0:\n",
    "                print(f\"Failures: {games_failed}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine into DFs\n",
    "    if all_game_summary:\n",
    "        game_summary_combined = pd.concat(all_game_summary, ignore_index=True)\n",
    "        team_stats_combined = pd.concat(all_team_stats, ignore_index=True)\n",
    "        refs_combined = pd.concat(all_refs, ignore_index=True)\n",
    "        inactive_combined = pd.concat(all_inactive, ignore_index=True)\n",
    "        game_info_combined = pd.concat(all_game_info, ignore_index=True)\n",
    "        points_qtr_combined = pd.concat(all_points_by_qtr, ignore_index=True)\n",
    "        last_meeting_combined = pd.concat(all_last_meeting, ignore_index=True)\n",
    "        \n",
    "        # Save combined dataframes\n",
    "        game_summary_combined.to_csv(f'game_summary{output_suffix}.csv', index=False)\n",
    "        team_stats_combined.to_csv(f'team_stats{output_suffix}.csv', index=False)\n",
    "        refs_combined.to_csv(f'refs{output_suffix}.csv', index=False)\n",
    "        inactive_combined.to_csv(f'inactive_players{output_suffix}.csv', index=False)\n",
    "        game_info_combined.to_csv(f'game_info{output_suffix}.csv', index=False)\n",
    "        points_qtr_combined.to_csv(f'points_by_quarter{output_suffix}.csv', index=False)\n",
    "        last_meeting_combined.to_csv(f'last_meeting{output_suffix}.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nPass {pass_num} data saved!\")\n",
    "        print(f\"Successfully processed: {games_processed}\")\n",
    "    else:\n",
    "        print(f\"\\nPass {pass_num}: No data collected - all games failed\")\n",
    "    \n",
    "    # Save failed game IDs\n",
    "    if failed_game_ids:\n",
    "        failed_df = pd.DataFrame({'GAME_ID': failed_game_ids})\n",
    "        failed_df.to_csv(f'failed_game_ids_bss_pass{pass_num}.csv', index=False)\n",
    "        print(f\"Failed game IDs saved to 'failed_game_ids_bss_pass{pass_num}.csv'\")\n",
    "        print(f\"Failed games: {len(failed_game_ids)}\")\n",
    "    else:\n",
    "        print(f\"Pass {pass_num}: All games successful!\")\n",
    "    \n",
    "    # Final Summary for this pass\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"PASS {pass_num} COMPLETE\")\n",
    "    print(f\"Successfully processed: {games_processed}/{len(game_ids_list)}\")\n",
    "    print(f\"Failed: {games_failed}/{len(game_ids_list)}\")\n",
    "    print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "    \n",
    "    # Check if we should continue to next pass\n",
    "    if not failed_game_ids:\n",
    "        print(f\"\\nALL GAMES PROCESSED SUCCESSFULLY! No need for more passes.\")\n",
    "        break\n",
    "    elif pass_num < MAX_PASSES:\n",
    "        print(f\"\\nWill attempt pass {pass_num + 1} with {len(failed_game_ids)} failed games...\")\n",
    "        time.sleep(5)  # Brief pause between passes\n",
    "    else:\n",
    "        print(f\"\\nReached maximum passes ({MAX_PASSES}). {len(failed_game_ids)} games still failed.\")\n",
    "\n",
    "print(f\"ALL PASSES COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd268331",
   "metadata": {},
   "source": [
    "This next cell block is code to concatenate all the various pass files together. This could have been more elegant but I hard coded the amount of passes I had for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028dbed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the csvs\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Dictionary to store all combined dataframes\n",
    "combined_data = {}\n",
    "\n",
    "# First Half files (3 passes)\n",
    "fh_files = [\n",
    "    ('first_half_players', 3),\n",
    "    ('first_half_teams', 3),\n",
    "    ('first_half_starters_bench', 3)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in fh_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\" Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# Complete Game files (5 passes)\n",
    "cg_files = [\n",
    "    ('complete_game_players', 5),\n",
    "    ('complete_game_teams', 5),\n",
    "    ('complete_game_starters_bench', 5)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in cg_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# BoxScoreSummary files (9 passes)\n",
    "bss_files = [\n",
    "    ('game_summary', 9),\n",
    "    ('team_stats', 9),\n",
    "    ('refs', 9),\n",
    "    ('inactive_players', 9),\n",
    "    ('game_info', 9),\n",
    "    ('points_by_quarter', 9),\n",
    "    ('last_meeting', 9)\n",
    "]\n",
    "\n",
    "for base_name, num_passes in bss_files:\n",
    "    all_dfs = []\n",
    "    \n",
    "    for pass_num in range(1, num_passes + 1):\n",
    "        if pass_num == 1:\n",
    "            filename = f'{base_name}.csv'\n",
    "        else:\n",
    "            filename = f'{base_name}_pt{pass_num}.csv'\n",
    "        \n",
    "        if os.path.exists(filename):\n",
    "            df = pd.read_csv(filename)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"Loaded {filename}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"{filename} not found, skipping\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        combined = pd.concat(all_dfs, ignore_index=True)\n",
    "        combined_data[base_name] = combined\n",
    "        print(f\"  ðŸ“Š Combined {base_name}: {len(combined)} total records\\n\")\n",
    "\n",
    "# Save all combined files\n",
    "print(\"SAVING COMBINED FILES\")\n",
    "\n",
    "for name, df in combined_data.items():\n",
    "    output_filename = f'{name}_COMBINED.csv'\n",
    "    df.to_csv(output_filename, index=False)\n",
    "    print(f\"Saved {output_filename}: {len(df)} records\")\n",
    "\n",
    "print(f\"\\nAll files combined and saved!\")\n",
    "print(f\"Total combined datasets: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6081a",
   "metadata": {},
   "source": [
    "This next cell block does a quick pass to check if we have a complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254500da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing games in combined datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the original game IDs list\n",
    "original_game_ids = pd.read_csv('unique_game_ids.csv', dtype={'GAME_ID': str})\n",
    "total_games = len(original_game_ids)\n",
    "all_game_ids = set(original_game_ids['GAME_ID'].tolist())\n",
    "\n",
    "print(f\"Total games expected: {total_games}\")\n",
    "print(f\"First game ID: {original_game_ids['GAME_ID'].iloc[0]}\")\n",
    "print(f\"Last game ID: {original_game_ids['GAME_ID'].iloc[-1]}\")\n",
    "\n",
    "# Dictionary to track coverage by dataset\n",
    "coverage_report = {}\n",
    "\n",
    "# Check First Half datasets\n",
    "print(\"FIRST HALF DATASETS:\")\n",
    "fh_files = [\n",
    "    'first_half_players_COMBINED.csv',\n",
    "    'first_half_teams_COMBINED.csv',\n",
    "    'first_half_starters_bench_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in fh_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"  {filename}:\")\n",
    "            print(f\"    Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"    Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "\n",
    "# Check Complete Game datasets\n",
    "print(\"COMPLETE GAME DATASETS:\")\n",
    "cg_files = [\n",
    "    'complete_game_players_COMBINED.csv',\n",
    "    'complete_game_teams_COMBINED.csv',\n",
    "    'complete_game_starters_bench_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in cg_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "# Check BoxScoreSummary datasets\n",
    "print(\"BOX SCORE SUMMARY DATASETS:\")\n",
    "bss_files = [\n",
    "    'game_summary_COMBINED.csv',\n",
    "    'team_stats_COMBINED.csv',\n",
    "    'refs_COMBINED.csv',\n",
    "    'inactive_players_COMBINED.csv',\n",
    "    'game_info_COMBINED.csv',\n",
    "    'points_by_quarter_COMBINED.csv',\n",
    "    'last_meeting_COMBINED.csv'\n",
    "]\n",
    "\n",
    "for filename in bss_files:\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        if 'GAME_ID' in df.columns:\n",
    "            unique_games = df['GAME_ID'].nunique()\n",
    "            game_ids_in_file = set(df['GAME_ID'].dropna().astype(str).unique())\n",
    "            missing_games = all_game_ids - game_ids_in_file\n",
    "            coverage_pct = (unique_games / total_games) * 100\n",
    "            \n",
    "            coverage_report[filename] = {\n",
    "                'games_found': unique_games,\n",
    "                'missing_count': len(missing_games),\n",
    "                'coverage_pct': coverage_pct,\n",
    "                'missing_ids': sorted(missing_games)\n",
    "            }\n",
    "            \n",
    "            print(f\"{filename}:\")\n",
    "            print(f\"Games found: {unique_games}/{total_games} ({coverage_pct:.1f}%)\")\n",
    "            print(f\"Missing: {len(missing_games)}\")\n",
    "        else:\n",
    "            print(f\"{filename}: No GAME_ID column found\")\n",
    "    else:\n",
    "        print(f\"{filename} not found\")\n",
    "\n",
    "# Summary\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"Total expected games: {total_games}\")\n",
    "\n",
    "if coverage_report:\n",
    "    best_coverage = max(coverage_report.items(), key=lambda x: x[1]['coverage_pct'])\n",
    "    worst_coverage = min(coverage_report.items(), key=lambda x: x[1]['coverage_pct'])\n",
    "    \n",
    "    print(f\"\\nBest coverage: {best_coverage[0]}\")\n",
    "    print(f\"  {best_coverage[1]['games_found']}/{total_games} ({best_coverage[1]['coverage_pct']:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nWorst coverage: {worst_coverage[0]}\")\n",
    "    print(f\"  {worst_coverage[1]['games_found']}/{total_games} ({worst_coverage[1]['coverage_pct']:.1f}%)\")\n",
    "    \n",
    "    # Find games missing from ALL datasets\n",
    "    all_missing = set.intersection(*[set(v['missing_ids']) for v in coverage_report.values()])\n",
    "    \n",
    "    if all_missing:\n",
    "        print(f\"\\nGames missing from ALL datasets: {len(all_missing)}\")\n",
    "        print(f\"Sample missing IDs: {list(all_missing)[:10]}\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        missing_df = pd.DataFrame({'GAME_ID': sorted(all_missing)})\n",
    "        missing_df.to_csv('games_missing_from_all_datasets.csv', index=False)\n",
    "        print(f\" Saved to 'games_missing_from_all_datasets.csv'\")\n",
    "    else:\n",
    "        print(f\"\\nNo games are missing from ALL datasets!\")\n",
    "        print(\"(Some datasets may have missing games, but coverage varies)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368816b",
   "metadata": {},
   "source": [
    "Upon inspecting that data we had thoughts about pulling more specific player information but this was incredibly cumbersome with the API since we would have to make individual pulls on each PLAYER_ID. In thinking about it more it did not make sense to go through the trouble since we want the model to generalize well since we're using an old dataset and players move teams, players retire, and new players arrive.\n",
    "\n",
    "The next cell block is for feature engineering. It is a bit of a mess and desperately needs to be refactored. We added more code snippets to add more features to the datasets. The need for more opponent features created another nested loop which felt easier at the time but simply kept growing it would have been better to replace it with a function. Also the dataset got to be over 1,000 columns and I added some redundant columns because I forgot which ones were already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "time_windows = [7, 14, 30] # 1 week, 2 week, and 1 month rolling snapshots\n",
    "player_groups = [1, 2, 3, 4, 5, 6, 7, 'rest'] #top 7 of the rotation averages and lump sum for rest of team\n",
    "\n",
    "# Stats to average\n",
    "player_stats_avg = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "player_stats_total = ['MIN']\n",
    "\n",
    "# Team Complete Game\n",
    "team_stats_avg_cg = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "team_stats_total_cg = ['MIN']\n",
    "\n",
    "# Team First Half\n",
    "team_stats_avg_fh = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA',\n",
    "                    'OREB', 'DREB', 'REB', 'AST', 'STL', 'BLK', 'TO', 'PF', 'PLUS_MINUS']\n",
    "team_stats_total_fh = ['MIN']\n",
    "\n",
    "# Load our data\n",
    "cg_players = pd.read_csv('complete_game_players_COMBINED.csv')\n",
    "cg_teams = pd.read_csv('complete_game_teams_COMBINED.csv')\n",
    "fh_players = pd.read_csv('first_half_players_COMBINED.csv')\n",
    "fh_teams = pd.read_csv('first_half_teams_COMBINED.csv')\n",
    "games_master = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "game_summary = pd.read_csv('game_summary_COMBINED.csv')\n",
    "refs = pd.read_csv('refs_COMBINED.csv')\n",
    "last_meeting = pd.read_csv('last_meeting_COMBINED.csv')\n",
    "\n",
    "# Cast numeric for the above\n",
    "for col in player_stats_avg + player_stats_total:\n",
    "    if col in cg_players.columns:\n",
    "        cg_players[col] = pd.to_numeric(cg_players[col], errors='coerce')\n",
    "        fh_players[col] = pd.to_numeric(fh_players[col], errors='coerce')\n",
    "\n",
    "for col in team_stats_avg + team_stats_total:\n",
    "    if col in cg_teams.columns:\n",
    "        cg_teams[col] = pd.to_numeric(cg_teams[col], errors='coerce')\n",
    "        fh_teams[col] = pd.to_numeric(fh_teams[col], errors='coerce')\n",
    "\n",
    "# Format Dates\n",
    "games_master['GAME_DATE'] = pd.to_datetime(games_master['GAME_DATE'])\n",
    "last_meeting['LAST_GAME_DATE_EST'] = pd.to_datetime(last_meeting['LAST_GAME_DATE_EST'], errors='coerce')\n",
    "\n",
    "\n",
    "# Creating an index with unique game_id and date\n",
    "game_date_map = games_master[['GAME_ID', 'GAME_DATE']].drop_duplicates('GAME_ID').set_index('GAME_ID')['GAME_DATE']\n",
    "\n",
    "# Apply it\n",
    "for df in [cg_players, fh_players, cg_teams, fh_teams]:\n",
    "    df['GAME_DATE'] = df['GAME_ID'].map(game_date_map)\n",
    "\n",
    "## Computing Second-Half Stats since we pulled first half and complete game\n",
    "# Merge Key\n",
    "cg_teams['merge_key'] = cg_teams['GAME_ID'].astype(str) + '_' + cg_teams['TEAM_ID'].astype(str)\n",
    "fh_teams['merge_key'] = fh_teams['GAME_ID'].astype(str) + '_' + fh_teams['TEAM_ID'].astype(str)\n",
    "\n",
    "# Merge Alignment\n",
    "merged = cg_teams.merge(\n",
    "    fh_teams[['merge_key', 'PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA', 'REB', 'AST', 'TO', 'STL', 'BLK']], \n",
    "    on='merge_key', \n",
    "    how='left',\n",
    "    suffixes=('', '_fh')\n",
    ")\n",
    "\n",
    "# Second-half stats to compute\n",
    "second_half_stats = ['PTS', 'FGM', 'FGA', 'FG3M', 'FG3A', 'FTM', 'FTA', 'REB', 'AST', 'TO', 'STL', 'BLK']\n",
    "\n",
    "# Quick diff for counting stats\n",
    "for stat in second_half_stats:\n",
    "    cg_teams[f'second_half_{stat}'] = merged[stat] - merged[f'{stat}_fh']\n",
    "\n",
    "# Calculate percentage stats\n",
    "cg_teams['second_half_FG_PCT'] = np.where(\n",
    "    cg_teams['second_half_FGA'] > 0,\n",
    "    cg_teams['second_half_FGM'] / cg_teams['second_half_FGA'],\n",
    "    np.nan\n",
    ")\n",
    "cg_teams['second_half_FG3_PCT'] = np.where(\n",
    "    cg_teams['second_half_FG3A'] > 0,\n",
    "    cg_teams['second_half_FG3M'] / cg_teams['second_half_FG3A'],\n",
    "    np.nan\n",
    ")\n",
    "cg_teams['second_half_FT_PCT'] = np.where(\n",
    "    cg_teams['second_half_FTA'] > 0,\n",
    "    cg_teams['second_half_FTM'] / cg_teams['second_half_FTA'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Kill merge key since its unnessary now\n",
    "cg_teams.drop('merge_key', axis=1, inplace=True)\n",
    "fh_teams.drop('merge_key', axis=1, inplace=True)\n",
    "\n",
    "# Add it to the stats list\n",
    "team_stats_avg_cg.extend([f'second_half_{stat}' for stat in second_half_stats])\n",
    "team_stats_avg_cg.extend(['second_half_FG_PCT', 'second_half_FG3_PCT', 'second_half_FT_PCT'])\n",
    "\n",
    "## Players\n",
    "# Lasso stats into time windows\n",
    "def calculate_player_time_features(player_df, game_id, game_date, player_id, team_id, days):\n",
    "    window_start = game_date - pd.Timedelta(days=days)\n",
    "    \n",
    "    player_games = player_df[\n",
    "        (player_df['PLAYER_ID'] == player_id) &\n",
    "        (player_df['TEAM_ID'] == team_id) &\n",
    "        (player_df['GAME_DATE'] >= window_start) &\n",
    "        (player_df['GAME_DATE'] < game_date) &\n",
    "        (player_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    # Excludes inactivity\n",
    "    if len(player_games) == 0:\n",
    "        base_features = {stat: np.nan for stat in player_stats_total + player_stats_avg}\n",
    "        base_features['FG_PCT'] = np.nan\n",
    "        base_features['FG3_PCT'] = np.nan\n",
    "        base_features['FT_PCT'] = np.nan\n",
    "        return base_features\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Sum counting stats for percentages later\n",
    "    for stat in player_stats_total:\n",
    "        features[stat] = player_games[stat].sum()\n",
    "        \n",
    "    # Averages for features\n",
    "    for stat in player_stats_avg:\n",
    "        features[stat] = player_games[stat].mean()\n",
    "    \n",
    "    # Calculate percentages from underlying counting stats instead of averaging percentages\n",
    "    total_fgm = player_games['FGM'].sum()\n",
    "    total_fga = player_games['FGA'].sum()\n",
    "    total_fg3m = player_games['FG3M'].sum()\n",
    "    total_fg3a = player_games['FG3A'].sum()\n",
    "    total_ftm = player_games['FTM'].sum()\n",
    "    total_fta = player_games['FTA'].sum()\n",
    "    \n",
    "    features['FG_PCT'] = total_fgm / total_fga if total_fga > 0 else np.nan\n",
    "    features['FG3_PCT'] = total_fg3m / total_fg3a if total_fg3a > 0 else np.nan\n",
    "    features['FT_PCT'] = total_ftm / total_fta if total_fta > 0 else np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Ranking our top players, which is done on a 30 day basis\n",
    "# 30 day basis will show dropoffs of top players.\n",
    "def get_team_top_players(player_df, game_id, game_date, team_id):\n",
    "    window_start = game_date - pd.Timedelta(days=30)\n",
    "    \n",
    "    team_players = player_df[\n",
    "        (player_df['TEAM_ID'] == team_id) &\n",
    "        (player_df['GAME_DATE'] >= window_start) &\n",
    "        (player_df['GAME_DATE'] < game_date) &\n",
    "        (player_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    # Excludes inactivity\n",
    "    if len(team_players) == 0:\n",
    "        return []\n",
    "    \n",
    "    player_minutes = team_players.groupby('PLAYER_ID')['MIN'].sum().sort_values(ascending=False)\n",
    "    return player_minutes.index.tolist()\n",
    "\n",
    "## Teamwide\n",
    "# Lasso stats into time windows\n",
    "def calculate_team_time_features(team_df, game_id, game_date, team_id, days, stats_avg, stats_total):\n",
    "    window_start = game_date - pd.Timedelta(days=days)\n",
    "    \n",
    "    team_games = team_df[\n",
    "        (team_df['TEAM_ID'] == team_id) &\n",
    "        (team_df['GAME_DATE'] >= window_start) &\n",
    "        (team_df['GAME_DATE'] < game_date) &\n",
    "        (team_df['GAME_ID'] != game_id)\n",
    "    ].copy()\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        base_features = {stat: np.nan for stat in stats_total + stats_avg}  # Changed\n",
    "        base_features['FG_PCT'] = np.nan\n",
    "        base_features['FG3_PCT'] = np.nan\n",
    "        base_features['FT_PCT'] = np.nan\n",
    "        base_features['WIN_PCT'] = np.nan\n",
    "        return base_features\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Sum totals\n",
    "    for stat in stats_total:  # Changed\n",
    "        features[stat] = team_games[stat].sum()\n",
    "    \n",
    "    # Average the averages (including second-half stats for CG only)\n",
    "    for stat in stats_avg:  # Changed\n",
    "        features[stat] = team_games[stat].mean()\n",
    "    \n",
    "    # Calculate TRUE percentages from summed makes/attempts\n",
    "    total_fgm = team_games['FGM'].sum()\n",
    "    total_fga = team_games['FGA'].sum()\n",
    "    total_fg3m = team_games['FG3M'].sum()\n",
    "    total_fg3a = team_games['FG3A'].sum()\n",
    "    total_ftm = team_games['FTM'].sum()\n",
    "    total_fta = team_games['FTA'].sum()\n",
    "    \n",
    "    features['FG_PCT'] = total_fgm / total_fga if total_fga > 0 else np.nan\n",
    "    features['FG3_PCT'] = total_fg3m / total_fg3a if total_fg3a > 0 else np.nan\n",
    "    features['FT_PCT'] = total_ftm / total_fta if total_fta > 0 else np.nan\n",
    "    \n",
    "    # Win percentage\n",
    "    team_game_ids = team_games['GAME_ID'].unique()\n",
    "    team_game_results = games_master[\n",
    "        (games_master['GAME_ID'].isin(team_game_ids)) &\n",
    "        (games_master['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    if len(team_game_results) > 0:\n",
    "        features['WIN_PCT'] = (team_game_results['WL'] == 'W').mean()\n",
    "    else:\n",
    "        features['WIN_PCT'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Functions for Schedule\n",
    "# Rest days, back-to-back games, and some general game density\n",
    "def calculate_schedule_features(games_master, game_id, game_date, team_id):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        features['days_since_last_game'] = np.nan\n",
    "        features['is_back_to_back'] = 0\n",
    "        features['games_in_last_3_days'] = 0\n",
    "        features['games_in_last_5_days'] = 0\n",
    "        features['games_in_last_7_days'] = 0\n",
    "    else:\n",
    "        # Here we compute last game and back to backs\n",
    "        last_game_date = team_games['GAME_DATE'].iloc[-1]\n",
    "        features['days_since_last_game'] = (game_date - last_game_date).days\n",
    "        features['is_back_to_back'] = 1 if features['days_since_last_game'] == 1 else 0\n",
    "        \n",
    "        # Game density\n",
    "        recent_games_3d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=3)]\n",
    "        recent_games_5d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=5)]\n",
    "        recent_games_7d = team_games[team_games['GAME_DATE'] >= game_date - pd.Timedelta(days=7)]\n",
    "        \n",
    "        features['games_in_last_3_days'] = len(recent_games_3d)\n",
    "        features['games_in_last_5_days'] = len(recent_games_5d)\n",
    "        features['games_in_last_7_days'] = len(recent_games_7d)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Lets get home and road game lengths (streaks of home or away)\n",
    "def calculate_home_road_context(games_master, game_id, game_date, team_id):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # Determine if current game is home\n",
    "    current_game = games_master[(games_master['GAME_ID'] == game_id) & (games_master['TEAM_ID'] == team_id)]\n",
    "    if len(current_game) > 0:\n",
    "        matchup = current_game['MATCHUP'].iloc[0]\n",
    "        features['is_home_game'] = 1 if 'vs.' in str(matchup) else 0 # vs and @ determined home vs away\n",
    "    else:\n",
    "        features['is_home_game'] = np.nan\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        # First game of season - count as game #1\n",
    "        features['home_stand_game_number'] = 1 if features['is_home_game'] == 1 else 0\n",
    "        features['road_trip_game_number'] = 1 if features['is_home_game'] == 0 else 0\n",
    "        return features\n",
    "    \n",
    "    # Count consecutive home or road games (BEFORE current game)\n",
    "    consecutive_count = 0\n",
    "    \n",
    "    for _, game in team_games.iloc[::-1].iterrows():\n",
    "        matchup = str(game['MATCHUP'])\n",
    "        is_home = 1 if 'vs.' in matchup else 0\n",
    "        \n",
    "        # Check if this past game matches current game's location\n",
    "        if features['is_home_game'] == is_home:\n",
    "            consecutive_count += 1\n",
    "        else:\n",
    "            break  # Different arena, stop counting\n",
    "    \n",
    "    # Add 1 to include the CURRENT game\n",
    "    features['home_stand_game_number'] = consecutive_count + 1 if features['is_home_game'] == 1 else 0\n",
    "    features['road_trip_game_number'] = consecutive_count + 1 if features['is_home_game'] == 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Record Features\n",
    "# Win and Loss streaks\n",
    "def calculate_streak_features(games_master, game_id, game_date, team_id, lookback_games=[3, 5, 10]):\n",
    "    team_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ].sort_values('GAME_DATE')\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    if len(team_games) == 0:\n",
    "        features['current_streak'] = 0\n",
    "        for n in lookback_games:\n",
    "            features[f'wins_last_{n}'] = np.nan\n",
    "        features['home_win_pct_last_10'] = np.nan\n",
    "        features['road_win_pct_last_10'] = np.nan\n",
    "        return features\n",
    "    \n",
    "    # Current streak (positive = wins, negative = losses)\n",
    "    recent_results = team_games['WL'].iloc[::-1].values\n",
    "    current_result = recent_results[0] if len(recent_results) > 0 else None\n",
    "    streak = 0\n",
    "    \n",
    "    for result in recent_results:\n",
    "        if result == current_result:\n",
    "            streak += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    features['current_streak'] = streak if current_result == 'W' else -streak\n",
    "    \n",
    "    # Wins in last N games\n",
    "    for n in lookback_games:\n",
    "        last_n = team_games.tail(n)\n",
    "        if len(last_n) > 0:\n",
    "            features[f'wins_last_{n}'] = (last_n['WL'] == 'W').sum()\n",
    "        else:\n",
    "            features[f'wins_last_{n}'] = np.nan\n",
    "    \n",
    "    # Home/Road splits\n",
    "    last_10 = team_games.tail(10)\n",
    "    home_games = last_10[last_10['MATCHUP'].str.contains('vs.', na=False)]\n",
    "    road_games = last_10[last_10['MATCHUP'].str.contains('@', na=False)]\n",
    "    \n",
    "    features['home_win_pct_last_10'] = (home_games['WL'] == 'W').mean() if len(home_games) > 0 else np.nan\n",
    "    features['road_win_pct_last_10'] = (road_games['WL'] == 'W').mean() if len(road_games) > 0 else np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Head to head record calculations (Team matched against their Opponent)\n",
    "def calculate_head_to_head_features(games_master, last_meeting, game_id, game_date, team_id):\n",
    "    features = {}\n",
    "    \n",
    "    # Get opponent team_id for this game\n",
    "    game_teams = games_master[games_master['GAME_ID'] == game_id]['TEAM_ID'].unique()\n",
    "    opponent_id = [t for t in game_teams if t != team_id]\n",
    "    \n",
    "    if len(opponent_id) == 0:\n",
    "        features['days_since_last_matchup'] = np.nan\n",
    "        features['won_last_matchup'] = np.nan\n",
    "        features['point_diff_last_matchup'] = np.nan\n",
    "        features['season_record_vs_opponent'] = np.nan\n",
    "        return features\n",
    "    \n",
    "    opponent_id = opponent_id[0]\n",
    "    \n",
    "    # Last meeting info\n",
    "    last_mtg = last_meeting[last_meeting['GAME_ID'] == game_id]\n",
    "    if len(last_mtg) > 0 and pd.notna(last_mtg['LAST_GAME_DATE_EST'].iloc[0]):\n",
    "        last_game_date = last_mtg['LAST_GAME_DATE_EST'].iloc[0]\n",
    "        features['days_since_last_matchup'] = (game_date - last_game_date).days\n",
    "        \n",
    "        # Determine who won\n",
    "        home_pts = last_mtg['LAST_GAME_HOME_TEAM_POINTS'].iloc[0]\n",
    "        visitor_pts = last_mtg['LAST_GAME_VISITOR_TEAM_POINTS'].iloc[0]\n",
    "        home_team = last_mtg['LAST_GAME_HOME_TEAM_ID'].iloc[0]\n",
    "        \n",
    "        if home_team == team_id:\n",
    "            features['won_last_matchup'] = 1 if home_pts > visitor_pts else 0\n",
    "            features['point_diff_last_matchup'] = home_pts - visitor_pts\n",
    "        else:\n",
    "            features['won_last_matchup'] = 1 if visitor_pts > home_pts else 0\n",
    "            features['point_diff_last_matchup'] = visitor_pts - home_pts\n",
    "    else:\n",
    "        features['days_since_last_matchup'] = np.nan\n",
    "        features['won_last_matchup'] = np.nan\n",
    "        features['point_diff_last_matchup'] = np.nan\n",
    "    \n",
    "    # Season record vs opponent\n",
    "    season_games = games_master[\n",
    "        (games_master['TEAM_ID'] == team_id) &\n",
    "        (games_master['GAME_DATE'] < game_date)\n",
    "    ]\n",
    "    \n",
    "    # Find games against this opponent\n",
    "    opponent_game_ids = games_master[\n",
    "        (games_master['TEAM_ID'] == opponent_id)\n",
    "    ]['GAME_ID'].unique()\n",
    "    \n",
    "    matchup_games = season_games[season_games['GAME_ID'].isin(opponent_game_ids)]\n",
    "    \n",
    "    if len(matchup_games) > 0:\n",
    "        features['season_record_vs_opponent'] = (matchup_games['WL'] == 'W').sum()\n",
    "    else:\n",
    "        features['season_record_vs_opponent'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "## Referee Features -- basically a join\n",
    "def get_referee_ids(refs, game_id):\n",
    "    game_refs = refs[refs['GAME_ID'] == game_id].sort_values('JERSEY_NUM')\n",
    "    \n",
    "    ref_ids = [np.nan, np.nan, np.nan]\n",
    "    for i, (_, ref) in enumerate(game_refs.iterrows()):\n",
    "        if i < 3:\n",
    "            ref_ids[i] = ref['OFFICIAL_ID']\n",
    "    \n",
    "    return {\n",
    "        'ref_1_id': ref_ids[0],\n",
    "        'ref_2_id': ref_ids[1],\n",
    "        'ref_3_id': ref_ids[2]\n",
    "    }\n",
    "\n",
    "## Opponent features -- this is calculating features for the opponent\n",
    "def get_opponent_id(games_master, game_id, team_id):\n",
    "    \"\"\"Get the opponent team ID for this game\"\"\"\n",
    "    game_teams = games_master[games_master['GAME_ID'] == game_id]['TEAM_ID'].unique()\n",
    "    opponent_ids = [t for t in game_teams if t != team_id]\n",
    "    return opponent_ids[0] if len(opponent_ids) > 0 else None\n",
    "\n",
    "# Unique filter to make sure we are not counting duplicates\n",
    "unique_games = games_master[['GAME_ID', 'GAME_DATE', 'TEAM_ID']].drop_duplicates()\n",
    "unique_games = unique_games.sort_values('GAME_DATE')\n",
    "\n",
    "# Index Reset\n",
    "unique_games = unique_games.reset_index(drop=True)\n",
    "\n",
    "# Preparing to run it\n",
    "all_features = []\n",
    "\n",
    "# Looping over all games\n",
    "for idx, row in unique_games.iterrows():    \n",
    "    if idx % 500 == 0:\n",
    "        print(f\"Progress: {idx}/{len(unique_games)}\")\n",
    "    \n",
    "    game_id = row['GAME_ID']\n",
    "    game_date = row['GAME_DATE']\n",
    "    team_id = row['TEAM_ID']\n",
    "    \n",
    "    game_features = {\n",
    "        'GAME_ID': game_id,\n",
    "        'GAME_DATE': game_date,\n",
    "        'TEAM_ID': team_id\n",
    "    }\n",
    "    \n",
    "    # Player features\n",
    "    top_players_cg = get_team_top_players(cg_players, game_id, game_date, team_id)\n",
    "    top_players_fh = get_team_top_players(fh_players, game_id, game_date, team_id)\n",
    "    \n",
    "    for days in time_windows:\n",
    "        # Complete game players\n",
    "        for n in player_groups:\n",
    "            if n == 'rest':\n",
    "                players = top_players_cg[7:] if len(top_players_cg) > 7 else []\n",
    "            else:\n",
    "                players = top_players_cg[:n] if len(top_players_cg) >= n else []\n",
    "            \n",
    "            if len(players) == 0:\n",
    "                for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                    game_features[f'top{n}_cg_{stat.lower()}_{days}d'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            player_features_list = [\n",
    "                calculate_player_time_features(cg_players, game_id, game_date, p, team_id, days)\n",
    "                for p in players\n",
    "            ]\n",
    "            \n",
    "            for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                values = [pf[stat] for pf in player_features_list if not pd.isna(pf[stat])]\n",
    "                game_features[f'top{n}_cg_{stat.lower()}_{days}d'] = np.mean(values) if values else np.nan\n",
    "        \n",
    "        # First half players\n",
    "        for n in player_groups:\n",
    "            if n == 'rest':\n",
    "                players = top_players_fh[7:] if len(top_players_fh) > 7 else []\n",
    "            else:\n",
    "                players = top_players_fh[:n] if len(top_players_fh) >= n else []\n",
    "            \n",
    "            if len(players) == 0:\n",
    "                for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                    game_features[f'top{n}_fh_{stat.lower()}_{days}d'] = np.nan\n",
    "                continue\n",
    "            \n",
    "            player_features_list = [\n",
    "                calculate_player_time_features(fh_players, game_id, game_date, p, team_id, days)\n",
    "                for p in players\n",
    "            ]\n",
    "            \n",
    "            for stat in player_stats_total + player_stats_avg + ['FG_PCT', 'FG3_PCT', 'FT_PCT']:\n",
    "                values = [pf[stat] for pf in player_features_list if not pd.isna(pf[stat])]\n",
    "                game_features[f'top{n}_fh_{stat.lower()}_{days}d'] = np.mean(values) if values else np.nan\n",
    "    \n",
    "    # Team features\n",
    "    for days in time_windows:\n",
    "    # Complete game with second-half stats\n",
    "        team_cg = calculate_team_time_features(cg_teams, game_id, game_date, team_id, days, \n",
    "                                                team_stats_avg_cg, team_stats_total_cg)\n",
    "        for stat, value in team_cg.items():\n",
    "            game_features[f'team_cg_{stat.lower()}_{days}d'] = value\n",
    "        \n",
    "        # First half WITHOUT second-half stats\n",
    "        team_fh = calculate_team_time_features(fh_teams, game_id, game_date, team_id, days,\n",
    "                                                team_stats_avg_fh, team_stats_total_fh)\n",
    "        for stat, value in team_fh.items():\n",
    "            game_features[f'team_fh_{stat.lower()}_{days}d'] = value\n",
    "    \n",
    "    # Schedule and Record Featurres\n",
    "    schedule_feats = calculate_schedule_features(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(schedule_feats)\n",
    "    \n",
    "    home_road_feats = calculate_home_road_context(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(home_road_feats)\n",
    "    \n",
    "    streak_feats = calculate_streak_features(games_master, game_id, game_date, team_id)\n",
    "    game_features.update(streak_feats)\n",
    "    \n",
    "    h2h_feats = calculate_head_to_head_features(games_master, last_meeting, game_id, game_date, team_id)\n",
    "    game_features.update(h2h_feats)\n",
    "    \n",
    "    # Ref features\n",
    "    ref_feats = get_referee_ids(refs, game_id)\n",
    "    game_features.update(ref_feats)\n",
    "    \n",
    "    # Current Game Stats columns\n",
    "    current_game_fh = fh_teams[\n",
    "        (fh_teams['GAME_ID'] == game_id) & \n",
    "        (fh_teams['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    \n",
    "    if len(current_game_fh) > 0:\n",
    "        game_features['current_fh_pts'] = current_game_fh['PTS'].iloc[0]\n",
    "        game_features['current_fh_fgm'] = current_game_fh['FGM'].iloc[0]\n",
    "        game_features['current_fh_fga'] = current_game_fh['FGA'].iloc[0]\n",
    "        game_features['current_fh_fg3m'] = current_game_fh['FG3M'].iloc[0]\n",
    "        game_features['current_fh_fg3a'] = current_game_fh['FG3A'].iloc[0]\n",
    "        game_features['current_fh_ftm'] = current_game_fh['FTM'].iloc[0]\n",
    "        game_features['current_fh_fta'] = current_game_fh['FTA'].iloc[0]\n",
    "        game_features['current_fh_reb'] = current_game_fh['REB'].iloc[0]\n",
    "        game_features['current_fh_ast'] = current_game_fh['AST'].iloc[0]\n",
    "        game_features['current_fh_to'] = current_game_fh['TO'].iloc[0]\n",
    "        game_features['current_fh_stl'] = current_game_fh['STL'].iloc[0]\n",
    "        game_features['current_fh_blk'] = current_game_fh['BLK'].iloc[0]\n",
    "        game_features['current_fh_pf'] = current_game_fh['PF'].iloc[0]\n",
    "        \n",
    "        # Calculate percentages and pace\n",
    "        fga = current_game_fh['FGA'].iloc[0]\n",
    "        fg3a = current_game_fh['FG3A'].iloc[0]\n",
    "        fta = current_game_fh['FTA'].iloc[0]\n",
    "        \n",
    "        game_features['current_fh_fg_pct'] = current_game_fh['FGM'].iloc[0] / fga if fga > 0 else np.nan\n",
    "        game_features['current_fh_fg3_pct'] = current_game_fh['FG3M'].iloc[0] / fg3a if fg3a > 0 else np.nan\n",
    "        game_features['current_fh_ft_pct'] = current_game_fh['FTM'].iloc[0] / fta if fta > 0 else np.nan\n",
    "        game_features['current_fh_pace'] = fga + current_game_fh['TO'].iloc[0]\n",
    "    else:\n",
    "        fh_stats = ['pts', 'fgm', 'fga', 'fg3m', 'fg3a', 'ftm', 'fta', 'reb', 'ast', 'to', \n",
    "                    'stl', 'blk', 'pf', 'fg_pct', 'fg3_pct', 'ft_pct', 'pace']\n",
    "        for stat in fh_stats:\n",
    "            game_features[f'current_fh_{stat}'] = np.nan\n",
    "\n",
    "    ## Opponnet Features\n",
    "    opponent_id = get_opponent_id(games_master, game_id, team_id)\n",
    "    \n",
    "    if opponent_id is not None:\n",
    "        # Opponent team features for each time window\n",
    "        for days in time_windows:\n",
    "            opp_team_cg = calculate_team_time_features(cg_teams, game_id, game_date, opponent_id, days,\n",
    "                                                        team_stats_avg_cg, team_stats_total_cg)\n",
    "            for stat, value in opp_team_cg.items():\n",
    "                game_features[f'opp_team_cg_{stat.lower()}_{days}d'] = value\n",
    "            \n",
    "            opp_team_fh = calculate_team_time_features(fh_teams, game_id, game_date, opponent_id, days,\n",
    "                                                        team_stats_avg_fh, team_stats_total_fh)\n",
    "            for stat, value in opp_team_fh.items():\n",
    "                game_features[f'opp_team_fh_{stat.lower()}_{days}d'] = value\n",
    "        \n",
    "        # Opponent schedule features\n",
    "        opp_schedule_feats = calculate_schedule_features(games_master, game_id, game_date, opponent_id)\n",
    "        for key, value in opp_schedule_feats.items():\n",
    "            game_features[f'opp_{key}'] = value\n",
    "        \n",
    "        # Opponent streak features\n",
    "        opp_streak_feats = calculate_streak_features(games_master, game_id, game_date, opponent_id)\n",
    "        for key, value in opp_streak_feats.items():\n",
    "            game_features[f'opp_{key}'] = value\n",
    "            \n",
    "        # Team vs Opponent Differentials\n",
    "        if not pd.isna(game_features.get('days_since_last_game')) and not pd.isna(game_features.get('opp_days_since_last_game')):\n",
    "            game_features['rest_advantage'] = game_features['days_since_last_game'] - game_features['opp_days_since_last_game']\n",
    "        else:\n",
    "            game_features['rest_advantage'] = np.nan\n",
    "        \n",
    "        # Win percentage differential\n",
    "        for days in time_windows:\n",
    "            team_win_pct = game_features.get(f'team_cg_win_pct_{days}d', np.nan)\n",
    "            opp_win_pct = game_features.get(f'opp_team_cg_win_pct_{days}d', np.nan)\n",
    "            if not pd.isna(team_win_pct) and not pd.isna(opp_win_pct):\n",
    "                game_features[f'win_pct_diff_{days}d'] = team_win_pct - opp_win_pct\n",
    "            else:\n",
    "                game_features[f'win_pct_diff_{days}d'] = np.nan\n",
    "        \n",
    "        # Recent form differential (wins in last 5)\n",
    "        team_wins_5 = game_features.get('wins_last_5', np.nan)\n",
    "        opp_wins_5 = game_features.get('opp_wins_last_5', np.nan)\n",
    "        if not pd.isna(team_wins_5) and not pd.isna(opp_wins_5):\n",
    "            game_features['recent_form_diff'] = team_wins_5 - opp_wins_5\n",
    "        else:\n",
    "            game_features['recent_form_diff'] = np.nan\n",
    "        \n",
    "        # Scoring differential\n",
    "        for days in time_windows:\n",
    "            team_pts = game_features.get(f'team_cg_pts_{days}d', np.nan)\n",
    "            opp_pts = game_features.get(f'opp_team_cg_pts_{days}d', np.nan)\n",
    "            if not pd.isna(team_pts) and not pd.isna(opp_pts):\n",
    "                game_features[f'avg_scoring_diff_{days}d'] = team_pts - opp_pts\n",
    "            else:\n",
    "                game_features[f'avg_scoring_diff_{days}d'] = np.nan\n",
    "        \n",
    "        # Opponent Current Game Columns        \n",
    "        opp_game_fh = fh_teams[\n",
    "            (fh_teams['GAME_ID'] == game_id) & \n",
    "            (fh_teams['TEAM_ID'] == opponent_id)\n",
    "        ]\n",
    "        \n",
    "        if len(opp_game_fh) > 0:\n",
    "            game_features['opp_current_fh_pts'] = opp_game_fh['PTS'].iloc[0]\n",
    "            game_features['opp_current_fh_fgm'] = opp_game_fh['FGM'].iloc[0]\n",
    "            game_features['opp_current_fh_fga'] = opp_game_fh['FGA'].iloc[0]\n",
    "            game_features['opp_current_fh_fg3m'] = opp_game_fh['FG3M'].iloc[0]\n",
    "            game_features['opp_current_fh_fg3a'] = opp_game_fh['FG3A'].iloc[0]\n",
    "            game_features['opp_current_fh_reb'] = opp_game_fh['REB'].iloc[0]\n",
    "            game_features['opp_current_fh_ast'] = opp_game_fh['AST'].iloc[0]\n",
    "            game_features['opp_current_fh_to'] = opp_game_fh['TO'].iloc[0]\n",
    "            game_features['opp_current_fh_pf'] = opp_game_fh['PF'].iloc[0]\n",
    "            \n",
    "            opp_fga = opp_game_fh['FGA'].iloc[0]\n",
    "            opp_fg3a = opp_game_fh['FG3A'].iloc[0]\n",
    "            \n",
    "            game_features['opp_current_fh_fg_pct'] = opp_game_fh['FGM'].iloc[0] / opp_fga if opp_fga > 0 else np.nan\n",
    "            game_features['opp_current_fh_fg3_pct'] = opp_game_fh['FG3M'].iloc[0] / opp_fg3a if opp_fg3a > 0 else np.nan\n",
    "            game_features['opp_current_fh_pace'] = opp_fga + opp_game_fh['TO'].iloc[0]\n",
    "        else:\n",
    "            opp_fh_stats = ['pts', 'fgm', 'fga', 'fg3m', 'fg3a', 'reb', 'ast', 'to', 'pf', \n",
    "                            'fg_pct', 'fg3_pct', 'pace']\n",
    "            for stat in opp_fh_stats:\n",
    "                game_features[f'opp_current_fh_{stat}'] = np.nan\n",
    "        \n",
    "        # Halftime Stats        \n",
    "        if len(current_game_fh) > 0 and len(opp_game_fh) > 0:\n",
    "            \n",
    "            # Totals\n",
    "            game_features['halftime_total'] = current_game_fh['PTS'].iloc[0] + opp_game_fh['PTS'].iloc[0]\n",
    "            \n",
    "            # PAce\n",
    "            team_pace = game_features.get('current_fh_pace', 0)\n",
    "            opp_pace = game_features.get('opp_current_fh_pace', 0)\n",
    "            game_features['halftime_total_pace'] = team_pace + opp_pace\n",
    "            \n",
    "            # Shooting PCTs\n",
    "            team_fg_pct = game_features.get('current_fh_fg_pct', np.nan)\n",
    "            opp_fg_pct = game_features.get('opp_current_fh_fg_pct', np.nan)\n",
    "            if not pd.isna(team_fg_pct) and not pd.isna(opp_fg_pct):\n",
    "                game_features['halftime_combined_fg_pct'] = (team_fg_pct + opp_fg_pct) / 2\n",
    "            else:\n",
    "                game_features['halftime_combined_fg_pct'] = np.nan\n",
    "            \n",
    "            # Turnovers\n",
    "            game_features['halftime_total_to'] = current_game_fh['TO'].iloc[0] + opp_game_fh['TO'].iloc[0]\n",
    "            \n",
    "            # Scoring\n",
    "            team_avg = game_features.get('team_fh_pts_7d', np.nan)\n",
    "            opp_avg = game_features.get('opp_team_fh_pts_7d', np.nan)\n",
    "            team_current = current_game_fh['PTS'].iloc[0]\n",
    "            opp_current = opp_game_fh['PTS'].iloc[0]\n",
    "            \n",
    "            if not pd.isna(team_avg) and not pd.isna(opp_avg):\n",
    "                team_var = team_current - team_avg\n",
    "                opp_var = opp_current - opp_avg\n",
    "                game_features['halftime_combined_scoring_variance'] = team_var + opp_var\n",
    "            else:\n",
    "                game_features['halftime_combined_scoring_variance'] = np.nan\n",
    "            \n",
    "            # Lead\n",
    "            game_features['halftime_lead_abs'] = abs(current_game_fh['PTS'].iloc[0] - opp_game_fh['PTS'].iloc[0])\n",
    "        \n",
    "        else:\n",
    "            game_features['halftime_total'] = np.nan\n",
    "            game_features['halftime_total_pace'] = np.nan\n",
    "            game_features['halftime_combined_fg_pct'] = np.nan\n",
    "            game_features['halftime_total_to'] = np.nan\n",
    "            game_features['halftime_combined_scoring_variance'] = np.nan\n",
    "            game_features['halftime_lead_abs'] = np.nan\n",
    "    \n",
    "    ## Getting Second Half Targets\n",
    "    current_game_cg = cg_teams[\n",
    "        (cg_teams['GAME_ID'] == game_id) & \n",
    "        (cg_teams['TEAM_ID'] == team_id)\n",
    "    ]\n",
    "    \n",
    "    # Team's second-half score\n",
    "    if len(current_game_cg) > 0 and len(current_game_fh) > 0:\n",
    "        complete_pts = current_game_cg['PTS'].iloc[0]\n",
    "        first_half_pts = current_game_fh['PTS'].iloc[0]\n",
    "        game_features['actual_second_half_pts'] = complete_pts - first_half_pts\n",
    "    else:\n",
    "        game_features['actual_second_half_pts'] = np.nan\n",
    "    \n",
    "    # Opponent's second-half score\n",
    "    if opponent_id is not None:\n",
    "        opp_game_cg = cg_teams[\n",
    "            (cg_teams['GAME_ID'] == game_id) & \n",
    "            (cg_teams['TEAM_ID'] == opponent_id)\n",
    "        ]\n",
    "        \n",
    "        if len(opp_game_cg) > 0 and len(opp_game_fh) > 0:\n",
    "            opp_complete_pts = opp_game_cg['PTS'].iloc[0]\n",
    "            opp_first_half_pts = opp_game_fh['PTS'].iloc[0]\n",
    "            opp_second_half_pts = opp_complete_pts - opp_first_half_pts\n",
    "            \n",
    "            # Both teams combined for second half (this is the target)\n",
    "            if not pd.isna(game_features['actual_second_half_pts']) and not pd.isna(opp_second_half_pts):\n",
    "                game_features['actual_second_half_total'] = game_features['actual_second_half_pts'] + opp_second_half_pts\n",
    "            else:\n",
    "                game_features['actual_second_half_total'] = np.nan\n",
    "        else:\n",
    "            game_features['actual_second_half_total'] = np.nan\n",
    "    else:\n",
    "        game_features['actual_second_half_total'] = np.nan\n",
    "\n",
    "    all_features.append(game_features)\n",
    "    \n",
    "# Create dataframe\n",
    "features_df = pd.DataFrame(all_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccbddbe",
   "metadata": {},
   "source": [
    "Used this to inspect the dataset and then also write the dataset. They're split as a relic of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()\n",
    "features_df.to_csv('nba_time_based_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2263bf",
   "metadata": {},
   "source": [
    "Next cell is using a mapping table to try and create a key based on the team and the date so we can join it to our NBA dataset. There were a number of games missing and a big bulk of those is because we only have half of the 2023 season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930364f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping table for betting line dataset to be joined to NBA data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "betting_data = pd.read_csv('betting_data.csv')\n",
    "games_master = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "\n",
    "# Format dates\n",
    "betting_data['date'] = pd.to_datetime(betting_data['date'])\n",
    "games_master['GAME_DATE'] = pd.to_datetime(games_master['GAME_DATE'])\n",
    "\n",
    "# Create team abbreviation mapping\n",
    "team_map = {\n",
    "    'atl': 'ATL', 'bos': 'BOS', 'bkn': 'BKN', 'cha': 'CHA', 'chi': 'CHI',\n",
    "    'cle': 'CLE', 'dal': 'DAL', 'den': 'DEN', 'det': 'DET', 'gs': 'GSW',\n",
    "    'hou': 'HOU', 'ind': 'IND', 'lac': 'LAC', 'lal': 'LAL', 'mem': 'MEM',\n",
    "    'mia': 'MIA', 'mil': 'MIL', 'min': 'MIN', 'no': 'NOP', 'nyk': 'NYK',\n",
    "    'okc': 'OKC', 'orl': 'ORL', 'phi': 'PHI', 'phx': 'PHX', 'por': 'POR',\n",
    "    'sac': 'SAC', 'sa': 'SAS', 'tor': 'TOR', 'utah': 'UTA', 'wsh': 'WAS',\n",
    "    'nj': 'BKN', 'ny': 'NYK'\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "betting_data['away_team'] = betting_data['away'].map(team_map)\n",
    "betting_data['home_team'] = betting_data['home'].map(team_map)\n",
    "\n",
    "# Use dates and teams to join\n",
    "betting_data['match_key'] = (\n",
    "    betting_data['date'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    betting_data['away_team'] + '_' + \n",
    "    betting_data['home_team']\n",
    ")\n",
    "\n",
    "# Create a key for NBA games\n",
    "games_master['is_home'] = games_master['MATCHUP'].str.contains('vs.', na=False)\n",
    "\n",
    "# Separate home and away games\n",
    "home_games = games_master[games_master['is_home'] == True][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "away_games = games_master[games_master['is_home'] == False][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "\n",
    "home_games.columns = ['GAME_ID', 'GAME_DATE', 'home_team']\n",
    "away_games.columns = ['GAME_ID', 'GAME_DATE', 'away_team']\n",
    "\n",
    "# Need a version for home and away\n",
    "game_matchups = home_games.merge(away_games, on=['GAME_ID', 'GAME_DATE'])\n",
    "\n",
    "# Create key for NBA games\n",
    "game_matchups['match_key'] = (\n",
    "    game_matchups['GAME_DATE'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    game_matchups['away_team'] + '_' + \n",
    "    game_matchups['home_team']\n",
    ")\n",
    "\n",
    "# Join based on keys\n",
    "game_id_target = game_matchups.merge(\n",
    "    betting_data[['match_key', 'h2_total']],\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Keep only GAME_ID and h2_total\n",
    "game_id_target = game_id_target[['GAME_ID', 'h2_total']].drop_duplicates()\n",
    "\n",
    "print(f\"\\nCreated GAME_ID to h2_total mapping\")\n",
    "print(f\"Total games: {len(game_id_target)}\")\n",
    "print(f\"Games with h2_total: {game_id_target['h2_total'].notna().sum()}\")\n",
    "print(f\"Games missing h2_total: {game_id_target['h2_total'].isna().sum()}\")\n",
    "\n",
    "# Save the mapping\n",
    "game_id_target.to_csv('game_id_to_h2_total.csv', index=False)\n",
    "print(f\"\\nSaved to game_id_to_h2_total.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ce94e",
   "metadata": {},
   "source": [
    "Cell below actually does the join. Upon further investigation we realized that the failed joins were preseason and international exhibition games so we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62608b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Betting Lines On and Filter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features_df = pd.read_csv('nba_time_based_features.csv')\n",
    "game_id_target = pd.read_csv('game_id_to_h2_total.csv')\n",
    "\n",
    "# Join h2_total to features\n",
    "features_with_target = features_df.merge(\n",
    "    game_id_target,\n",
    "    on='GAME_ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"After join: {features_with_target.shape}\")\n",
    "print(f\"Rows with h2_total: {features_with_target['h2_total'].notna().sum()}\")\n",
    "print(f\"Rows without h2_total: {features_with_target['h2_total'].isna().sum()}\")\n",
    "\n",
    "# Drop rows without h2_total (preseason/exhibition games)\n",
    "features_with_target = features_with_target[features_with_target['h2_total'].notna()]\n",
    "\n",
    "print(f\"\\nAfter dropping preseason/exhibition: {features_with_target.shape}\")\n",
    "\n",
    "# Save the joined dataset\n",
    "print(f\"\\nSaving to nba_features_with_target.csv...\")\n",
    "features_with_target.to_csv('nba_features_with_target.csv', index=False)\n",
    "print(f\"Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4791",
   "metadata": {},
   "source": [
    "This next cell block preps the data for modelling. First it drops all columns that would cost data leaks (basically ones that have second half information) since we are using walk forward building dates are fine. We are testing on the second most recent season we had betting information on and before that is train data. We held out the last partial season of data for seperate validation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e3efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test splits for data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('nba_features_with_target.csv')\n",
    "df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'])\n",
    "\n",
    "# Drop leakage columns\n",
    "drop_columns = [\n",
    "    # Rolling second-half team stats\n",
    "    'team_cg_second_half_pts_7d', 'team_cg_second_half_fgm_7d', \n",
    "    'team_cg_second_half_fga_7d', 'team_cg_second_half_fg3m_7d',\n",
    "    'team_cg_second_half_fg3a_7d', 'team_cg_second_half_ftm_7d',\n",
    "    'team_cg_second_half_fta_7d', 'team_cg_second_half_reb_7d',\n",
    "    'team_cg_second_half_ast_7d', 'team_cg_second_half_to_7d',\n",
    "    'team_cg_second_half_stl_7d', 'team_cg_second_half_blk_7d',\n",
    "    'team_cg_second_half_fg_pct_7d', 'team_cg_second_half_fg3_pct_7d',\n",
    "    'team_cg_second_half_ft_pct_7d', 'team_cg_second_half_pts_14d',\n",
    "    'team_cg_second_half_fgm_14d', 'team_cg_second_half_fga_14d',\n",
    "    'team_cg_second_half_fg3m_14d', 'team_cg_second_half_fg3a_14d',\n",
    "    'team_cg_second_half_ftm_14d', 'team_cg_second_half_fta_14d',\n",
    "    'team_cg_second_half_reb_14d', 'team_cg_second_half_ast_14d',\n",
    "    'team_cg_second_half_to_14d', 'team_cg_second_half_stl_14d',\n",
    "    'team_cg_second_half_blk_14d', 'team_cg_second_half_fg_pct_14d',\n",
    "    'team_cg_second_half_fg3_pct_14d', 'team_cg_second_half_ft_pct_14d',\n",
    "    'team_cg_second_half_pts_30d', 'team_cg_second_half_fgm_30d',\n",
    "    'team_cg_second_half_fga_30d', 'team_cg_second_half_fg3m_30d',\n",
    "    'team_cg_second_half_fg3a_30d', 'team_cg_second_half_ftm_30d',\n",
    "    'team_cg_second_half_fta_30d', 'team_cg_second_half_reb_30d',\n",
    "    'team_cg_second_half_ast_30d', 'team_cg_second_half_to_30d',\n",
    "    'team_cg_second_half_stl_30d', 'team_cg_second_half_blk_30d',\n",
    "    'team_cg_second_half_fg_pct_30d', 'team_cg_second_half_fg3_pct_30d',\n",
    "    'team_cg_second_half_ft_pct_30d',\n",
    "\n",
    "    # Direct post-game leakage columns\n",
    "    'actual_second_half_pts', 'actual_second_half_fgm',\n",
    "    'actual_second_half_fga', 'actual_second_half_fg3m',\n",
    "    'actual_second_half_fg3a', 'actual_second_half_ftm',\n",
    "    'actual_second_half_fta', 'actual_second_half_reb',\n",
    "    'actual_second_half_ast', 'actual_second_half_to',\n",
    "    'actual_second_half_stl', 'actual_second_half_blk',\n",
    "    'actual_second_half_fg_pct', 'actual_second_half_fg3_pct',\n",
    "    'actual_second_half_ft_pct', 'actual_second_half_plus_minus'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=drop_columns, errors='ignore')\n",
    "\n",
    "# Get seasons\n",
    "def get_nba_season(date):\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    return f\"{year}-{year+1}\" if month >= 10 else f\"{year-1}-{year}\"\n",
    "\n",
    "df['season'] = df['GAME_DATE'].apply(get_nba_season)\n",
    "df = df.sort_values('GAME_DATE')\n",
    "\n",
    "seasons = sorted(df['season'].unique())\n",
    "\n",
    "# Train on earliest seasons, test on second-to-last, holdout on last (where we have partial data to validate seperately)\n",
    "holdout_season = seasons[-1]\n",
    "test_season = seasons[-2]\n",
    "train_seasons = seasons[:-2]\n",
    "\n",
    "# Filter data (need both target and betting line)\n",
    "train_data = df[\n",
    "    (df['season'].isin(train_seasons)) & \n",
    "    (df['actual_second_half_total'].notna()) &\n",
    "    (df['h2_total'].notna())\n",
    "]\n",
    "\n",
    "test_data = df[\n",
    "    (df['season'] == test_season) & \n",
    "    (df['actual_second_half_total'].notna()) &\n",
    "    (df['h2_total'].notna())\n",
    "]\n",
    "\n",
    "holdout_data = df[\n",
    "    (df['season'] == holdout_season) & \n",
    "    (df['actual_second_half_total'].notna())\n",
    "]\n",
    "\n",
    "# Print statements to give us a sense of dataset size\n",
    "print(f\"\\nTrain: {len(train_data)} games\")\n",
    "print(f\"Test: {len(test_data)} games\")\n",
    "print(f\"Holdout: {len(holdout_data)} games\")\n",
    "\n",
    "# Prepare features (exclude metadata, target, and betting line)\n",
    "metadata_cols = ['GAME_ID', 'GAME_DATE', 'TEAM_ID', 'season']\n",
    "target_col = 'actual_second_half_total'\n",
    "betting_line_col = 'h2_total'\n",
    "\n",
    "feature_cols = [c for c in df.columns \n",
    "                if c not in metadata_cols + [target_col, betting_line_col]]\n",
    "\n",
    "# Save datasets\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "holdout_data.to_csv('holdout_data.csv', index=False)\n",
    "\n",
    "with open('feature_list.txt', 'w') as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "print(\"\\nSaved: train_data.csv, test_data.csv, holdout_data.csv, feature_list.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556de189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First XGBoost Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Format Dates\n",
    "train_data['GAME_DATE'] = pd.to_datetime(train_data['GAME_DATE'])\n",
    "test_data['GAME_DATE'] = pd.to_datetime(test_data['GAME_DATE'])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = ['GAME_ID', 'GAME_DATE', 'TEAM_ID', 'season']\n",
    "\n",
    "# Target\n",
    "target_col = 'actual_second_half_total'\n",
    "\n",
    "# Comparison at end not used in modelling\n",
    "betting_line_col = 'h2_total'\n",
    "\n",
    "# Feature columns\n",
    "drop_from_features = set(metadata_cols + [target_col])\n",
    "if betting_line_col in train_data.columns:\n",
    "    drop_from_features.add(betting_line_col)\n",
    "\n",
    "feature_cols = [c for c in train_data.columns if c not in drop_from_features]\n",
    "\n",
    "# Split into X and y\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data[target_col]\n",
    "\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data[target_col]\n",
    "\n",
    "# Imputer using Median\n",
    "\n",
    "missing_train = X_train.isnull().sum()\n",
    "cols_with_missing = missing_train[missing_train > 0]\n",
    "\n",
    "if len(cols_with_missing) > 0:\n",
    "    for col in cols_with_missing.index:\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)  # use train medians for test\n",
    "    print(\"Missing values handled!\")\n",
    "else:\n",
    "    print(\"No missing values!\")\n",
    "\n",
    "## Model\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=0.05,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=120,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "\n",
    "# Evalutation \n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"MAE:  {mae_test:.3f} points\")\n",
    "print(f\"RMSE: {rmse_test:.3f} points\")\n",
    "print(f\"RÂ²:   {r2_test:.3f}\")\n",
    "\n",
    "# Export Feature Importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Store Predictions\n",
    "test_results = test_data.copy()\n",
    "test_results['actual_second_half_total_predicted'] = y_pred_test\n",
    "test_results['prediction_error'] = np.abs(y_test - y_pred_test)\n",
    "\n",
    "show_cols = ['GAME_ID', 'GAME_DATE', 'actual_second_half_total',\n",
    "            'actual_second_half_total_predicted', 'prediction_error']\n",
    "if betting_line_col in test_results.columns:\n",
    "    test_results['edge_vs_line'] = test_results['actual_second_half_total_predicted'] - test_results[betting_line_col]\n",
    "    show_cols.append(betting_line_col)\n",
    "    show_cols.append('edge_vs_line')\n",
    "\n",
    "\n",
    "# Save model and results\n",
    "model.save_model('xgboost_model.json')\n",
    "print(\"Saved xgboost_model.json\")\n",
    "\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(\"Saved feature_importance.csv\")\n",
    "\n",
    "test_results.to_csv('test_predictions.csv', index=False)\n",
    "print(\"Saved test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost using h2_total residual\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Format dates\n",
    "train_data[\"GAME_DATE\"] = pd.to_datetime(train_data[\"GAME_DATE\"])\n",
    "test_data[\"GAME_DATE\"] = pd.to_datetime(test_data[\"GAME_DATE\"])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Residual\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = [c for c in train_data.columns if c not in metadata_cols + [target_col]]\n",
    "\n",
    "X_train = train_data[feature_cols].copy()\n",
    "# Add Residual Here\n",
    "y_train = train_data[target_col] - train_data[vegas_col]\n",
    "\n",
    "\n",
    "X_test = test_data[feature_cols].copy()\n",
    "# Add Residual Here\n",
    "y_test = test_data[target_col] - test_data[vegas_col]\n",
    "\n",
    "# Imputer for missing values uses median\n",
    "\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].isnull().any():\n",
    "        median_val = X_train[col].median()\n",
    "        X_train[col].fillna(median_val, inplace=True)\n",
    "        X_test[col].fillna(median_val, inplace=True)\n",
    "\n",
    "## Model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1800,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.75,\n",
    "    gamma=0.05,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.6,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=120,\n",
    "    eval_metric='rmse'\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Evalutation\n",
    "\n",
    "y_pred_resid = model.predict(X_test)\n",
    "mae_resid = mean_absolute_error(y_test, y_pred_resid)\n",
    "rmse_resid = np.sqrt(mean_squared_error(y_test, y_pred_resid))\n",
    "r2_resid = r2_score(y_test, y_pred_resid)\n",
    "\n",
    "print(f\"\\nResidual Model Metrics:\")\n",
    "print(f\"MAE:  {mae_resid:.3f} pts\")\n",
    "print(f\"RMSE: {rmse_resid:.3f} pts\")\n",
    "print(f\"RÂ²:   {r2_resid:.3f}\")\n",
    "\n",
    "# Reconstruct Predictions\n",
    "y_pred_total = y_pred_resid + X_test[vegas_col]\n",
    "y_true_total = y_test + X_test[vegas_col]\n",
    "\n",
    "mae_total = mean_absolute_error(y_true_total, y_pred_total)\n",
    "rmse_total = np.sqrt(mean_squared_error(y_true_total, y_pred_total))\n",
    "r2_total = r2_score(y_true_total, y_pred_total)\n",
    "\n",
    "print(\"FULL TOTAL PERFORMANCE (Actual Second Half Totals)\")\n",
    "print(f\"MAE:  {mae_total:.3f} pts\")\n",
    "print(f\"RMSE: {rmse_total:.3f} pts\")\n",
    "print(f\"RÂ²:   {r2_total:.3f}\")\n",
    "\n",
    "# Prepping test results\n",
    "test_results = test_data.copy()\n",
    "test_results[\"predicted_residual\"] = y_pred_resid\n",
    "test_results[\"predicted_total\"] = y_pred_total\n",
    "test_results[\"residual_error\"] = y_test - y_pred_resid\n",
    "test_results[\"actual_residual\"] = y_test\n",
    "test_results[\"edge_vs_vegas\"] = y_pred_resid\n",
    "\n",
    "# Save everything\n",
    "model.save_model(\"xgboost_residual_model.json\")\n",
    "test_results.to_csv(\"test_predictions_residual.csv\", index=False)\n",
    "\n",
    "print(\"Saved model (xgboost_residual_model.json)\")\n",
    "print(\"Saved predictions (test_predictions_residual.csv)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load models\n",
    "resid = pd.read_csv(\"test_predictions.csv\")                  # main (no residual) model\n",
    "no_resid = pd.read_csv(\"test_predictions_residual.csv\")      # residual model\n",
    "\n",
    "# I named the columns badly...\n",
    "def get_pred_col(df):\n",
    "    if \"predicted_total\" in df.columns:\n",
    "        return \"predicted_total\"\n",
    "    elif \"actual_second_half_total_predicted\" in df.columns:\n",
    "        return \"actual_second_half_total_predicted\"\n",
    "    else:\n",
    "        raise KeyError(\"still can't get the columns right\")\n",
    "\n",
    "pred_col_resid = get_pred_col(resid)\n",
    "pred_col_no_resid = get_pred_col(no_resid)\n",
    "\n",
    "# Targets\n",
    "y_true = resid[\"actual_second_half_total\"]\n",
    "y_vegas = resid[\"h2_total\"]\n",
    "\n",
    "# Predictions\n",
    "y_pred_no_residual = resid[pred_col_resid]\n",
    "y_pred_residual = no_resid[pred_col_no_resid]\n",
    "\n",
    "# Evaluator function\n",
    "def evaluate(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Metrics Calc\n",
    "results = {\n",
    "    \"Vegas Line\": evaluate(y_true, y_vegas),\n",
    "    \"No Residual Model\": evaluate(y_true, y_pred_no_residual),\n",
    "    \"Residual Model\": evaluate(y_true, y_pred_residual),\n",
    "}\n",
    "\n",
    "# Summary output\n",
    "summary = pd.DataFrame(results, index=[\"MAE\", \"RMSE\", \"RÂ²\"]).T\n",
    "summary[\"Î” MAE vs Vegas\"] = summary[\"MAE\"] - summary.loc[\"Vegas Line\", \"MAE\"]\n",
    "summary[\"Î” RMSE vs Vegas\"] = summary[\"RMSE\"] - summary.loc[\"Vegas Line\", \"RMSE\"]\n",
    "\n",
    "print(\"SECOND HALF TOTALS â€“ MODEL BENCHMARK COMPARISON\")\n",
    "print(summary.to_string(float_format=lambda x: f\"{x:0.3f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0f916",
   "metadata": {},
   "source": [
    "Basically as we were testing we found out that the Vegas line is actually very very good. Even without using first half data its actually better then our tuned XGBoost model. Knowing that we have the Vegas second half total we used that as a residual in another XGBoost model, which gave us some improvements over the Vegas line. Is this enough to make betting model that is profitable? We do not quite know yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ce12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Loading training data\n",
    "df = pd.read_csv(\"train_data.csv\")\n",
    "df[\"GAME_DATE\"] = pd.to_datetime(df[\"GAME_DATE\"])\n",
    "\n",
    "## Sorting Columns\n",
    "# Filters\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Comparison col\n",
    "betting_col = \"h2_total\"\n",
    "\n",
    "# Drop unnecceasry columns and keep only features\n",
    "drop_cols = set(metadata_cols + [target_col, betting_col])\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# Imputer with medium\n",
    "missing = X.isna().sum().sum()\n",
    "if missing > 0:\n",
    "    for c in feature_cols:\n",
    "        if X[c].isna().any():\n",
    "            X[c].fillna(X[c].median(), inplace=True)\n",
    "else:\n",
    "    print(\"No imputing neccesary\")\n",
    "\n",
    "\n",
    "# Scale and apply PCA\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# keep up to 30 PCs or fewer if features < 30\n",
    "n_components = min(30, X_scaled.shape[1])\n",
    "pca = PCA(n_components=n_components, svd_solver=\"auto\", random_state=42)\n",
    "X_pcs = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_.cumsum()\n",
    "var_99 = np.argmax(explained >= 0.99) + 1 if (explained >= 0.99).any() else n_components\n",
    "\n",
    "print(f\"\\nPCA components: {n_components}\")\n",
    "print(f\"Explained variance (first 5 PCs cumulative): {explained[:5]}\")\n",
    "print(f\"Components to reach ~99% variance: {var_99}\")\n",
    "\n",
    "# KMeans test and evaluating based on silhouette\n",
    "best = {\"k\": None, \"score\": -1, \"model\": None, \"labels\": None}\n",
    "for k in [6, 8, 10, 12]:\n",
    "    km = KMeans(n_clusters=k, n_init=20, max_iter=500, random_state=42)\n",
    "    labels = km.fit_predict(X_pcs)\n",
    "    score = silhouette_score(X_pcs, labels) if len(np.unique(labels)) > 1 else -1\n",
    "    print(f\"k={k:2d} -> silhouette: {score:.4f}\")\n",
    "    if score > best[\"score\"]:\n",
    "        best = {\"k\": k, \"score\": score, \"model\": km, \"labels\": labels}\n",
    "\n",
    "k_best = best[\"k\"]\n",
    "kmeans = best[\"model\"]\n",
    "labels = best[\"labels\"]\n",
    "print(f\"\\nSelected k={k_best} (silhouette={best['score']:.4f})\")\n",
    "\n",
    "# Display Cluster Information\n",
    "pc_cols = [f\"PC{i+1}\" for i in range(X_pcs.shape[1])]\n",
    "out = df[[\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]].copy()\n",
    "for i, col in enumerate(pc_cols):\n",
    "    out[col] = X_pcs[:, i]\n",
    "out[\"cluster_id\"] = labels\n",
    "\n",
    "# Adding target col and betting col to explore clusters\n",
    "if target_col in df.columns:\n",
    "    out[target_col] = df[target_col].values\n",
    "if betting_col in df.columns:\n",
    "    out[betting_col] = df[betting_col].values\n",
    "    if target_col in df.columns:\n",
    "        out[\"edge_vs_line\"] = out[target_col] - out[betting_col]\n",
    "\n",
    "out.to_csv(\"pca_kmeans_train.csv\", index=False)\n",
    "\n",
    "# Summary for Cluster\n",
    "if target_col in out.columns:\n",
    "    agg_cols[target_col] = [\"mean\", \"median\"]\n",
    "if betting_col in out.columns:\n",
    "    agg_cols[betting_col] = [\"mean\", \"median\"]\n",
    "if \"edge_vs_line\" in out.columns:\n",
    "    agg_cols[\"edge_vs_line\"] = [\"mean\", \"median\"]\n",
    "\n",
    "summary = (\n",
    "    out.groupby(\"cluster_id\")\n",
    "        .agg(**{k: pd.NamedAgg(column=k, aggfunc=v) for k, v in {\n",
    "           **({target_col: \"mean\"} if target_col in out.columns else {}),\n",
    "           **({betting_col: \"mean\"} if betting_col in out.columns else {}),\n",
    "           **({\"edge_vs_line\": \"mean\"} if \"edge_vs_line\" in out.columns else {})\n",
    "        }.items()})\n",
    "        .rename(columns={\n",
    "            target_col: \"avg_actual_2H_total\",\n",
    "            betting_col: \"avg_h2_total\",\n",
    "            \"edge_vs_line\": \"avg_edge_vs_line\"\n",
    "        })\n",
    "        .reset_index()\n",
    ")\n",
    "sizes = out[\"cluster_id\"].value_counts().rename_axis(\"cluster_id\").reset_index(name=\"count\")\n",
    "cluster_summary = sizes.merge(summary, on=\"cluster_id\", how=\"left\")\n",
    "cluster_summary.sort_values(\"count\", ascending=False, inplace=True)\n",
    "cluster_summary.to_csv(\"cluster_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nCluster Summary (top 8):\")\n",
    "print(cluster_summary.head(8).to_string(index=False))\n",
    "\n",
    "# Save unsupervised learning to pickle files\n",
    "joblib.dump(scaler, \"unsup_scaler.pkl\")\n",
    "joblib.dump(pca, \"unsup_pca.pkl\")\n",
    "joblib.dump(kmeans, \"unsup_kmeans.pkl\")\n",
    "print(\"\\nSaved: unsup_scaler.pkl, unsup_pca.pkl, unsup_kmeans.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eabf0e",
   "metadata": {},
   "source": [
    " PCA performing well is not surprising so many of the features we created were derivatives of stats that are already present. So it makes sense that PCA was able to compress these down into 30 features, there might only be 30 features that observe different qualities of an NBA game from the dataset.\n",
    "\n",
    " Cluster doees not seem to add much of anything. The silhouettes were incredibly small. We think this makes sense because the NBA is a multi-billion dollar industry and it makes sense that its converged similar team composition, style of play, and game outcomes.  Yes there is innovation and variance but teams are incentivized so heavily to win that we do not actually see these innovations completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the XGBoost with residual with the PCA treatment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nLoading train and test data...\")\n",
    "train = pd.read_csv(\"train_data.csv\")\n",
    "test = pd.read_csv(\"test_data.csv\")\n",
    "\n",
    "# Format Data\n",
    "for df in (train, test):\n",
    "    if 'GAME_DATE' in df.columns:\n",
    "        df['GAME_DATE'] = pd.to_datetime(df['GAME_DATE'], errors='coerce')\n",
    "\n",
    "## Columns\n",
    "# Filter\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"actual_second_half_total\"\n",
    "\n",
    "# Residual\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "# Features\n",
    "feature_cols = [c for c in train.columns if c not in (metadata_cols + [target_col])]\n",
    "\n",
    "# Split\n",
    "X_train = train[feature_cols].copy()\n",
    "X_test = test[feature_cols].copy()\n",
    "\n",
    "y_train_total = train[target_col].values\n",
    "y_test_total = test[target_col].values\n",
    "\n",
    "# Residual targets\n",
    "y_train = (train[target_col] - train[vegas_col]).values\n",
    "y_test = (test[target_col]  - test[vegas_col]).values\n",
    "\n",
    "# Impute with median this method is so much easier\n",
    "medians = X_train.median(numeric_only=True)\n",
    "X_train = X_train.fillna(medians)\n",
    "X_test = X_test.fillna(medians)\n",
    "\n",
    "\n",
    "# Load our scaler and pca\n",
    "use_saved = False\n",
    "scaler_path = \"unsup_scaler.pkl\"\n",
    "pca_path = \"unsup_pca.pkl\"\n",
    "\n",
    "if os.path.exists(scaler_path) and os.path.exists(pca_path):\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    pca    = joblib.load(pca_path)\n",
    "    # Verify feature count matches\n",
    "    try:\n",
    "        if hasattr(scaler, \"n_features_in_\") and scaler.n_features_in_ == X_train.shape[1]:\n",
    "            use_saved = True\n",
    "    except Exception:\n",
    "        use_saved = False\n",
    "\n",
    "if use_saved:\n",
    "    print(\"\\nUsing saved scaler & PCA (unsup_scaler.pkl / unsup_pca.pkl)\")\n",
    "else:\n",
    "    print(\"Falling back to recreating scale and pca\")\n",
    "\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Choose components: keep up to 99% variance, cap at 50 to be safe\n",
    "    pca_full = PCA(svd_solver=\"auto\", random_state=42)\n",
    "    pca_full.fit(X_train_scaled)\n",
    "    cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "    n_comp = int(np.searchsorted(cumsum, 0.99) + 1)\n",
    "    n_comp = min(max(n_comp, 10), 50)  # between 10 and 50\n",
    "    print(f\"Selected PCA components: {n_comp} (â‰ˆ99% variance)\")\n",
    "\n",
    "    pca = PCA(n_components=n_comp, svd_solver=\"auto\", random_state=42)\n",
    "    # Re-fit with chosen n_components\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Save for reuse next time\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    joblib.dump(pca, pca_path)\n",
    "    print(\"Saved new unsup_scaler.pkl and unsup_pca.pkl\")\n",
    "\n",
    "# Transform using the scaler/PCA in use (saved or fresh)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_pca = pca.transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"\\nPCA features shape -> train: {X_train_pca.shape}, test: {X_test_pca.shape}\")\n",
    "\n",
    "# Model with specs\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=1300,\n",
    "    learning_rate=0.018,\n",
    "    max_depth=6,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0.15,\n",
    "    reg_alpha=0.6,\n",
    "    reg_lambda=1.2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=75,\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_pca, y_train,\n",
    "    eval_set=[(X_train_pca, y_train), (X_test_pca, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "# Eval\n",
    "print(\"RESIDUAL PERFORMANCE (Model vs Vegas)\")\n",
    "\n",
    "y_pred_resid = model.predict(X_test_pca)\n",
    "\n",
    "# Model Stats base\n",
    "mae_resid = mean_absolute_error(y_test, y_pred_resid)\n",
    "rmse_resid = np.sqrt(mean_squared_error(y_test, y_pred_resid))\n",
    "r2_resid = r2_score(y_test, y_pred_resid)\n",
    "\n",
    "\n",
    "# Reconstruct totals\n",
    "y_pred_total = y_pred_resid + test[vegas_col].values\n",
    "mae_total = mean_absolute_error(y_test_total, y_pred_total)\n",
    "rmse_total = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "r2_total = r2_score(y_test_total, y_pred_total)\n",
    "\n",
    "# Vegas baseline on the same split\n",
    "mae_line = mean_absolute_error(y_test_total, test[vegas_col].values)\n",
    "rmse_line = np.sqrt(mean_squared_error(y_test_total, test[vegas_col].values))\n",
    "r2_line = r2_score(y_test_total, test[vegas_col].values)\n",
    "\n",
    "print(\"ðŸ” BENCHMARK vs VEGAS\")\n",
    "print(f\"Vegas    -> MAE: {mae_line:.3f} | RMSE: {rmse_line:.3f} | RÂ²: {r2_line:.3f}\")\n",
    "print(f\"Residual -> MAE: {mae_total:.3f} | RMSE: {rmse_total:.3f} | RÂ²: {r2_total:.3f}\")\n",
    "print(f\"Î”MAE vs Vegas:  {mae_total - mae_line:+.3f}\")\n",
    "print(f\"Î”RMSE vs Vegas: {rmse_total - rmse_line:+.3f}\")\n",
    "\n",
    "# Saves\n",
    "test_out = test.copy()\n",
    "test_out[\"predicted_residual_pca\"] = y_pred_resid\n",
    "test_out[\"predicted_total_pca\"] = y_pred_total\n",
    "test_out[\"edge_vs_vegas_pca\"] = y_pred_resid\n",
    "test_out[\"abs_error_pca\"] = np.abs(y_test_total - y_pred_total)\n",
    "\n",
    "test_out.to_csv(\"test_predictions_residual_pca.csv\", index=False)\n",
    "model.save_model(\"xgboost_residual_pca.json\")\n",
    "\n",
    "# Save another feature list\n",
    "with open(\"pca_feature_cols.json\", \"w\") as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "\n",
    "print(\"Saved test_predictions_residual_pca.csv\")\n",
    "print(\"Saved xgboost_residual_pca.json\")\n",
    "print(\"Saved pca_feature_cols.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61c8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find where h2_total ends and analyze joins for that specific range\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load datasets\n",
    "games = pd.read_csv('nba_games_2021_to_2024.csv')\n",
    "betting = pd.read_csv('betting_data.csv')\n",
    "\n",
    "# Convert dates\n",
    "games['GAME_DATE'] = pd.to_datetime(games['GAME_DATE'])\n",
    "betting['date'] = pd.to_datetime(betting['date'])\n",
    "\n",
    "nba_min = games['GAME_DATE'].min()\n",
    "nba_max = games['GAME_DATE'].max()\n",
    "\n",
    "print(f\"\\nNBA Games: {games.shape}\")\n",
    "print(f\"  Date range: {nba_min} to {nba_max}\")\n",
    "\n",
    "print(f\"\\nBetting Data: {betting.shape}\")\n",
    "print(f\"  Date range: {betting['date'].min()} to {betting['date'].max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIND WHERE H2_TOTAL DATA ENDS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING H2_TOTAL AVAILABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter to betting data with h2_total only\n",
    "betting_with_h2 = betting[betting['h2_total'].notna()].copy()\n",
    "\n",
    "print(f\"\\nBetting rows with h2_total: {len(betting_with_h2):,} / {len(betting):,}\")\n",
    "print(f\"  h2_total date range: {betting_with_h2['date'].min()} to {betting_with_h2['date'].max()}\")\n",
    "\n",
    "# Filter to NBA date range\n",
    "betting_h2_in_nba_range = betting_with_h2[\n",
    "    (betting_with_h2['date'] >= nba_min) &\n",
    "    (betting_with_h2['date'] <= nba_max)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nBetting with h2_total in NBA date range: {len(betting_h2_in_nba_range):,}\")\n",
    "print(f\"  Date range: {betting_h2_in_nba_range['date'].min()} to {betting_h2_in_nba_range['date'].max()}\")\n",
    "\n",
    "# Group by month to see when h2_total stops\n",
    "betting_h2_in_nba_range['year_month'] = betting_h2_in_nba_range['date'].dt.to_period('M')\n",
    "monthly_h2 = betting_h2_in_nba_range.groupby('year_month').size()\n",
    "\n",
    "print(f\"\\nh2_total availability by month:\")\n",
    "print(monthly_h2)\n",
    "\n",
    "# Find the last date with h2_total\n",
    "h2_total_end_date = betting_h2_in_nba_range['date'].max()\n",
    "h2_total_start_date = betting_h2_in_nba_range['date'].min()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ H2_TOTAL DATE RANGE:\")\n",
    "print(f\"  Start: {h2_total_start_date}\")\n",
    "print(f\"  End: {h2_total_end_date}\")\n",
    "print(f\"  Duration: {(h2_total_end_date - h2_total_start_date).days} days\")\n",
    "\n",
    "# ============================================================================\n",
    "# FILTER NBA GAMES TO H2_TOTAL RANGE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILTERING NBA GAMES TO H2_TOTAL RANGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "games_in_h2_range = games[\n",
    "    (games['GAME_DATE'] >= h2_total_start_date) &\n",
    "    (games['GAME_DATE'] <= h2_total_end_date)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nOriginal NBA games: {len(games):,}\")\n",
    "print(f\"NBA games in h2_total range: {len(games_in_h2_range):,}\")\n",
    "print(f\"Excluded (outside h2_total range): {len(games) - len(games_in_h2_range):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEAM MAPPING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEAM MAPPING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "team_map = {\n",
    "    'atl': 'ATL', 'bos': 'BOS', 'bkn': 'BKN', 'cha': 'CHA', 'chi': 'CHI',\n",
    "    'cle': 'CLE', 'dal': 'DAL', 'den': 'DEN', 'det': 'DET', 'gs': 'GSW',\n",
    "    'hou': 'HOU', 'ind': 'IND', 'lac': 'LAC', 'lal': 'LAL', 'mem': 'MEM',\n",
    "    'mia': 'MIA', 'mil': 'MIL', 'min': 'MIN', 'no': 'NOP', 'nyk': 'NYK',\n",
    "    'okc': 'OKC', 'orl': 'ORL', 'phi': 'PHI', 'phx': 'PHX', 'por': 'POR',\n",
    "    'sac': 'SAC', 'sa': 'SAS', 'tor': 'TOR', 'utah': 'UTA', 'wsh': 'WAS',\n",
    "    'nj': 'BKN', 'ny': 'NYK'\n",
    "}\n",
    "\n",
    "betting_h2_in_nba_range['away_team'] = betting_h2_in_nba_range['away'].map(team_map)\n",
    "betting_h2_in_nba_range['home_team'] = betting_h2_in_nba_range['home'].map(team_map)\n",
    "\n",
    "unmapped = betting_h2_in_nba_range[\n",
    "    betting_h2_in_nba_range['away_team'].isna() | \n",
    "    betting_h2_in_nba_range['home_team'].isna()\n",
    "]\n",
    "print(f\"Unmapped teams: {len(unmapped)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE MATCHUPS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING MATCHUPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# NBA matchups\n",
    "games_in_h2_range['is_home'] = games_in_h2_range['MATCHUP'].str.contains('vs.', na=False)\n",
    "\n",
    "home_games = games_in_h2_range[games_in_h2_range['is_home'] == True][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "away_games = games_in_h2_range[games_in_h2_range['is_home'] == False][\n",
    "    ['GAME_ID', 'GAME_DATE', 'TEAM_ABBREVIATION']\n",
    "].copy()\n",
    "\n",
    "home_games.columns = ['GAME_ID', 'GAME_DATE', 'home_team']\n",
    "away_games.columns = ['GAME_ID', 'GAME_DATE', 'away_team']\n",
    "\n",
    "nba_matchups = home_games.merge(away_games, on=['GAME_ID', 'GAME_DATE'])\n",
    "\n",
    "print(f\"NBA matchups: {len(nba_matchups):,}\")\n",
    "\n",
    "# Create match keys\n",
    "nba_matchups['match_key'] = (\n",
    "    nba_matchups['GAME_DATE'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    nba_matchups['away_team'] + '_' + \n",
    "    nba_matchups['home_team']\n",
    ")\n",
    "\n",
    "betting_h2_in_nba_range['match_key'] = (\n",
    "    betting_h2_in_nba_range['date'].dt.strftime('%Y-%m-%d') + '_' + \n",
    "    betting_h2_in_nba_range['away_team'] + '_' + \n",
    "    betting_h2_in_nba_range['home_team']\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# JOIN ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JOIN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "nba_keys = set(nba_matchups['match_key'])\n",
    "betting_keys = set(betting_h2_in_nba_range['match_key'].dropna())\n",
    "\n",
    "matched = nba_keys & betting_keys\n",
    "nba_only = nba_keys - betting_keys\n",
    "betting_only = betting_keys - nba_keys\n",
    "\n",
    "print(f\"\\nðŸ“Š Match Results:\")\n",
    "print(f\"  NBA games (in h2_total range): {len(nba_keys):,}\")\n",
    "print(f\"  Betting lines with h2_total: {len(betting_keys):,}\")\n",
    "print(f\"  âœ… Matched: {len(matched):,}\")\n",
    "print(f\"  âš ï¸  NBA games without h2_total: {len(nba_only):,}\")\n",
    "print(f\"  âš ï¸  Betting h2_total without NBA: {len(betting_only):,}\")\n",
    "\n",
    "match_rate = len(matched) / len(nba_keys) * 100 if len(nba_keys) > 0 else 0\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Match Rate: {match_rate:.1f}%\")\n",
    "print(f\"   ({len(matched):,} / {len(nba_keys):,} NBA games have h2_total)\")\n",
    "\n",
    "# ============================================================================\n",
    "# JOIN AND SAVE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Perform join\n",
    "nba_with_h2 = nba_matchups.merge(\n",
    "    betting_h2_in_nba_range[['match_key', 'h2_total', 'h2_spread', 'spread', 'total']],\n",
    "    on='match_key',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save main dataset\n",
    "nba_with_h2.to_csv('nba_games_with_h2_total.csv', index=False)\n",
    "print(f\"âœ… Saved nba_games_with_h2_total.csv\")\n",
    "print(f\"   Total games: {len(nba_with_h2):,}\")\n",
    "print(f\"   With h2_total: {nba_with_h2['h2_total'].notna().sum():,}\")\n",
    "print(f\"   Without h2_total: {nba_with_h2['h2_total'].isna().sum():,}\")\n",
    "\n",
    "# Save failed NBA joins (NBA games that should have h2_total but don't)\n",
    "if len(nba_only) > 0:\n",
    "    failed_nba = nba_matchups[nba_matchups['match_key'].isin(nba_only)]\n",
    "    failed_nba.to_csv('nba_games_missing_h2_total.csv', index=False)\n",
    "    print(f\"\\nâŒ Saved nba_games_missing_h2_total.csv\")\n",
    "    print(f\"   NBA games without h2_total: {len(failed_nba):,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\n   Sample games missing h2_total:\")\n",
    "    print(failed_nba[['GAME_DATE', 'away_team', 'home_team', 'GAME_ID']].head(10))\n",
    "    \n",
    "    # Group by date to see patterns\n",
    "    failed_by_date = failed_nba.groupby(failed_nba['GAME_DATE'].dt.date).size()\n",
    "    print(f\"\\n   Missing h2_total by date (first 20 days):\")\n",
    "    print(failed_by_date.head(20))\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ H2_TOTAL Coverage:\")\n",
    "print(f\"  Date range analyzed: {h2_total_start_date} to {h2_total_end_date}\")\n",
    "print(f\"  NBA games in range: {len(nba_matchups):,}\")\n",
    "print(f\"  Games with h2_total: {len(matched):,} ({match_rate:.1f}%)\")\n",
    "print(f\"  Games missing h2_total: {len(nba_only):,} ({len(nba_only)/len(nba_matchups)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files Created:\")\n",
    "print(f\"  - nba_games_with_h2_total.csv (use this for modeling)\")\n",
    "print(f\"  - nba_games_missing_h2_total.csv (failed joins to investigate)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Next Step:\")\n",
    "print(f\"   Review nba_games_missing_h2_total.csv to understand why {len(nba_only)} games\")\n",
    "print(f\"   within the h2_total date range don't have betting lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Holdouts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Loading Model\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(\"xgboost_residual_model.json\")\n",
    "\n",
    "# Load Data\n",
    "holdout_data = pd.read_csv(\"holdout_data.csv\")\n",
    "print(f\"Holdout: {len(holdout_data)} rows\")\n",
    "\n",
    "# Features\n",
    "metadata_cols = [\"GAME_ID\", \"GAME_DATE\", \"TEAM_ID\", \"season\"]\n",
    "target_col = \"actual_second_half_total\"\n",
    "vegas_col = \"h2_total\"\n",
    "\n",
    "feature_cols = [c for c in holdout_data.columns if c not in metadata_cols + [target_col]]\n",
    "X_holdout = holdout_data[feature_cols].copy()\n",
    "\n",
    "# Impute missing values\n",
    "for col in X_holdout.columns:\n",
    "    if X_holdout[col].isnull().any():\n",
    "        X_holdout[col].fillna(X_holdout[col].median(), inplace=True)\n",
    "\n",
    "# Predict\n",
    "predicted_residuals = model.predict(X_holdout)\n",
    "\n",
    "# Write predictions to DF\n",
    "holdout_data[\"predicted_residual\"] = predicted_residuals\n",
    "holdout_data[\"predicted_total\"] = predicted_residuals + holdout_data[vegas_col]\n",
    "holdout_data[\"bet_edge\"] = predicted_residuals\n",
    "\n",
    "# Save complete dataset\n",
    "holdout_data.to_csv(\"holdout_predictions.csv\", index=False)\n",
    "print(f\"Saved holdout_predictions.csv\")\n",
    "\n",
    "## Clean up for a streamlined df.\n",
    "\n",
    "# Select key columns\n",
    "clean_predictions = holdout_data[[\n",
    "    'GAME_ID',\n",
    "    'GAME_DATE',\n",
    "    'TEAM_ID',\n",
    "    'h2_total',\n",
    "    'predicted_total',\n",
    "    'actual_second_half_total',\n",
    "    'bet_edge'\n",
    "]].copy()\n",
    "\n",
    "# Format dates\n",
    "clean_predictions['GAME_DATE'] = pd.to_datetime(clean_predictions['GAME_DATE']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Rename columns\n",
    "clean_predictions.columns = [\n",
    "    'game_id',\n",
    "    'date',\n",
    "    'team_id',\n",
    "    'h2_total',\n",
    "    'predicted_total',\n",
    "    'actual_total',\n",
    "    'bet_edge'\n",
    "]\n",
    "\n",
    "# Sort by date\n",
    "clean_predictions = clean_predictions.sort_values('date')\n",
    "\n",
    "# Save clean (two rows per game - home and away teams)\n",
    "clean_predictions.to_csv('holdout_predictions_clean.csv', index=False)\n",
    "print(f\"Saved holdout_predictions_clean.csv\")\n",
    "\n",
    "# Save game-level (one row per game)\n",
    "game_level = clean_predictions.drop_duplicates('game_id').copy()\n",
    "game_level.to_csv('holdout_predictions_games.csv', index=False)\n",
    "print(f\"Saved holdout_predictions_games.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f56721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding betting thresholds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Use games data\n",
    "holdout_data = pd.read_csv(\"holdout_predictions_games.csv\")\n",
    "\n",
    "# Filter to games with both actuals and betting lines\n",
    "bettable = holdout_data[\n",
    "    (holdout_data[\"h2_total\"].notna()) &\n",
    "    (holdout_data[\"actual_total\"].notna())\n",
    "].copy()\n",
    "\n",
    "# Comparison cols\n",
    "bet_edge = bettable[\"bet_edge\"].values\n",
    "actual_total = bettable[\"actual_total\"].values  # Changed\n",
    "vegas_line = bettable[\"h2_total\"].values\n",
    "\n",
    "# Results dict\n",
    "results = []\n",
    "\n",
    "# Testing a thresholds in a range .5 all the way to 7.25 in .25 increments\n",
    "thresholds = np.arange(0.5, 7.25, 0.25)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Filtering bets\n",
    "    bet_over = bet_edge > threshold\n",
    "    bet_under = bet_edge < -threshold\n",
    "    \n",
    "    # Count bets\n",
    "    over_count = bet_over.sum()\n",
    "    under_count = bet_under.sum()\n",
    "    total_bets = over_count + under_count\n",
    "    \n",
    "    if total_bets == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate wins\n",
    "    over_wins = ((actual_total > vegas_line) & bet_over).sum()\n",
    "    under_wins = ((actual_total < vegas_line) & bet_under).sum()\n",
    "    total_wins = over_wins + under_wins\n",
    "    \n",
    "    # Win rates\n",
    "    over_win_rate = over_wins / over_count * 100 if over_count > 0 else 0\n",
    "    under_win_rate = under_wins / under_count * 100 if under_count > 0 else 0\n",
    "    total_win_rate = total_wins / total_bets * 100\n",
    "    \n",
    "    # Profit with 110 odds\n",
    "    over_profit = (over_wins * 100) - ((over_count - over_wins) * 110)\n",
    "    under_profit = (under_wins * 100) - ((under_count - under_wins) * 110)\n",
    "    total_profit = over_profit + under_profit\n",
    "    \n",
    "    # ROI\n",
    "    total_risked = total_bets * 110\n",
    "    roi = (total_profit / total_risked * 100) if total_risked > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'total_bets': total_bets,\n",
    "        'over_bets': over_count,\n",
    "        'under_bets': under_count,\n",
    "        'total_wins': total_wins,\n",
    "        'over_wins': over_wins,\n",
    "        'under_wins': under_wins,\n",
    "        'win_rate': total_win_rate,\n",
    "        'over_win_rate': over_win_rate,\n",
    "        'under_win_rate': under_win_rate,\n",
    "        'profit': total_profit,\n",
    "        'roi': roi\n",
    "    })\n",
    "\n",
    "# CAst to DF\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# prints\n",
    "top_roi = results_df.nlargest(10, 'roi')\n",
    "print(top_roi[['threshold', 'total_bets', 'win_rate', 'profit', 'roi']].to_string(index=False))\n",
    "\n",
    "# Write results\n",
    "results_df.to_csv('threshold_optimization_results.csv', index=False)\n",
    "print(f\"Saved threshold_optimization_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
